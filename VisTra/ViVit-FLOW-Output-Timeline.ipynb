{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d494f63c-6724-495b-b201-247d658eda70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViVit model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from threading import Thread, Lock\n",
    "import torchvision.transforms as T\n",
    "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
    "from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, \n",
    "                            QHBoxLayout, QLabel, QPushButton, QSlider, QStyle, \n",
    "                            QFileDialog, QFrame)\n",
    "from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QThread, QMutex\n",
    "from PyQt5.QtGui import QImage, QPixmap, QPainter, QColor, QPen, QBrush\n",
    "\n",
    "# Paths and constants\n",
    "SAVED_MODEL_PATH = 'F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/flow/best_model_acc.pt'\n",
    "LABEL_MAP = {0: 'Normal', 1: 'Explosion', 2: 'Fighting', 3: 'Car Accident', 4: 'Shooting', 5: 'Riot'}\n",
    "COLOR_MAP = {\n",
    "    'Normal': QColor(50, 200, 50),     # Green\n",
    "    'Explosion': QColor(255, 127, 0),  # Orange\n",
    "    'Fighting': QColor(200, 50, 50),   # Red\n",
    "    'Car Accident': QColor(50, 50, 200), # Blue\n",
    "    'Shooting': QColor(200, 0, 200),   # Purple\n",
    "    'Riot': QColor(255, 255, 0)        # Yellow\n",
    "}\n",
    "CLIP_LEN = 32\n",
    "WINDOW_SIZE_SECONDS = 3  # Process 3 seconds of video at a time\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading ViVit model and processor...\")\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\", do_rescale=None, offset=None)\n",
    "model = VivitForVideoClassification.from_pretrained(\n",
    "    \"google/vivit-b-16x2\",\n",
    "    num_labels=len(LABEL_MAP),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Load saved weights\n",
    "model.load_state_dict(torch.load(SAVED_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Transform for preprocessing frames\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8c6a98-f4d7-481c-8ac3-7642cb6e6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for prediction worker thread\n",
    "class PredictionWorker(QThread):\n",
    "    predictionReady = pyqtSignal(float, float, str)\n",
    "    \n",
    "    def __init__(self, model, processor, transform):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.frames_queue = deque()\n",
    "        self.timestamps = []\n",
    "        self.running = True\n",
    "        self.mutex = QMutex()\n",
    "        \n",
    "    def add_frames(self, frames, start_time, end_time):\n",
    "        self.mutex.lock()\n",
    "        self.frames_queue.append((frames, start_time, end_time))\n",
    "        self.mutex.unlock()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        \n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            if self.frames_queue:\n",
    "                self.mutex.lock()\n",
    "                frames, start_time, end_time = self.frames_queue.popleft()\n",
    "                self.mutex.unlock()\n",
    "                \n",
    "                if len(frames) < CLIP_LEN:\n",
    "                    # Pad with repeated frames if needed\n",
    "                    if len(frames) > 0:\n",
    "                        frames = frames + [frames[-1]] * (CLIP_LEN - len(frames))\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                # Process and make prediction\n",
    "                try:\n",
    "                    # Preprocess frames\n",
    "                    processed_frames = [self.transform(frame) for frame in frames]\n",
    "                    frames_numpy = [frame.permute(1, 2, 0).numpy() for frame in processed_frames]\n",
    "                    \n",
    "                    # Process with ViVit processor\n",
    "                    inputs = self.processor(frames_numpy, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        logits = outputs.logits\n",
    "                        predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "                        predicted_class = LABEL_MAP[predicted_id]\n",
    "                    \n",
    "                    # Emit the prediction result\n",
    "                    self.predictionReady.emit(start_time, end_time, predicted_class)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error making prediction: {e}\")\n",
    "            \n",
    "            # Sleep to avoid high CPU usage\n",
    "            time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d90acc-36ea-48b8-bf67-b16989640ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main application window\n",
    "class VideoPlayerWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup UI\n",
    "        self.setWindowTitle(\"Anomaly Detection Video Player\")\n",
    "        self.setGeometry(100, 100, 1000, 600)\n",
    "        \n",
    "        # Create central widget and layout\n",
    "        self.central_widget = QWidget()\n",
    "        self.setCentralWidget(self.central_widget)\n",
    "        self.layout = QVBoxLayout(self.central_widget)\n",
    "        \n",
    "        # Video display area\n",
    "        self.video_frame = QLabel()\n",
    "        self.video_frame.setAlignment(Qt.AlignCenter)\n",
    "        self.video_frame.setMinimumSize(640, 480)\n",
    "        self.video_frame.setStyleSheet(\"background-color: black;\")\n",
    "        self.layout.addWidget(self.video_frame)\n",
    "        \n",
    "        # Current classification display\n",
    "        self.classification_label = QLabel(\"Classification: None\")\n",
    "        self.classification_label.setAlignment(Qt.AlignCenter)\n",
    "        self.classification_label.setStyleSheet(\"font-size: 18px; font-weight: bold;\")\n",
    "        self.layout.addWidget(self.classification_label)\n",
    "        \n",
    "        # Timeline widget\n",
    "        self.timeline_widget = TimelineWidget()\n",
    "        self.layout.addWidget(self.timeline_widget)\n",
    "        \n",
    "        # Controls layout\n",
    "        self.controls_layout = QHBoxLayout()\n",
    "        \n",
    "        # Play/Pause button\n",
    "        self.play_button = QPushButton()\n",
    "        self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "        self.play_button.clicked.connect(self.toggle_play)\n",
    "        self.controls_layout.addWidget(self.play_button)\n",
    "        \n",
    "        # Time display\n",
    "        self.time_label = QLabel(\"00:00 / 00:00\")\n",
    "        self.controls_layout.addWidget(self.time_label)\n",
    "        \n",
    "        # Position slider\n",
    "        self.position_slider = QSlider(Qt.Horizontal)\n",
    "        self.position_slider.sliderMoved.connect(self.set_position)\n",
    "        self.controls_layout.addWidget(self.position_slider)\n",
    "        \n",
    "        # Open file button\n",
    "        self.open_button = QPushButton(\"Open Video\")\n",
    "        self.open_button.clicked.connect(self.open_file)\n",
    "        self.controls_layout.addWidget(self.open_button)\n",
    "        \n",
    "        self.layout.addLayout(self.controls_layout)\n",
    "        \n",
    "        # Video processing variables\n",
    "        self.cap = None\n",
    "        self.timer = QTimer(self)\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.current_frame = 0\n",
    "        self.fps = 0\n",
    "        self.total_frames = 0\n",
    "        self.playing = False\n",
    "        \n",
    "        # Anomaly detection variables\n",
    "        self.anomaly_segments = []\n",
    "        self.current_window_frames = []\n",
    "        self.current_window_start_time = 0\n",
    "        self.frames_since_last_prediction = 0\n",
    "        self.prediction_interval = 0  # Will be set based on video fps\n",
    "        \n",
    "        # Start prediction thread\n",
    "        self.prediction_worker = PredictionWorker(model, processor, transform)\n",
    "        self.prediction_worker.predictionReady.connect(self.update_anomaly)\n",
    "        self.prediction_worker.start()\n",
    "\n",
    "    def open_file(self):\n",
    "        file_path, _ = QFileDialog.getOpenFileName(self, \"Open Video File\", \"\", \n",
    "                                                 \"Video Files (*.mp4 *.avi *.mkv *.mov);;All Files (*)\")\n",
    "        if file_path:\n",
    "            self.load_video(file_path)\n",
    "    \n",
    "    def load_video(self, file_path):\n",
    "        # Stop current video if playing\n",
    "        if self.playing:\n",
    "            self.timer.stop()\n",
    "            self.playing = False\n",
    "        \n",
    "        # Release previous capture if any\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        \n",
    "        # Open new video\n",
    "        self.cap = cv2.VideoCapture(file_path)\n",
    "        if not self.cap.isOpened():\n",
    "            print(f\"Error: Could not open video {file_path}\")\n",
    "            return\n",
    "        \n",
    "        # Get video properties\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.current_frame = 0\n",
    "        \n",
    "        # Set prediction interval based on fps and window size\n",
    "        self.prediction_interval = int(self.fps * WINDOW_SIZE_SECONDS)\n",
    "        \n",
    "        # Clear previous anomalies\n",
    "        self.anomaly_segments = []\n",
    "        self.timeline_widget.set_anomalies([])\n",
    "        self.timeline_widget.set_duration(self.total_frames / self.fps)\n",
    "        \n",
    "        # Reset UI\n",
    "        self.position_slider.setRange(0, self.total_frames)\n",
    "        duration_str = self.format_time(self.total_frames / self.fps)\n",
    "        self.time_label.setText(f\"00:00 / {duration_str}\")\n",
    "        self.classification_label.setText(\"Classification: None\")\n",
    "        \n",
    "        # Reset frame collection\n",
    "        self.current_window_frames = []\n",
    "        self.frames_since_last_prediction = 0\n",
    "        self.current_window_start_time = 0\n",
    "        \n",
    "        # Show first frame\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            self.display_frame(frame)\n",
    "    \n",
    "    def toggle_play(self):\n",
    "        if self.cap is None:\n",
    "            return\n",
    "            \n",
    "        if self.playing:\n",
    "            self.timer.stop()\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "        else:\n",
    "            self.timer.start(1000 // 30)  # 30 fps display\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPause))\n",
    "        \n",
    "        self.playing = not self.playing\n",
    "    \n",
    "    def update_frame(self):\n",
    "        if self.cap is None or not self.playing:\n",
    "            return\n",
    "            \n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            # End of video\n",
    "            self.timer.stop()\n",
    "            self.playing = False\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "            return\n",
    "        \n",
    "        # Convert frame for prediction (PIL Image) - FIXED CONVERSION\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_frame = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Display the frame\n",
    "        self.display_frame(frame)\n",
    "        \n",
    "        # Collect frames for prediction\n",
    "        current_time = self.current_frame / self.fps\n",
    "        \n",
    "        # If this is the first frame in a window, set the start time\n",
    "        if len(self.current_window_frames) == 0:\n",
    "            self.current_window_start_time = current_time\n",
    "        \n",
    "        # Add frame to current window\n",
    "        self.current_window_frames.append(pil_frame)\n",
    "        self.frames_since_last_prediction += 1\n",
    "        \n",
    "        # If enough frames collected, process them\n",
    "        if self.frames_since_last_prediction >= self.prediction_interval:\n",
    "            # Subsample to get CLIP_LEN frames\n",
    "            if len(self.current_window_frames) > CLIP_LEN:\n",
    "                indices = np.linspace(0, len(self.current_window_frames) - 1, CLIP_LEN, dtype=int)\n",
    "                frames_to_process = [self.current_window_frames[i] for i in indices]\n",
    "            else:\n",
    "                frames_to_process = self.current_window_frames\n",
    "            \n",
    "            # Add to prediction queue\n",
    "            self.prediction_worker.add_frames(\n",
    "                frames_to_process, \n",
    "                self.current_window_start_time,\n",
    "                current_time\n",
    "            )\n",
    "            \n",
    "            # Reset for next window\n",
    "            self.current_window_frames = []\n",
    "            self.frames_since_last_prediction = 0\n",
    "        \n",
    "        # Update slider and time label\n",
    "        self.position_slider.setValue(self.current_frame)\n",
    "        current_time_str = self.format_time(current_time)\n",
    "        duration_str = self.format_time(self.total_frames / self.fps)\n",
    "        self.time_label.setText(f\"{current_time_str} / {duration_str}\")\n",
    "        \n",
    "        # Update classification display\n",
    "        current_class = self.get_classification_at_time(current_time)\n",
    "        self.classification_label.setText(f\"Classification: {current_class}\")\n",
    "        \n",
    "        # Set classification color\n",
    "        color = COLOR_MAP.get(current_class, QColor(100, 100, 100))\n",
    "        self.classification_label.setStyleSheet(f\"font-size: 18px; font-weight: bold; color: rgb({color.red()}, {color.green()}, {color.blue()})\")\n",
    "        \n",
    "        # Increment frame counter\n",
    "        self.current_frame += 1\n",
    "    \n",
    "    def display_frame(self, frame):\n",
    "        # Convert frame to QImage and display\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = frame_rgb.shape\n",
    "        bytes_per_line = ch * w\n",
    "        q_img = QImage(frame_rgb.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "        self.video_frame.setPixmap(QPixmap.fromImage(q_img).scaled(\n",
    "            self.video_frame.width(), self.video_frame.height(), \n",
    "            Qt.KeepAspectRatio, Qt.SmoothTransformation))\n",
    "    \n",
    "    def set_position(self, position):\n",
    "        if self.cap is None:\n",
    "            return\n",
    "            \n",
    "        # Seek to position\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, position)\n",
    "        self.current_frame = position\n",
    "        \n",
    "        # Read and display the frame\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            self.display_frame(frame)\n",
    "            \n",
    "            # Update time label\n",
    "            current_time = position / self.fps\n",
    "            current_time_str = self.format_time(current_time)\n",
    "            duration_str = self.format_time(self.total_frames / self.fps)\n",
    "            self.time_label.setText(f\"{current_time_str} / {duration_str}\")\n",
    "            \n",
    "            # Update classification display\n",
    "            current_class = self.get_classification_at_time(current_time)\n",
    "            self.classification_label.setText(f\"Classification: {current_class}\")\n",
    "            \n",
    "            # Set classification color\n",
    "            color = COLOR_MAP.get(current_class, QColor(100, 100, 100))\n",
    "            self.classification_label.setStyleSheet(f\"font-size: 18px; font-weight: bold; color: rgb({color.red()}, {color.green()}, {color.blue()})\")\n",
    "    \n",
    "    def update_anomaly(self, start_time, end_time, anomaly_type):\n",
    "        # Add new anomaly segment\n",
    "        self.anomaly_segments.append((start_time, end_time, anomaly_type))\n",
    "        # Update timeline display\n",
    "        self.timeline_widget.set_anomalies(self.anomaly_segments)\n",
    "    \n",
    "    def get_classification_at_time(self, time_point):\n",
    "        # Find the most recent classification for the current time\n",
    "        for start_time, end_time, anomaly_type in reversed(self.anomaly_segments):\n",
    "            if start_time <= time_point and time_point <= end_time:\n",
    "                return anomaly_type\n",
    "        return \"None\"\n",
    "    \n",
    "    def format_time(self, seconds):\n",
    "        minutes, seconds = divmod(int(seconds), 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\" if hours else f\"{minutes:02d}:{seconds:02d}\"\n",
    "    \n",
    "    def closeEvent(self, event):\n",
    "        # Clean up resources\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        self.prediction_worker.stop()\n",
    "        self.prediction_worker.wait()\n",
    "        event.accept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a869f82-0892-4934-bdca-c37fdc4998bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline widget to show anomaly segments\n",
    "class TimelineWidget(QFrame):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setMinimumHeight(40)\n",
    "        self.setStyleSheet(\"background-color: #2d2d2d;\")\n",
    "        \n",
    "        self.anomalies = []\n",
    "        self.duration = 0\n",
    "        \n",
    "    def set_anomalies(self, anomalies):\n",
    "        self.anomalies = anomalies\n",
    "        self.update()\n",
    "        \n",
    "    def set_duration(self, duration):\n",
    "        self.duration = duration\n",
    "        self.update()\n",
    "        \n",
    "    def paintEvent(self, event):\n",
    "        if self.duration <= 0:\n",
    "            return\n",
    "            \n",
    "        painter = QPainter(self)\n",
    "        painter.setRenderHint(QPainter.Antialiasing)\n",
    "        \n",
    "        # Draw background\n",
    "        painter.fillRect(self.rect(), QBrush(QColor(45, 45, 45)))\n",
    "        \n",
    "        # Draw timeline base\n",
    "        painter.setPen(QPen(QColor(200, 200, 200), 1))\n",
    "        y_middle = self.height() // 2\n",
    "        painter.drawLine(0, y_middle, self.width(), y_middle)\n",
    "        \n",
    "        # Draw time markers\n",
    "        painter.setPen(QPen(QColor(150, 150, 150), 1))\n",
    "        marker_interval = self.width() / 10\n",
    "        for i in range(11):\n",
    "            x = i * marker_interval\n",
    "            painter.drawLine(int(x), y_middle - 5, int(x), y_middle + 5)\n",
    "            \n",
    "            # Draw time text\n",
    "            time_at_marker = (i / 10) * self.duration\n",
    "            minutes = int(time_at_marker / 60)\n",
    "            seconds = int(time_at_marker % 60)\n",
    "            time_text = f\"{minutes:02d}:{seconds:02d}\"\n",
    "            painter.drawText(int(x) - 15, y_middle + 20, time_text)\n",
    "        \n",
    "        # Draw anomaly segments\n",
    "        for start_time, end_time, anomaly_type in self.anomalies:\n",
    "            if start_time >= self.duration:\n",
    "                continue\n",
    "                \n",
    "            # Calculate positions\n",
    "            start_pos = int((start_time / self.duration) * self.width())\n",
    "            end_pos = int((min(end_time, self.duration) / self.duration) * self.width())\n",
    "            \n",
    "            # Get color for anomaly type\n",
    "            color = COLOR_MAP.get(anomaly_type, QColor(100, 100, 100))\n",
    "            \n",
    "            # Draw segment\n",
    "            painter.fillRect(start_pos, 5, end_pos - start_pos, self.height() - 10, QBrush(color))\n",
    "            \n",
    "            # Draw label if segment is wide enough\n",
    "            if end_pos - start_pos > 50:\n",
    "                painter.setPen(QPen(QColor(255, 255, 255), 1))\n",
    "                painter.drawText(start_pos + 5, y_middle + 5, anomaly_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af396b-a7c7-4aa5-b6bb-63d859931a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main application\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = VideoPlayerWindow()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVi",
   "language": "python",
   "name": "vivi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
