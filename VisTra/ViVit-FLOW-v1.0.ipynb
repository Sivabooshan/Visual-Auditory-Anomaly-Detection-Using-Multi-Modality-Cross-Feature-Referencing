{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8276\\1464558941.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('label', group_keys=False).apply(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8276\\1464558941.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('label', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Fighting: Moving 4 videos (5760 frames) from test to train\n",
      "Label Riot: Moving 13 videos (18300 frames) from test to train\n",
      "\n",
      "Updated frame distribution:\n",
      "Label Normal: 9384 frames in test set\n",
      "Label Explosion: 8604 frames in test set\n",
      "Label Fighting: 9841 frames in test set\n",
      "Label Car Accident: 3861 frames in test set\n",
      "Label Shooting: 4422 frames in test set\n",
      "Label Riot: 8920 frames in test set\n",
      "\n",
      "Balanced number of videos per label in the training set:\n",
      "Normal (0): 110 videos\n",
      "Explosion (1): 106 videos\n",
      "Fighting (2): 114 videos\n",
      "Car Accident (3): 110 videos\n",
      "Shooting (4): 110 videos\n",
      "Riot (5): 123 videos\n",
      "\n",
      "Balanced number of videos per label in the validation set:\n",
      "Normal (0): 30 videos\n",
      "Explosion (1): 27 videos\n",
      "Fighting (2): 26 videos\n",
      "Car Accident (3): 30 videos\n",
      "Shooting (4): 30 videos\n",
      "Riot (5): 17 videos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import VivitForVideoClassification, VivitImageProcessor, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load and split the data\n",
    "df = pd.read_csv('E:/SRC-Bhuvaneswari/processed files/video/ftest/test_data.csv')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Save split datasets\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "TRAIN_CSV_PATH = 'train_data.csv'\n",
    "TEST_CSV_PATH = 'test_data.csv'\n",
    "SAVE_DIR = 'F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/flow'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Define label mapping\n",
    "LABEL_MAP = {'Normal': 0, 'Explosion': 1, 'Fighting': 2, 'Car Accident': 3, 'Shooting': 4, 'Riot': 5}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# Define maximum clips per label - adjusted for full frame usage\n",
    "MAX_TRAIN_SAMPLES = 110\n",
    "MAX_TEST_SAMPLES = 30\n",
    "\n",
    "# Define maximum frames per label for 80-20 ratio\n",
    "# For a 80-20 ratio, if we assume each label should have equal representation\n",
    "MAX_FRAME = 40000  # 80% of frames per label\n",
    "MAX_TEST_FRAME = 10000  # 20% of frames per label\n",
    "\n",
    "# Improved hyperparameters\n",
    "LEARNING_RATE = 5e-5\n",
    "TRAIN_BATCH_SIZE = 3  # Reduced to handle more frames\n",
    "EVAL_BATCH_SIZE = 3\n",
    "SEED = 24\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Increased for stability\n",
    "NUM_EPOCHS = 10\n",
    "CLIP_LEN = 32\n",
    "FRAME_SAMPLE_RATE = 1  # Changed to 1 to use all frames\n",
    "TARGET_FRAMES = 128\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "def load_data_from_csv(csv_path, video_path_column='flow_video_path'):\n",
    "    \"\"\"Load and clean dataframe from CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=[video_path_column, 'label'])\n",
    "    return df\n",
    "\n",
    "df_train = load_data_from_csv(TRAIN_CSV_PATH)\n",
    "df_test = load_data_from_csv(TEST_CSV_PATH)\n",
    "\n",
    "def balance_dataset(df, max_samples):\n",
    "    \"\"\"Balance dataset to have equal number of samples per class.\"\"\"\n",
    "    return df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples), random_state=SEED)\n",
    "    )\n",
    "\n",
    "df_train = balance_dataset(df_train, MAX_TRAIN_SAMPLES)\n",
    "df_test = balance_dataset(df_test, MAX_TEST_SAMPLES)\n",
    "\n",
    "# Function to balance test and train sets based on frame count\n",
    "def balance_test_and_train_sets(df_train, df_test):\n",
    "    \"\"\"Balance test set to MAX_TEST_FRAME per label and move excess to training.\"\"\"\n",
    "    # Group test videos by label\n",
    "    test_videos_by_label = {label: [] for label in LABEL_MAP.values()}\n",
    "    test_frames_by_label = {label: 0 for label in LABEL_MAP.values()}\n",
    "    \n",
    "    # Count frames per label in test set\n",
    "    for _, row in df_test.iterrows():\n",
    "        video_path = row['flow_video_path']\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            frame_count = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            test_videos_by_label[label].append((video_path, frame_count))\n",
    "            test_frames_by_label[label] += frame_count\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test video {video_path}: {e}\")\n",
    "    \n",
    "    # Move excess videos from test to train\n",
    "    videos_to_move = []\n",
    "    \n",
    "    for label, videos in test_videos_by_label.items():\n",
    "        if test_frames_by_label[label] > MAX_TEST_FRAME:\n",
    "            # Sort videos by frame count (optional - depends on your strategy)\n",
    "            videos.sort(key=lambda x: x[1])\n",
    "            \n",
    "            current_frames = 0\n",
    "            keep_idx = 0\n",
    "            \n",
    "            # Find how many videos to keep in test set\n",
    "            for i, (_, frame_count) in enumerate(videos):\n",
    "                if current_frames + frame_count <= MAX_TEST_FRAME:\n",
    "                    current_frames += frame_count\n",
    "                    keep_idx = i + 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Identify videos to move to training\n",
    "            videos_to_move.extend([path for path, _ in videos[keep_idx:]])\n",
    "            \n",
    "            print(f\"Label {INV_LABEL_MAP[label]}: Moving {len(videos) - keep_idx} videos \" \n",
    "                  f\"({test_frames_by_label[label] - current_frames} frames) from test to train\")\n",
    "            \n",
    "            # Update frame count\n",
    "            test_frames_by_label[label] = current_frames\n",
    "    \n",
    "    # Create masks for videos to keep in test and move to train\n",
    "    keep_mask = ~df_test['flow_video_path'].isin(videos_to_move)\n",
    "    \n",
    "    # Extract videos to move\n",
    "    df_to_move = df_test[~keep_mask].copy()\n",
    "    \n",
    "    # Update test dataframe\n",
    "    df_test_balanced = df_test[keep_mask].copy()\n",
    "    \n",
    "    # Add moved videos to training\n",
    "    df_train_updated = pd.concat([df_train, df_to_move], ignore_index=True)\n",
    "    \n",
    "    print(\"\\nUpdated frame distribution:\")\n",
    "    for label, frames in test_frames_by_label.items():\n",
    "        print(f\"Label {INV_LABEL_MAP[label]}: {frames} frames in test set\")\n",
    "    \n",
    "    return df_train_updated, df_test_balanced\n",
    "\n",
    "# Apply the balancing function\n",
    "df_train, df_test = balance_test_and_train_sets(df_train, df_test)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nBalanced number of videos per label in the training set:\")\n",
    "train_label_counts = df_train['label'].value_counts().sort_index().to_dict()\n",
    "for label_id, count in train_label_counts.items():\n",
    "    print(f\"{INV_LABEL_MAP[label_id]} ({label_id}): {count} videos\")\n",
    "\n",
    "print(\"\\nBalanced number of videos per label in the validation set:\")\n",
    "val_label_counts = df_test['label'].value_counts().sort_index().to_dict()\n",
    "for label_id, count in val_label_counts.items():\n",
    "    print(f\"{INV_LABEL_MAP[label_id]} ({label_id}): {count} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label_frame_counts, label_video_counts\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Run frame distribution analysis\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m label_frame_counts, label_video_counts \u001b[38;5;241m=\u001b[39m analyze_frame_distribution()\n",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m, in \u001b[0;36manalyze_frame_distribution\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     container \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mopen(video_path)\n\u001b[0;32m     14\u001b[0m     total_frames \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mvideo[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mframes\n\u001b[0;32m     15\u001b[0m     container\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Analyze frame distribution\n",
    "def analyze_frame_distribution():\n",
    "    \"\"\"Analyze frame distribution across labels to understand dataset imbalance.\"\"\"\n",
    "    label_frame_counts = {}\n",
    "    label_video_counts = {}\n",
    "    problematic_videos = []\n",
    "    \n",
    "    for _, row in df_train.iterrows():\n",
    "        video_path = row['flow_video_path']\n",
    "        label = row['label']\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            if label not in label_frame_counts:\n",
    "                label_frame_counts[label] = 0\n",
    "                label_video_counts[label] = 0\n",
    "                \n",
    "            label_frame_counts[label] += total_frames\n",
    "            label_video_counts[label] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            problematic_videos.append((video_path, str(e)))\n",
    "            print(f\"Error processing video {video_path}: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Total frames per label in training set:\")\n",
    "    for label, count in label_frame_counts.items():\n",
    "        label_name = INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label\n",
    "        avg_frames = count / label_video_counts[label] if label_video_counts[label] > 0 else 0\n",
    "        print(f\"{label_name}: {count} frames across {label_video_counts[label]} videos (avg: {avg_frames:.2f} frames/video)\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'Label': [INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label for label in label_frame_counts.keys()],\n",
    "        'Total Frames': label_frame_counts.values(),\n",
    "        'Video Count': [label_video_counts[label] for label in label_frame_counts.keys()],\n",
    "        'Avg Frames/Video': [label_frame_counts[label]/label_video_counts[label] \n",
    "                             if label_video_counts[label] > 0 else 0 \n",
    "                             for label in label_frame_counts.keys()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataFrame of training results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Now analyze test set\n",
    "    label_frame_counts_test = {}\n",
    "    label_video_counts_test = {}\n",
    "    \n",
    "    for _, row in df_test.iterrows():\n",
    "        video_path = row['flow_video_path']\n",
    "        label = row['label']\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            if label not in label_frame_counts_test:\n",
    "                label_frame_counts_test[label] = 0\n",
    "                label_video_counts_test[label] = 0\n",
    "                \n",
    "            label_frame_counts_test[label] += total_frames\n",
    "            label_video_counts_test[label] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test video {video_path}: {e}\")\n",
    "    \n",
    "    print(\"\\nTotal frames per label in test set:\")\n",
    "    for label, count in label_frame_counts_test.items():\n",
    "        label_name = INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label\n",
    "        avg_frames = count / label_video_counts_test[label] if label_video_counts_test[label] > 0 else 0\n",
    "        print(f\"{label_name}: {count} frames across {label_video_counts_test[label]} videos (avg: {avg_frames:.2f} frames/video)\")\n",
    "    \n",
    "    results_df_test = pd.DataFrame({\n",
    "        'Label': [INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label for label in label_frame_counts_test.keys()],\n",
    "        'Total Frames': label_frame_counts_test.values(),\n",
    "        'Video Count': [label_video_counts_test[label] for label in label_frame_counts_test.keys()],\n",
    "        'Avg Frames/Video': [label_frame_counts_test[label]/label_video_counts_test[label] \n",
    "                             if label_video_counts_test[label] > 0 else 0 \n",
    "                             for label in label_frame_counts_test.keys()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataFrame of test results:\")\n",
    "    print(results_df_test)\n",
    "    \n",
    "    # Calculate total frames and ratio\n",
    "    total_train_frames = sum(label_frame_counts.values())\n",
    "    total_test_frames = sum(label_frame_counts_test.values())\n",
    "    total_frames = total_train_frames + total_test_frames\n",
    "    \n",
    "    print(f\"\\nTotal frames - Train: {total_train_frames}, Test: {total_test_frames}\")\n",
    "    print(f\"Train-Test ratio: {total_train_frames/total_frames:.2f}:{total_test_frames/total_frames:.2f}\")\n",
    "    \n",
    "    if problematic_videos:\n",
    "        print(f\"\\nFound {len(problematic_videos)} problematic videos. Check logs for details.\")\n",
    "        with open(\"problematic_videos.log\", \"w\") as f:\n",
    "            for path, error in problematic_videos:\n",
    "                f.write(f\"{path}: {error}\\n\")\n",
    "    \n",
    "    return label_frame_counts, label_video_counts\n",
    "\n",
    "# Run frame distribution analysis\n",
    "label_frame_counts, label_video_counts = analyze_frame_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data augmentation with strong augmentation option\n",
    "def read_video_pyav(container, indices, is_training=True, strong_augment=False):\n",
    "    \"\"\"Read video frames with appropriate augmentation.\"\"\"\n",
    "    if is_training:\n",
    "        if strong_augment:\n",
    "            # Stronger augmentation for underrepresented classes\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "                T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                T.RandomRotation(30),\n",
    "                T.RandomHorizontalFlip(p=0.7),\n",
    "                T.RandomVerticalFlip(p=0.3),\n",
    "                T.RandomGrayscale(p=0.2),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            # Regular augmentation\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                T.RandomRotation(15),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "    else:\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    try:\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i in indices:\n",
    "                frame = transform(frame.to_image())\n",
    "                frames.append(frame)\n",
    "            if len(frames) == len(indices):\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error during frame decoding: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not frames:\n",
    "        return None\n",
    "        \n",
    "    return torch.stack(frames)\n",
    "\n",
    "def split_into_chunks(total_frames, clip_len, overlap=0):\n",
    "    \"\"\"Split video into overlapping chunks.\"\"\"\n",
    "    step = clip_len - overlap\n",
    "    chunks = [(start, start + clip_len) for start in range(0, total_frames, step) if start + clip_len <= total_frames]\n",
    "    return chunks\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, clip_len=32, frame_sample_rate=1, \n",
    "                 target_frames=128, overlap=0, is_training=True, video_path_column='flow_video_path',\n",
    "                 max_frames_per_label=MAX_FRAME):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_sample_rate = frame_sample_rate\n",
    "        self.target_frames = target_frames\n",
    "        self.overlap = overlap\n",
    "        self.is_training = is_training\n",
    "        self.video_path_column = video_path_column\n",
    "        self.max_frames_per_label = max_frames_per_label\n",
    "        self.problematic_files = []  # Initialize before calling _prepare_data\n",
    "        self.data = self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare dataset by processing videos into clips.\"\"\"\n",
    "        prepared_data = []\n",
    "        short_clips = []\n",
    "        \n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            video_path = row[self.video_path_column]\n",
    "            label = int(row['label'])\n",
    "            try:\n",
    "                container = av.open(video_path)\n",
    "                total_frames = container.streams.video[0].frames\n",
    "                container.close()\n",
    "                \n",
    "                if total_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                    chunks = split_into_chunks(total_frames, self.clip_len * self.frame_sample_rate, self.overlap)\n",
    "                    for start, end in chunks:\n",
    "                        prepared_data.append({\n",
    "                            \"video_path\": video_path, \n",
    "                            \"label\": label, \n",
    "                            \"start\": start, \n",
    "                            \"end\": end,\n",
    "                            \"combined\": False\n",
    "                        })\n",
    "                else:\n",
    "                    short_clips.append({\n",
    "                        \"video_path\": video_path,\n",
    "                        \"label\": label,\n",
    "                        \"frames\": total_frames,\n",
    "                        \"combined\": True\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                self.problematic_files.append((video_path, str(e)))\n",
    "                print(f\"Error processing video {video_path}: {e}\")\n",
    "        \n",
    "        # Log problematic files\n",
    "        if self.problematic_files:\n",
    "            log_path = \"problematic_files_dataset.log\"\n",
    "            with open(log_path, \"w\") as f:\n",
    "                for path, error in self.problematic_files:\n",
    "                    f.write(f\"{path}: {error}\\n\")\n",
    "            print(f\"Logged {len(self.problematic_files)} problematic files to {log_path}\")\n",
    "        \n",
    "        combined_clips = self._combine_short_clips(short_clips)\n",
    "        prepared_data.extend(combined_clips)\n",
    "        \n",
    "        # Balance clips by frame count\n",
    "        if self.is_training:\n",
    "            prepared_data = self._balance_clips_by_label(prepared_data)\n",
    "        \n",
    "        print(f\"\\nTotal clips: {len(prepared_data)}, including {len(combined_clips)} combined clips\")\n",
    "        print(f\"Combined clips processing summary: {len(combined_clips)} clips created from {len(short_clips)} short clips, {len(short_clips) - sum([len(clip['video_paths']) for clip in combined_clips])} clips discarded.\")\n",
    "\n",
    "        return prepared_data\n",
    "    \n",
    "    def _balance_clips_by_label(self, clips):\n",
    "        \"\"\"Balance clips to ensure each label has max_frames_per_label frames.\"\"\"\n",
    "        label_groups = {}\n",
    "        label_frame_counts = {}\n",
    "    \n",
    "        # Calculate frame counts per clip and per label\n",
    "        for clip in clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in label_groups:\n",
    "                label_groups[label] = []\n",
    "                label_frame_counts[label] = 0\n",
    "            \n",
    "            # Calculate actual frame count for this clip\n",
    "            if not clip.get(\"combined\", False):\n",
    "                frame_count = (clip[\"end\"] - clip[\"start\"]) // self.frame_sample_rate\n",
    "            else:\n",
    "                frame_count = sum(clip.get(\"frames_per_clip\", [0])) // self.frame_sample_rate\n",
    "            \n",
    "            clip['frame_count'] = frame_count  # Store frame count directly in clip\n",
    "            label_groups[label].append(clip)\n",
    "            label_frame_counts[label] += frame_count\n",
    "    \n",
    "        balanced_clips = []\n",
    "    \n",
    "        for label, label_clips in label_groups.items():\n",
    "            current_frames = label_frame_counts[label]\n",
    "            \n",
    "            if current_frames > self.max_frames_per_label:\n",
    "                # Downsample strategy: keep largest clips first\n",
    "                sorted_clips = sorted(label_clips, key=lambda x: x['frame_count'], reverse=True)\n",
    "                accumulated = 0\n",
    "                selected_clips = []\n",
    "                \n",
    "                for clip in sorted_clips:\n",
    "                    if accumulated + clip['frame_count'] <= self.max_frames_per_label:\n",
    "                        selected_clips.append(clip)\n",
    "                        accumulated += clip['frame_count']\n",
    "                    elif accumulated == 0:  # Handle case where single clip exceeds max\n",
    "                        selected_clips.append(clip)\n",
    "                        accumulated += clip['frame_count']\n",
    "                        break\n",
    "                \n",
    "                balanced_clips.extend(selected_clips)\n",
    "                print(f\"Label {INV_LABEL_MAP[label]} ({label}): \"\n",
    "                      f\"Reduced from {current_frames} to {accumulated} frames\")\n",
    "    \n",
    "            else:\n",
    "                # Upsample strategy: augment until reaching max_frames_per_label\n",
    "                balanced_clips.extend(label_clips)\n",
    "                accumulated = current_frames\n",
    "                deficit = self.max_frames_per_label - accumulated\n",
    "                \n",
    "                if deficit > 0:\n",
    "                    original_clips = label_clips.copy()\n",
    "                    while deficit > 0:\n",
    "                        for clip in original_clips:\n",
    "                            if deficit <= 0:\n",
    "                                break\n",
    "                                \n",
    "                            augmented_clip = clip.copy()\n",
    "                            augmented_clip[\"augment_strongly\"] = True\n",
    "                            balanced_clips.append(augmented_clip)\n",
    "                            deficit -= clip['frame_count']\n",
    "                            accumulated += clip['frame_count']\n",
    "    \n",
    "                    print(f\"Label {INV_LABEL_MAP[label]} ({label}): \"\n",
    "                          f\"Increased from {current_frames} to {accumulated - deficit} frames \"\n",
    "                          f\"(added {len(balanced_clips) - len(label_clips)} augmented clips)\")\n",
    "        \n",
    "        # Verification step to confirm balancing\n",
    "        final_label_frames = {}\n",
    "        for clip in balanced_clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in final_label_frames:\n",
    "                final_label_frames[label] = 0\n",
    "            final_label_frames[label] += clip['frame_count']\n",
    "        \n",
    "        print(\"\\nFinal frame count per label after balancing:\")\n",
    "        for label, count in final_label_frames.items():\n",
    "            print(f\"Label {INV_LABEL_MAP[label]} ({label}): {count} frames\")\n",
    "    \n",
    "        return balanced_clips\n",
    "\n",
    "    \n",
    "    def _combine_short_clips(self, short_clips):\n",
    "        \"\"\"Combine short clips to reach minimum length.\"\"\"\n",
    "        label_groups = {}\n",
    "        for clip in short_clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in label_groups:\n",
    "                label_groups[label] = []\n",
    "            label_groups[label].append(clip)\n",
    "        \n",
    "        combined_data = []\n",
    "        for label, clips in label_groups.items():\n",
    "            current_clips = []\n",
    "            current_frames = 0\n",
    "            \n",
    "            for clip in clips:\n",
    "                if current_frames + clip[\"frames\"] <= self.target_frames:\n",
    "                    current_clips.append(clip)\n",
    "                    current_frames += clip[\"frames\"]\n",
    "                    \n",
    "                    if current_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                        combined_data.append({\n",
    "                            \"video_paths\": [c[\"video_path\"] for c in current_clips],\n",
    "                            \"label\": label,\n",
    "                            \"frames_per_clip\": [c[\"frames\"] for c in current_clips],\n",
    "                            \"combined\": True\n",
    "                        })\n",
    "                        current_clips = []\n",
    "                        current_frames = 0\n",
    "            \n",
    "            if current_clips and current_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                combined_data.append({\n",
    "                    \"video_paths\": [c[\"video_path\"] for c in current_clips],\n",
    "                    \"label\": label,\n",
    "                    \"frames_per_clip\": [c[\"frames\"] for c in current_clips],\n",
    "                    \"combined\": True\n",
    "                })\n",
    "        \n",
    "        return combined_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        strong_augment = item.get(\"augment_strongly\", False)\n",
    "        \n",
    "        if not item.get(\"combined\", False):\n",
    "            video_path, label, start, end = item[\"video_path\"], item[\"label\"], item[\"start\"], item[\"end\"]\n",
    "            try:\n",
    "                container = av.open(video_path)\n",
    "                indices = list(range(start, end, self.frame_sample_rate))\n",
    "                if len(indices) != self.clip_len:\n",
    "                    container.close()\n",
    "                    return None\n",
    "                    \n",
    "                video = read_video_pyav(container, indices, self.is_training, strong_augment)\n",
    "                container.close()\n",
    "                \n",
    "                if video is None:\n",
    "                    return None\n",
    "                \n",
    "                # Check if frames are valid\n",
    "                if torch.isnan(video).any() or torch.isinf(video).any():\n",
    "                    print(f\"Invalid frame values detected in {video_path}\")\n",
    "                    return None\n",
    "                \n",
    "                # Add temporal dropout for additional augmentation when training\n",
    "                if self.is_training and np.random.random() < 0.3:\n",
    "                    # Randomly drop up to 10% of frames and repeat adjacent frames\n",
    "                    num_frames_to_drop = max(1, int(0.1 * video.shape[0]))\n",
    "                    frames_to_drop = np.random.choice(video.shape[0], num_frames_to_drop, replace=False)\n",
    "                    for idx in frames_to_drop:\n",
    "                        # Replace with previous or next frame\n",
    "                        replace_idx = max(0, idx-1) if idx > 0 else min(idx+1, video.shape[0]-1)\n",
    "                        video[idx] = video[replace_idx]\n",
    "                \n",
    "                inputs = self.processor(list(video.permute(0, 2, 3, 1).numpy()), return_tensors=\"pt\")\n",
    "                return {\"pixel_values\": inputs[\"pixel_values\"].squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading video chunk from {video_path}: {e}\")\n",
    "                self.problematic_files.append((video_path, str(e)))\n",
    "                return None\n",
    "        \n",
    "        else:\n",
    "            video_paths = item[\"video_paths\"]\n",
    "            frames_per_clip = item[\"frames_per_clip\"]\n",
    "            label = item[\"label\"]\n",
    "            \n",
    "            all_frames = []\n",
    "            current_frame_count = 0\n",
    "            \n",
    "            for i, video_path in enumerate(video_paths):\n",
    "                try:\n",
    "                    container = av.open(video_path)\n",
    "                    frames_needed = min(frames_per_clip[i], self.clip_len * self.frame_sample_rate - current_frame_count)\n",
    "                    indices = list(range(0, frames_needed))\n",
    "                    video = read_video_pyav(container, indices, self.is_training, strong_augment)\n",
    "                    container.close()\n",
    "                    \n",
    "                    if video is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if frames are valid\n",
    "                    if torch.isnan(video).any() or torch.isinf(video).any():\n",
    "                        print(f\"Invalid frame values detected in {video_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    all_frames.append(video)\n",
    "                    current_frame_count += frames_needed\n",
    "                    \n",
    "                    if current_frame_count >= self.clip_len * self.frame_sample_rate:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading combined video from {video_path}: {e}\")\n",
    "                    self.problematic_files.append((video_path, str(e)))\n",
    "            \n",
    "            if not all_frames or current_frame_count < self.clip_len * self.frame_sample_rate:\n",
    "                return None\n",
    "                \n",
    "            combined_video = torch.cat(all_frames, dim=0)\n",
    "            \n",
    "            if combined_video.shape[0] > self.clip_len:\n",
    "                combined_video = combined_video[:self.clip_len]\n",
    "                \n",
    "            inputs = self.processor(list(combined_video.permute(0, 2, 3, 1).numpy()), return_tensors=\"pt\")\n",
    "            return {\"pixel_values\": inputs[\"pixel_values\"].squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function that handles None values in batch.\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Fixed rescaling issue\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\", do_rescale=None, offset=None)\n",
    "model = VivitForVideoClassification.from_pretrained(\n",
    "    \"google/vivit-b-16x2\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    hidden_dropout_prob=0.1,  # Reduced from 0.2\n",
    "    attention_probs_dropout_prob=0.1  # Reduced from 0.2\n",
    ")\n",
    "\n",
    "# Initialize datasets and dataloaders with different max frames for train and test\n",
    "train_dataset = VideoDataset(df_train, processor, CLIP_LEN, FRAME_SAMPLE_RATE, \n",
    "                            target_frames=TARGET_FRAMES, overlap=16, is_training=True,\n",
    "                            max_frames_per_label=MAX_FRAME)\n",
    "test_dataset = VideoDataset(df_test, processor, CLIP_LEN, FRAME_SAMPLE_RATE, \n",
    "                           target_frames=TARGET_FRAMES, overlap=16, is_training=False,\n",
    "                           max_frames_per_label=MAX_TEST_FRAME)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, \n",
    "                             shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, \n",
    "                            shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Switch optimizer and adjust weight decay\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=1e-6  # Reduced from 1e-5\n",
    ")\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Calculate class weights for loss function\n",
    "train_labels = df_train[\"label\"].values\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Use label smoothing for better generalization\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": []\n",
    "}\n",
    "\n",
    "scaler = GradScaler()\n",
    "best_val_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    epoch_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", leave=True)\n",
    "    for step, batch in enumerate(epoch_progress_bar):\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"])\n",
    "            loss = loss_fn(outputs.logits, batch[\"labels\"]) / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            # Add this line to unscale gradients first\n",
    "            scaler.unscale_(optimizer)\n",
    "    \n",
    "            # Add gradient clipping here\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total_predictions += batch[\"labels\"].size(0)\n",
    "\n",
    "        epoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        epoch_progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}\",\n",
    "            \"Accuracy\": f\"{(correct_predictions / total_predictions) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    history[\"train_loss\"].append(avg_epoch_loss)\n",
    "    history[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Validation\"):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"], interpolate_pos_encoding=True)\n",
    "            val_loss += loss_fn(outputs.logits, batch[\"labels\"]).item()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            val_correct_predictions += (predictions == batch[\"labels\"]).sum().item()\n",
    "            val_total_predictions += batch[\"labels\"].size(0)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_model_acc.pt\"))\n",
    "        print(f\"New best model saved with validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save model at each epoch\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, f\"vivit_epoch_{epoch + 1}.pt\"))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Train Loss: {avg_epoch_loss:.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy * 100:.2f}%, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Create confusion matrix every epoch\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(LABEL_MAP.keys()))\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - Epoch {epoch + 1}\")\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f'confusion_matrix_epoch_{epoch + 1}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Training and Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'training_metrics.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ViVi",
   "language": "python",
   "name": "vivi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa280c088d9450abcf24174d16b3c60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae09b7163b6242249038eff8c54f1270",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a2e0189ae3574945b2336e7415fa7cc2",
      "value": " 96/96 [00:05&lt;00:00, 16.12 examples/s]"
     }
    },
    "111140a7c8834455aed97be4a5b2253e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33b69a8b67d54f20bbdb663d41a53c60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "3fbca77d0b4945b480bde77597109dd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "453888b3c6e9432dac50ba5c26d19842": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_111140a7c8834455aed97be4a5b2253e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5059041a1938466c8f11ae43c6e20c90",
      "value": " 960/960 [00:40&lt;00:00, 23.94 examples/s]"
     }
    },
    "5059041a1938466c8f11ae43c6e20c90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5132ae33e2a9420b855b48a1b3320d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dce3d31d53c441d4bf5cc6c2ae39dac7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_966d4e0db390439ea4f7e9243c07d4f4",
      "value": "Map: 100%"
     }
    },
    "780cedaa760a43b2ae99fe694e417126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1a94e8ac6634a5ab6f1782c42e1eac8",
      "max": 96,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fbca77d0b4945b480bde77597109dd2",
      "value": 96
     }
    },
    "81efd04ce230429d9c0cae00813d62a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966d4e0db390439ea4f7e9243c07d4f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a85f9a5d728459a929696169d3dcad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbfc602d75334b8886a4ca88e5cf3cb8",
      "max": 960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0d1bda1a0c74f1293eca52412632ec4",
      "value": 960
     }
    },
    "9fd242834e4841f6b50c54048be9e694": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e38dd76466024573b3868d2e2af0b730",
       "IPY_MODEL_780cedaa760a43b2ae99fe694e417126",
       "IPY_MODEL_0aa280c088d9450abcf24174d16b3c60"
      ],
      "layout": "IPY_MODEL_b63d2279bf974dc0bea8d889e98abc6a"
     }
    },
    "a2e0189ae3574945b2336e7415fa7cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8b703bbcdd440ee92c67f7abcb80eca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae09b7163b6242249038eff8c54f1270": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0d1bda1a0c74f1293eca52412632ec4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b63d2279bf974dc0bea8d889e98abc6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "bbfc602d75334b8886a4ca88e5cf3cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dce3d31d53c441d4bf5cc6c2ae39dac7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e38dd76466024573b3868d2e2af0b730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8b703bbcdd440ee92c67f7abcb80eca",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_81efd04ce230429d9c0cae00813d62a0",
      "value": "Map: 100%"
     }
    },
    "e96df79b4c4046cba35d84ac56d54295": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5132ae33e2a9420b855b48a1b3320d54",
       "IPY_MODEL_9a85f9a5d728459a929696169d3dcad5",
       "IPY_MODEL_453888b3c6e9432dac50ba5c26d19842"
      ],
      "layout": "IPY_MODEL_33b69a8b67d54f20bbdb663d41a53c60"
     }
    },
    "f1a94e8ac6634a5ab6f1782c42e1eac8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
