{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Fighting: Moving 4 videos (5760 frames) from test to train\n",
      "Label Riot: Moving 13 videos (18300 frames) from test to train\n",
      "\n",
      "Updated frame distribution:\n",
      "Label Normal: 9384 frames in test set\n",
      "Label Explosion: 8604 frames in test set\n",
      "Label Fighting: 9841 frames in test set\n",
      "Label Car Accident: 3861 frames in test set\n",
      "Label Shooting: 4422 frames in test set\n",
      "Label Riot: 8920 frames in test set\n",
      "\n",
      "Balanced number of videos per label in the training set:\n",
      "Normal (0): 110 videos\n",
      "Explosion (1): 106 videos\n",
      "Fighting (2): 114 videos\n",
      "Car Accident (3): 110 videos\n",
      "Shooting (4): 110 videos\n",
      "Riot (5): 123 videos\n",
      "\n",
      "Balanced number of videos per label in the validation set:\n",
      "Normal (0): 30 videos\n",
      "Explosion (1): 27 videos\n",
      "Fighting (2): 26 videos\n",
      "Car Accident (3): 30 videos\n",
      "Shooting (4): 30 videos\n",
      "Riot (5): 17 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11688\\775800491.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('label', group_keys=False).apply(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11688\\775800491.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('label', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import VivitForVideoClassification, VivitImageProcessor, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load and split the data\n",
    "df = pd.read_csv('E:/SRC-Bhuvaneswari/processed files/video/ftest/test_data.csv')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Save split datasets\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "TRAIN_CSV_PATH = 'train_data.csv'\n",
    "TEST_CSV_PATH = 'test_data.csv'\n",
    "SAVE_DIR = 'F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Define label mapping\n",
    "LABEL_MAP = {'Normal': 0, 'Explosion': 1, 'Fighting': 2, 'Car Accident': 3, 'Shooting': 4, 'Riot': 5}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# Define maximum clips per label - adjusted for full frame usage\n",
    "MAX_TRAIN_SAMPLES = 110\n",
    "MAX_TEST_SAMPLES = 30\n",
    "\n",
    "# Define maximum frames per label for 80-20 ratio\n",
    "# For a 80-20 ratio, if we assume each label should have equal representation\n",
    "MAX_FRAME = 40000  # 80% of frames per label\n",
    "MAX_TEST_FRAME = 10000  # 20% of frames per label\n",
    "\n",
    "# Improved hyperparameters\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAIN_BATCH_SIZE = 3  # Reduced to handle more frames\n",
    "EVAL_BATCH_SIZE = 3\n",
    "SEED = 24\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Increased for stability\n",
    "NUM_EPOCHS = 10\n",
    "CLIP_LEN = 32\n",
    "FRAME_SAMPLE_RATE = 1  # Changed to 1 to use all frames\n",
    "TARGET_FRAMES = 128\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "def load_data_from_csv(csv_path, video_path_column='rgb_video_path'):\n",
    "    \"\"\"Load and clean dataframe from CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=[video_path_column, 'label'])\n",
    "    return df\n",
    "\n",
    "df_train = load_data_from_csv(TRAIN_CSV_PATH)\n",
    "df_test = load_data_from_csv(TEST_CSV_PATH)\n",
    "\n",
    "def balance_dataset(df, max_samples):\n",
    "    \"\"\"Balance dataset to have equal number of samples per class.\"\"\"\n",
    "    return df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples), random_state=SEED)\n",
    "    )\n",
    "\n",
    "df_train = balance_dataset(df_train, MAX_TRAIN_SAMPLES)\n",
    "df_test = balance_dataset(df_test, MAX_TEST_SAMPLES)\n",
    "\n",
    "# Function to balance test and train sets based on frame count\n",
    "def balance_test_and_train_sets(df_train, df_test):\n",
    "    \"\"\"Balance test set to MAX_TEST_FRAME per label and move excess to training.\"\"\"\n",
    "    # Group test videos by label\n",
    "    test_videos_by_label = {label: [] for label in LABEL_MAP.values()}\n",
    "    test_frames_by_label = {label: 0 for label in LABEL_MAP.values()}\n",
    "    \n",
    "    # Count frames per label in test set\n",
    "    for _, row in df_test.iterrows():\n",
    "        video_path = row['rgb_video_path']\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            frame_count = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            test_videos_by_label[label].append((video_path, frame_count))\n",
    "            test_frames_by_label[label] += frame_count\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test video {video_path}: {e}\")\n",
    "    \n",
    "    # Move excess videos from test to train\n",
    "    videos_to_move = []\n",
    "    \n",
    "    for label, videos in test_videos_by_label.items():\n",
    "        if test_frames_by_label[label] > MAX_TEST_FRAME:\n",
    "            # Sort videos by frame count (optional - depends on your strategy)\n",
    "            videos.sort(key=lambda x: x[1])\n",
    "            \n",
    "            current_frames = 0\n",
    "            keep_idx = 0\n",
    "            \n",
    "            # Find how many videos to keep in test set\n",
    "            for i, (_, frame_count) in enumerate(videos):\n",
    "                if current_frames + frame_count <= MAX_TEST_FRAME:\n",
    "                    current_frames += frame_count\n",
    "                    keep_idx = i + 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Identify videos to move to training\n",
    "            videos_to_move.extend([path for path, _ in videos[keep_idx:]])\n",
    "            \n",
    "            print(f\"Label {INV_LABEL_MAP[label]}: Moving {len(videos) - keep_idx} videos \" \n",
    "                  f\"({test_frames_by_label[label] - current_frames} frames) from test to train\")\n",
    "            \n",
    "            # Update frame count\n",
    "            test_frames_by_label[label] = current_frames\n",
    "    \n",
    "    # Create masks for videos to keep in test and move to train\n",
    "    keep_mask = ~df_test['rgb_video_path'].isin(videos_to_move)\n",
    "    \n",
    "    # Extract videos to move\n",
    "    df_to_move = df_test[~keep_mask].copy()\n",
    "    \n",
    "    # Update test dataframe\n",
    "    df_test_balanced = df_test[keep_mask].copy()\n",
    "    \n",
    "    # Add moved videos to training\n",
    "    df_train_updated = pd.concat([df_train, df_to_move], ignore_index=True)\n",
    "    \n",
    "    print(\"\\nUpdated frame distribution:\")\n",
    "    for label, frames in test_frames_by_label.items():\n",
    "        print(f\"Label {INV_LABEL_MAP[label]}: {frames} frames in test set\")\n",
    "    \n",
    "    return df_train_updated, df_test_balanced\n",
    "\n",
    "# Apply the balancing function\n",
    "df_train, df_test = balance_test_and_train_sets(df_train, df_test)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nBalanced number of videos per label in the training set:\")\n",
    "train_label_counts = df_train['label'].value_counts().sort_index().to_dict()\n",
    "for label_id, count in train_label_counts.items():\n",
    "    print(f\"{INV_LABEL_MAP[label_id]} ({label_id}): {count} videos\")\n",
    "\n",
    "print(\"\\nBalanced number of videos per label in the validation set:\")\n",
    "val_label_counts = df_test['label'].value_counts().sort_index().to_dict()\n",
    "for label_id, count in val_label_counts.items():\n",
    "    print(f\"{INV_LABEL_MAP[label_id]} ({label_id}): {count} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames per label in training set:\n",
      "Normal: 28988 frames across 110 videos (avg: 263.53 frames/video)\n",
      "Explosion: 29586 frames across 106 videos (avg: 279.11 frames/video)\n",
      "Fighting: 57256 frames across 114 videos (avg: 502.25 frames/video)\n",
      "Car Accident: 15132 frames across 110 videos (avg: 137.56 frames/video)\n",
      "Shooting: 22851 frames across 110 videos (avg: 207.74 frames/video)\n",
      "Riot: 126027 frames across 123 videos (avg: 1024.61 frames/video)\n",
      "\n",
      "DataFrame of training results:\n",
      "          Label  Total Frames  Video Count  Avg Frames/Video\n",
      "0        Normal         28988          110        263.527273\n",
      "1     Explosion         29586          106        279.113208\n",
      "2      Fighting         57256          114        502.245614\n",
      "3  Car Accident         15132          110        137.563636\n",
      "4      Shooting         22851          110        207.736364\n",
      "5          Riot        126027          123       1024.609756\n",
      "\n",
      "Total frames per label in test set:\n",
      "Normal: 9384 frames across 30 videos (avg: 312.80 frames/video)\n",
      "Explosion: 8604 frames across 27 videos (avg: 318.67 frames/video)\n",
      "Fighting: 9841 frames across 26 videos (avg: 378.50 frames/video)\n",
      "Car Accident: 3861 frames across 30 videos (avg: 128.70 frames/video)\n",
      "Shooting: 4422 frames across 30 videos (avg: 147.40 frames/video)\n",
      "Riot: 8920 frames across 17 videos (avg: 524.71 frames/video)\n",
      "\n",
      "DataFrame of test results:\n",
      "          Label  Total Frames  Video Count  Avg Frames/Video\n",
      "0        Normal          9384           30        312.800000\n",
      "1     Explosion          8604           27        318.666667\n",
      "2      Fighting          9841           26        378.500000\n",
      "3  Car Accident          3861           30        128.700000\n",
      "4      Shooting          4422           30        147.400000\n",
      "5          Riot          8920           17        524.705882\n",
      "\n",
      "Total frames - Train: 279840, Test: 45032\n",
      "Train-Test ratio: 0.86:0.14\n"
     ]
    }
   ],
   "source": [
    "# Analyze frame distribution\n",
    "def analyze_frame_distribution():\n",
    "    \"\"\"Analyze frame distribution across labels to understand dataset imbalance.\"\"\"\n",
    "    label_frame_counts = {}\n",
    "    label_video_counts = {}\n",
    "    problematic_videos = []\n",
    "    \n",
    "    for _, row in df_train.iterrows():\n",
    "        video_path = row['rgb_video_path']\n",
    "        label = row['label']\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            if label not in label_frame_counts:\n",
    "                label_frame_counts[label] = 0\n",
    "                label_video_counts[label] = 0\n",
    "                \n",
    "            label_frame_counts[label] += total_frames\n",
    "            label_video_counts[label] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            problematic_videos.append((video_path, str(e)))\n",
    "            print(f\"Error processing video {video_path}: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Total frames per label in training set:\")\n",
    "    for label, count in label_frame_counts.items():\n",
    "        label_name = INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label\n",
    "        avg_frames = count / label_video_counts[label] if label_video_counts[label] > 0 else 0\n",
    "        print(f\"{label_name}: {count} frames across {label_video_counts[label]} videos (avg: {avg_frames:.2f} frames/video)\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'Label': [INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label for label in label_frame_counts.keys()],\n",
    "        'Total Frames': label_frame_counts.values(),\n",
    "        'Video Count': [label_video_counts[label] for label in label_frame_counts.keys()],\n",
    "        'Avg Frames/Video': [label_frame_counts[label]/label_video_counts[label] \n",
    "                             if label_video_counts[label] > 0 else 0 \n",
    "                             for label in label_frame_counts.keys()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataFrame of training results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Now analyze test set\n",
    "    label_frame_counts_test = {}\n",
    "    label_video_counts_test = {}\n",
    "    \n",
    "    for _, row in df_test.iterrows():\n",
    "        video_path = row['rgb_video_path']\n",
    "        label = row['label']\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            if label not in label_frame_counts_test:\n",
    "                label_frame_counts_test[label] = 0\n",
    "                label_video_counts_test[label] = 0\n",
    "                \n",
    "            label_frame_counts_test[label] += total_frames\n",
    "            label_video_counts_test[label] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test video {video_path}: {e}\")\n",
    "    \n",
    "    print(\"\\nTotal frames per label in test set:\")\n",
    "    for label, count in label_frame_counts_test.items():\n",
    "        label_name = INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label\n",
    "        avg_frames = count / label_video_counts_test[label] if label_video_counts_test[label] > 0 else 0\n",
    "        print(f\"{label_name}: {count} frames across {label_video_counts_test[label]} videos (avg: {avg_frames:.2f} frames/video)\")\n",
    "    \n",
    "    results_df_test = pd.DataFrame({\n",
    "        'Label': [INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label for label in label_frame_counts_test.keys()],\n",
    "        'Total Frames': label_frame_counts_test.values(),\n",
    "        'Video Count': [label_video_counts_test[label] for label in label_frame_counts_test.keys()],\n",
    "        'Avg Frames/Video': [label_frame_counts_test[label]/label_video_counts_test[label] \n",
    "                             if label_video_counts_test[label] > 0 else 0 \n",
    "                             for label in label_frame_counts_test.keys()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataFrame of test results:\")\n",
    "    print(results_df_test)\n",
    "    \n",
    "    # Calculate total frames and ratio\n",
    "    total_train_frames = sum(label_frame_counts.values())\n",
    "    total_test_frames = sum(label_frame_counts_test.values())\n",
    "    total_frames = total_train_frames + total_test_frames\n",
    "    \n",
    "    print(f\"\\nTotal frames - Train: {total_train_frames}, Test: {total_test_frames}\")\n",
    "    print(f\"Train-Test ratio: {total_train_frames/total_frames:.2f}:{total_test_frames/total_frames:.2f}\")\n",
    "    \n",
    "    if problematic_videos:\n",
    "        print(f\"\\nFound {len(problematic_videos)} problematic videos. Check logs for details.\")\n",
    "        with open(\"problematic_videos.log\", \"w\") as f:\n",
    "            for path, error in problematic_videos:\n",
    "                f.write(f\"{path}: {error}\\n\")\n",
    "    \n",
    "    return label_frame_counts, label_video_counts\n",
    "\n",
    "# Run frame distribution analysis\n",
    "label_frame_counts, label_video_counts = analyze_frame_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data augmentation with strong augmentation option\n",
    "def read_video_pyav(container, indices, is_training=True, strong_augment=False):\n",
    "    \"\"\"Read video frames with appropriate augmentation.\"\"\"\n",
    "    if is_training:\n",
    "        if strong_augment:\n",
    "            # Stronger augmentation for underrepresented classes\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "                T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                T.RandomRotation(30),\n",
    "                T.RandomHorizontalFlip(p=0.7),\n",
    "                T.RandomVerticalFlip(p=0.3),\n",
    "                T.RandomGrayscale(p=0.2),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            # Regular augmentation\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                T.RandomRotation(15),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "    else:\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    try:\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i in indices:\n",
    "                frame = transform(frame.to_image())\n",
    "                frames.append(frame)\n",
    "            if len(frames) == len(indices):\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error during frame decoding: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not frames:\n",
    "        return None\n",
    "        \n",
    "    return torch.stack(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(total_frames, clip_len, overlap=0):\n",
    "    \"\"\"Split video into overlapping chunks.\"\"\"\n",
    "    step = clip_len - overlap\n",
    "    chunks = [(start, start + clip_len) for start in range(0, total_frames, step) if start + clip_len <= total_frames]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, clip_len=32, frame_sample_rate=1, \n",
    "                 target_frames=128, overlap=0, is_training=True, video_path_column='rgb_video_path',\n",
    "                 max_frames_per_label=MAX_FRAME):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_sample_rate = frame_sample_rate\n",
    "        self.target_frames = target_frames\n",
    "        self.overlap = overlap\n",
    "        self.is_training = is_training\n",
    "        self.video_path_column = video_path_column\n",
    "        self.max_frames_per_label = max_frames_per_label\n",
    "        self.problematic_files = []  # Initialize before calling _prepare_data\n",
    "        self.data = self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare dataset by processing videos into clips.\"\"\"\n",
    "        prepared_data = []\n",
    "        short_clips = []\n",
    "        \n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            video_path = row[self.video_path_column]\n",
    "            label = int(row['label'])\n",
    "            try:\n",
    "                container = av.open(video_path)\n",
    "                total_frames = container.streams.video[0].frames\n",
    "                container.close()\n",
    "                \n",
    "                if total_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                    chunks = split_into_chunks(total_frames, self.clip_len * self.frame_sample_rate, self.overlap)\n",
    "                    for start, end in chunks:\n",
    "                        prepared_data.append({\n",
    "                            \"video_path\": video_path, \n",
    "                            \"label\": label, \n",
    "                            \"start\": start, \n",
    "                            \"end\": end,\n",
    "                            \"combined\": False\n",
    "                        })\n",
    "                else:\n",
    "                    short_clips.append({\n",
    "                        \"video_path\": video_path,\n",
    "                        \"label\": label,\n",
    "                        \"frames\": total_frames,\n",
    "                        \"combined\": True\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                self.problematic_files.append((video_path, str(e)))\n",
    "                print(f\"Error processing video {video_path}: {e}\")\n",
    "        \n",
    "        # Log problematic files\n",
    "        if self.problematic_files:\n",
    "            log_path = \"problematic_files_dataset.log\"\n",
    "            with open(log_path, \"w\") as f:\n",
    "                for path, error in self.problematic_files:\n",
    "                    f.write(f\"{path}: {error}\\n\")\n",
    "            print(f\"Logged {len(self.problematic_files)} problematic files to {log_path}\")\n",
    "        \n",
    "        combined_clips = self._combine_short_clips(short_clips)\n",
    "        prepared_data.extend(combined_clips)\n",
    "        \n",
    "        # Balance clips by frame count\n",
    "        if self.is_training:\n",
    "            prepared_data = self._balance_clips_by_label(prepared_data)\n",
    "        \n",
    "        print(f\"\\nTotal clips: {len(prepared_data)}, including {len(combined_clips)} combined clips\")\n",
    "        print(f\"Combined clips processing summary: {len(combined_clips)} clips created from {len(short_clips)} short clips, {len(short_clips) - sum([len(clip['video_paths']) for clip in combined_clips])} clips discarded.\")\n",
    "\n",
    "        return prepared_data\n",
    "    \n",
    "    def _balance_clips_by_label(self, clips):\n",
    "        \"\"\"Balance clips to ensure each label has max_frames_per_label frames.\"\"\"\n",
    "        label_groups = {}\n",
    "        label_frame_counts = {}\n",
    "    \n",
    "        # Calculate frame counts per clip and per label\n",
    "        for clip in clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in label_groups:\n",
    "                label_groups[label] = []\n",
    "                label_frame_counts[label] = 0\n",
    "            \n",
    "            # Calculate actual frame count for this clip\n",
    "            if not clip.get(\"combined\", False):\n",
    "                frame_count = (clip[\"end\"] - clip[\"start\"]) // self.frame_sample_rate\n",
    "            else:\n",
    "                frame_count = sum(clip.get(\"frames_per_clip\", [0])) // self.frame_sample_rate\n",
    "            \n",
    "            clip['frame_count'] = frame_count  # Store frame count directly in clip\n",
    "            label_groups[label].append(clip)\n",
    "            label_frame_counts[label] += frame_count\n",
    "    \n",
    "        balanced_clips = []\n",
    "    \n",
    "        for label, label_clips in label_groups.items():\n",
    "            current_frames = label_frame_counts[label]\n",
    "            \n",
    "            if current_frames > self.max_frames_per_label:\n",
    "                # Downsample strategy: keep largest clips first\n",
    "                sorted_clips = sorted(label_clips, key=lambda x: x['frame_count'], reverse=True)\n",
    "                accumulated = 0\n",
    "                selected_clips = []\n",
    "                \n",
    "                for clip in sorted_clips:\n",
    "                    if accumulated + clip['frame_count'] <= self.max_frames_per_label:\n",
    "                        selected_clips.append(clip)\n",
    "                        accumulated += clip['frame_count']\n",
    "                    elif accumulated == 0:  # Handle case where single clip exceeds max\n",
    "                        selected_clips.append(clip)\n",
    "                        accumulated += clip['frame_count']\n",
    "                        break\n",
    "                \n",
    "                balanced_clips.extend(selected_clips)\n",
    "                print(f\"Label {INV_LABEL_MAP[label]} ({label}): \"\n",
    "                      f\"Reduced from {current_frames} to {accumulated} frames\")\n",
    "    \n",
    "            else:\n",
    "                # Upsample strategy: augment until reaching max_frames_per_label\n",
    "                balanced_clips.extend(label_clips)\n",
    "                accumulated = current_frames\n",
    "                deficit = self.max_frames_per_label - accumulated\n",
    "                \n",
    "                if deficit > 0:\n",
    "                    original_clips = label_clips.copy()\n",
    "                    while deficit > 0:\n",
    "                        for clip in original_clips:\n",
    "                            if deficit <= 0:\n",
    "                                break\n",
    "                                \n",
    "                            augmented_clip = clip.copy()\n",
    "                            augmented_clip[\"augment_strongly\"] = True\n",
    "                            balanced_clips.append(augmented_clip)\n",
    "                            deficit -= clip['frame_count']\n",
    "                            accumulated += clip['frame_count']\n",
    "    \n",
    "                    print(f\"Label {INV_LABEL_MAP[label]} ({label}): \"\n",
    "                          f\"Increased from {current_frames} to {accumulated - deficit} frames \"\n",
    "                          f\"(added {len(balanced_clips) - len(label_clips)} augmented clips)\")\n",
    "        \n",
    "        # Verification step to confirm balancing\n",
    "        final_label_frames = {}\n",
    "        for clip in balanced_clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in final_label_frames:\n",
    "                final_label_frames[label] = 0\n",
    "            final_label_frames[label] += clip['frame_count']\n",
    "        \n",
    "        print(\"\\nFinal frame count per label after balancing:\")\n",
    "        for label, count in final_label_frames.items():\n",
    "            print(f\"Label {INV_LABEL_MAP[label]} ({label}): {count} frames\")\n",
    "    \n",
    "        return balanced_clips\n",
    "\n",
    "    \n",
    "    def _combine_short_clips(self, short_clips):\n",
    "        \"\"\"Combine short clips to reach minimum length.\"\"\"\n",
    "        label_groups = {}\n",
    "        for clip in short_clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in label_groups:\n",
    "                label_groups[label] = []\n",
    "            label_groups[label].append(clip)\n",
    "        \n",
    "        combined_data = []\n",
    "        for label, clips in label_groups.items():\n",
    "            current_clips = []\n",
    "            current_frames = 0\n",
    "            \n",
    "            for clip in clips:\n",
    "                if current_frames + clip[\"frames\"] <= self.target_frames:\n",
    "                    current_clips.append(clip)\n",
    "                    current_frames += clip[\"frames\"]\n",
    "                    \n",
    "                    if current_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                        combined_data.append({\n",
    "                            \"video_paths\": [c[\"video_path\"] for c in current_clips],\n",
    "                            \"label\": label,\n",
    "                            \"frames_per_clip\": [c[\"frames\"] for c in current_clips],\n",
    "                            \"combined\": True\n",
    "                        })\n",
    "                        current_clips = []\n",
    "                        current_frames = 0\n",
    "            \n",
    "            if current_clips and current_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                combined_data.append({\n",
    "                    \"video_paths\": [c[\"video_path\"] for c in current_clips],\n",
    "                    \"label\": label,\n",
    "                    \"frames_per_clip\": [c[\"frames\"] for c in current_clips],\n",
    "                    \"combined\": True\n",
    "                })\n",
    "        \n",
    "        return combined_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        strong_augment = item.get(\"augment_strongly\", False)\n",
    "        \n",
    "        if not item.get(\"combined\", False):\n",
    "            video_path, label, start, end = item[\"video_path\"], item[\"label\"], item[\"start\"], item[\"end\"]\n",
    "            try:\n",
    "                container = av.open(video_path)\n",
    "                indices = list(range(start, end, self.frame_sample_rate))\n",
    "                if len(indices) != self.clip_len:\n",
    "                    container.close()\n",
    "                    return None\n",
    "                    \n",
    "                video = read_video_pyav(container, indices, self.is_training, strong_augment)\n",
    "                container.close()\n",
    "                \n",
    "                if video is None:\n",
    "                    return None\n",
    "                \n",
    "                # Check if frames are valid\n",
    "                if torch.isnan(video).any() or torch.isinf(video).any():\n",
    "                    print(f\"Invalid frame values detected in {video_path}\")\n",
    "                    return None\n",
    "                \n",
    "                # Add temporal dropout for additional augmentation when training\n",
    "                if self.is_training and np.random.random() < 0.3:\n",
    "                    # Randomly drop up to 10% of frames and repeat adjacent frames\n",
    "                    num_frames_to_drop = max(1, int(0.1 * video.shape[0]))\n",
    "                    frames_to_drop = np.random.choice(video.shape[0], num_frames_to_drop, replace=False)\n",
    "                    for idx in frames_to_drop:\n",
    "                        # Replace with previous or next frame\n",
    "                        replace_idx = max(0, idx-1) if idx > 0 else min(idx+1, video.shape[0]-1)\n",
    "                        video[idx] = video[replace_idx]\n",
    "                \n",
    "                inputs = self.processor(list(video.permute(0, 2, 3, 1).numpy()), return_tensors=\"pt\")\n",
    "                return {\"pixel_values\": inputs[\"pixel_values\"].squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading video chunk from {video_path}: {e}\")\n",
    "                self.problematic_files.append((video_path, str(e)))\n",
    "                return None\n",
    "        \n",
    "        else:\n",
    "            video_paths = item[\"video_paths\"]\n",
    "            frames_per_clip = item[\"frames_per_clip\"]\n",
    "            label = item[\"label\"]\n",
    "            \n",
    "            all_frames = []\n",
    "            current_frame_count = 0\n",
    "            \n",
    "            for i, video_path in enumerate(video_paths):\n",
    "                try:\n",
    "                    container = av.open(video_path)\n",
    "                    frames_needed = min(frames_per_clip[i], self.clip_len * self.frame_sample_rate - current_frame_count)\n",
    "                    indices = list(range(0, frames_needed))\n",
    "                    video = read_video_pyav(container, indices, self.is_training, strong_augment)\n",
    "                    container.close()\n",
    "                    \n",
    "                    if video is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if frames are valid\n",
    "                    if torch.isnan(video).any() or torch.isinf(video).any():\n",
    "                        print(f\"Invalid frame values detected in {video_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    all_frames.append(video)\n",
    "                    current_frame_count += frames_needed\n",
    "                    \n",
    "                    if current_frame_count >= self.clip_len * self.frame_sample_rate:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading combined video from {video_path}: {e}\")\n",
    "                    self.problematic_files.append((video_path, str(e)))\n",
    "            \n",
    "            if not all_frames or current_frame_count < self.clip_len * self.frame_sample_rate:\n",
    "                return None\n",
    "                \n",
    "            combined_video = torch.cat(all_frames, dim=0)\n",
    "            \n",
    "            if combined_video.shape[0] > self.clip_len:\n",
    "                combined_video = combined_video[:self.clip_len]\n",
    "                \n",
    "            inputs = self.processor(list(combined_video.permute(0, 2, 3, 1).numpy()), return_tensors=\"pt\")\n",
    "            return {\"pixel_values\": inputs[\"pixel_values\"].squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function that handles None values in batch.\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Normal (0): Reduced from 53041 to 39985 frames\n",
      "Label Explosion (1): Reduced from 54426 to 39994 frames\n",
      "Label Fighting (2): Reduced from 109427 to 39987 frames\n",
      "Label Car Accident (3): Increased from 25293 to 40026 frames (added 4205 augmented clips)\n",
      "Label Shooting (4): Reduced from 40820 to 39988 frames\n",
      "Label Riot (5): Reduced from 247072 to 40000 frames\n",
      "\n",
      "Final frame count per label after balancing:\n",
      "Label Normal (0): 39985 frames\n",
      "Label Explosion (1): 39994 frames\n",
      "Label Fighting (2): 39987 frames\n",
      "Label Car Accident (3): 40013 frames\n",
      "Label Shooting (4): 39988 frames\n",
      "Label Riot (5): 40000 frames\n",
      "\n",
      "Total clips: 7489, including 19 combined clips\n",
      "Combined clips processing summary: 19 clips created from 42 short clips, 3 clips discarded.\n",
      "\n",
      "Total clips: 2585, including 3 combined clips\n",
      "Combined clips processing summary: 3 clips created from 8 short clips, 1 clips discarded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11688\\2650921144.py:59: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Fixed rescaling issue\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\", do_rescale=True)\n",
    "model = VivitForVideoClassification.from_pretrained(\n",
    "    \"google/vivit-b-16x2\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    attention_probs_dropout_prob=0.2\n",
    ")\n",
    "\n",
    "# Initialize datasets and dataloaders with different max frames for train and test\n",
    "train_dataset = VideoDataset(df_train, processor, CLIP_LEN, FRAME_SAMPLE_RATE, \n",
    "                            target_frames=TARGET_FRAMES, overlap=16, is_training=True,\n",
    "                            max_frames_per_label=MAX_FRAME)\n",
    "test_dataset = VideoDataset(df_test, processor, CLIP_LEN, FRAME_SAMPLE_RATE, \n",
    "                           target_frames=TARGET_FRAMES, overlap=16, is_training=False,\n",
    "                           max_frames_per_label=MAX_TEST_FRAME)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, \n",
    "                             shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, \n",
    "                            shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Calculate class weights for loss function\n",
    "train_labels = df_train[\"label\"].values\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Use label smoothing for better generalization\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": []\n",
    "}\n",
    "\n",
    "scaler = GradScaler()\n",
    "best_val_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Epoch 1/10: 100%|██████████████████████████████████| 2497/2497 [3:19:04<00:00,  4.78s/it, Loss=1.8050, Accuracy=16.01%]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 862/862 [25:38<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 20.62%\n",
      "Epoch 1 completed. Train Loss: 1.8171, Train Accuracy: 16.01%, Val Loss: 1.8007, Val Accuracy: 20.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████████████████████████████| 2497/2497 [3:17:30<00:00,  4.75s/it, Loss=1.7817, Accuracy=17.00%]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 862/862 [25:52<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation accuracy: 21.20%\n",
      "Epoch 2 completed. Train Loss: 1.8080, Train Accuracy: 17.00%, Val Loss: 1.7749, Val Accuracy: 21.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████████████████████████████| 2497/2497 [3:18:21<00:00,  4.77s/it, Loss=1.7452, Accuracy=16.04%]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 862/862 [25:35<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Train Loss: 1.8067, Train Accuracy: 16.04%, Val Loss: 1.7915, Val Accuracy: 20.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████████████████████████████| 2497/2497 [3:18:30<00:00,  4.77s/it, Loss=1.8038, Accuracy=16.58%]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 862/862 [25:56<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Train Loss: 1.8012, Train Accuracy: 16.58%, Val Loss: 1.8148, Val Accuracy: 7.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  68%|█████████████████████▋          | 1693/2497 [2:14:21<1:03:48,  4.76s/it, Loss=1.8168, Accuracy=16.16%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     27\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m==\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     29\u001b[0m total_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m GRADIENT_ACCUMULATION_STEPS\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    epoch_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", leave=True)\n",
    "    for step, batch in enumerate(epoch_progress_bar):\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"])\n",
    "            loss = loss_fn(outputs.logits, batch[\"labels\"]) / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total_predictions += batch[\"labels\"].size(0)\n",
    "\n",
    "        epoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        epoch_progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}\",\n",
    "            \"Accuracy\": f\"{(correct_predictions / total_predictions) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    history[\"train_loss\"].append(avg_epoch_loss)\n",
    "    history[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Validation\"):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"], interpolate_pos_encoding=True)\n",
    "            val_loss += loss_fn(outputs.logits, batch[\"labels\"]).item()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            val_correct_predictions += (predictions == batch[\"labels\"]).sum().item()\n",
    "            val_total_predictions += batch[\"labels\"].size(0)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_model_acc.pt\"))\n",
    "        print(f\"New best model saved with validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save model at each epoch\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, f\"vivit_epoch_{epoch + 1}.pt\"))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Train Loss: {avg_epoch_loss:.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy * 100:.2f}%, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Create confusion matrix every epoch\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(LABEL_MAP.keys()))\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - Epoch {epoch + 1}\")\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f'confusion_matrix_epoch_{epoch + 1}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Training and Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'training_metrics.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ViVi",
   "language": "python",
   "name": "vivi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa280c088d9450abcf24174d16b3c60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae09b7163b6242249038eff8c54f1270",
      "placeholder": "​",
      "style": "IPY_MODEL_a2e0189ae3574945b2336e7415fa7cc2",
      "value": " 96/96 [00:05&lt;00:00, 16.12 examples/s]"
     }
    },
    "111140a7c8834455aed97be4a5b2253e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33b69a8b67d54f20bbdb663d41a53c60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "3fbca77d0b4945b480bde77597109dd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "453888b3c6e9432dac50ba5c26d19842": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_111140a7c8834455aed97be4a5b2253e",
      "placeholder": "​",
      "style": "IPY_MODEL_5059041a1938466c8f11ae43c6e20c90",
      "value": " 960/960 [00:40&lt;00:00, 23.94 examples/s]"
     }
    },
    "5059041a1938466c8f11ae43c6e20c90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5132ae33e2a9420b855b48a1b3320d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dce3d31d53c441d4bf5cc6c2ae39dac7",
      "placeholder": "​",
      "style": "IPY_MODEL_966d4e0db390439ea4f7e9243c07d4f4",
      "value": "Map: 100%"
     }
    },
    "780cedaa760a43b2ae99fe694e417126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1a94e8ac6634a5ab6f1782c42e1eac8",
      "max": 96,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fbca77d0b4945b480bde77597109dd2",
      "value": 96
     }
    },
    "81efd04ce230429d9c0cae00813d62a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966d4e0db390439ea4f7e9243c07d4f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a85f9a5d728459a929696169d3dcad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbfc602d75334b8886a4ca88e5cf3cb8",
      "max": 960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0d1bda1a0c74f1293eca52412632ec4",
      "value": 960
     }
    },
    "9fd242834e4841f6b50c54048be9e694": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e38dd76466024573b3868d2e2af0b730",
       "IPY_MODEL_780cedaa760a43b2ae99fe694e417126",
       "IPY_MODEL_0aa280c088d9450abcf24174d16b3c60"
      ],
      "layout": "IPY_MODEL_b63d2279bf974dc0bea8d889e98abc6a"
     }
    },
    "a2e0189ae3574945b2336e7415fa7cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8b703bbcdd440ee92c67f7abcb80eca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae09b7163b6242249038eff8c54f1270": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0d1bda1a0c74f1293eca52412632ec4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b63d2279bf974dc0bea8d889e98abc6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "bbfc602d75334b8886a4ca88e5cf3cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dce3d31d53c441d4bf5cc6c2ae39dac7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e38dd76466024573b3868d2e2af0b730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8b703bbcdd440ee92c67f7abcb80eca",
      "placeholder": "​",
      "style": "IPY_MODEL_81efd04ce230429d9c0cae00813d62a0",
      "value": "Map: 100%"
     }
    },
    "e96df79b4c4046cba35d84ac56d54295": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5132ae33e2a9420b855b48a1b3320d54",
       "IPY_MODEL_9a85f9a5d728459a929696169d3dcad5",
       "IPY_MODEL_453888b3c6e9432dac50ba5c26d19842"
      ],
      "layout": "IPY_MODEL_33b69a8b67d54f20bbdb663d41a53c60"
     }
    },
    "f1a94e8ac6634a5ab6f1782c42e1eac8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
