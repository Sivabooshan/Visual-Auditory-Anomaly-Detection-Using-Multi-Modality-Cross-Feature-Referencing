{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import VivitForVideoClassification, VivitImageProcessor, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "\n",
    "# Create a custom iterator that handles dictionary batches\n",
    "class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "    def inputs_labels_from_batch(self, batch_data):\n",
    "        if batch_data is None:\n",
    "            # If batch is None, return empty tensors\n",
    "            return torch.tensor([]), torch.tensor([])\n",
    "        # Extract pixel_values and labels from the dictionary batch\n",
    "        inputs = batch_data[\"pixel_values\"]\n",
    "        labels = batch_data[\"labels\"]\n",
    "        return inputs, labels\n",
    "\n",
    "# Load and split the data\n",
    "df = pd.read_csv('E:/SRC-Bhuvaneswari/processed files/video/ftest/test_data.csv')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Save split datasets\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "TRAIN_CSV_PATH = 'train_data.csv'\n",
    "TEST_CSV_PATH = 'test_data.csv'\n",
    "SAVE_DIR = 'F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/v2.0'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Define label mapping\n",
    "LABEL_MAP = {'Normal': 0, 'Explosion': 1, 'Fighting': 2, 'Car Accident': 3, 'Shooting': 4, 'Riot': 5}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# Define maximum clips per label - adjusted for full frame usage\n",
    "MAX_TRAIN_SAMPLES = 110\n",
    "MAX_TEST_SAMPLES = 30\n",
    "\n",
    "# Define maximum frames per label for 80-20 ratio\n",
    "# For a 80-20 ratio, if we assume each label should have equal representation\n",
    "MAX_FRAME = 40000  # 80% of frames per label\n",
    "MAX_TEST_FRAME = 10000  # 20% of frames per label\n",
    "\n",
    "# Improved hyperparameters\n",
    "LEARNING_RATE = 8.538808556803076e-05  # Will be updated by learning rate finder\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "EVAL_BATCH_SIZE = 3\n",
    "SEED = 24\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "NUM_EPOCHS = 20\n",
    "CLIP_LEN = 32\n",
    "FRAME_SAMPLE_RATE = 1\n",
    "TARGET_FRAMES = 128\n",
    "\n",
    "# Resume training from a checkpoint\n",
    "# Set to None to start training a new model from scratch\n",
    "# Set to path of saved model to continue training from that point\n",
    "# Can be any model checkpoint - doesn't need to follow a specific naming format\n",
    "RESUME_FROM_CHECKPOINT = 'F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/v2.0/best_model_acc.pt'\n",
    "\n",
    "# Early stopping parameters\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9976\\753900442.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('label', group_keys=False).apply(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9976\\753900442.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('label', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Fighting: Moving 4 videos (5760 frames) from test to train\n",
      "Label Riot: Moving 13 videos (18300 frames) from test to train\n",
      "\n",
      "Updated frame distribution:\n",
      "Label Normal: 9384 frames in test set\n",
      "Label Explosion: 8604 frames in test set\n",
      "Label Fighting: 9841 frames in test set\n",
      "Label Car Accident: 3861 frames in test set\n",
      "Label Shooting: 4422 frames in test set\n",
      "Label Riot: 8920 frames in test set\n",
      "\n",
      "Balanced number of videos per label in the training set:\n",
      "Normal (0): 110 videos\n",
      "Explosion (1): 106 videos\n",
      "Fighting (2): 114 videos\n",
      "Car Accident (3): 110 videos\n",
      "Shooting (4): 110 videos\n",
      "Riot (5): 123 videos\n",
      "\n",
      "Balanced number of videos per label in the validation set:\n",
      "Normal (0): 30 videos\n",
      "Explosion (1): 27 videos\n",
      "Fighting (2): 26 videos\n",
      "Car Accident (3): 30 videos\n",
      "Shooting (4): 30 videos\n",
      "Riot (5): 17 videos\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_csv(csv_path, video_path_column='rgb_video_path'):\n",
    "    \"\"\"Load and clean dataframe from CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=[video_path_column, 'label'])\n",
    "    return df\n",
    "\n",
    "df_train = load_data_from_csv(TRAIN_CSV_PATH)\n",
    "df_test = load_data_from_csv(TEST_CSV_PATH)\n",
    "\n",
    "def balance_dataset(df, max_samples):\n",
    "    \"\"\"Balance dataset to have equal number of samples per class.\"\"\"\n",
    "    return df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), max_samples), random_state=SEED)\n",
    "    )\n",
    "\n",
    "df_train = balance_dataset(df_train, MAX_TRAIN_SAMPLES)\n",
    "df_test = balance_dataset(df_test, MAX_TEST_SAMPLES)\n",
    "\n",
    "# Function to balance test and train sets based on frame count\n",
    "def balance_test_and_train_sets(df_train, df_test):\n",
    "    \"\"\"Balance test set to MAX_TEST_FRAME per label and move excess to training.\"\"\"\n",
    "    # Group test videos by label\n",
    "    test_videos_by_label = {label: [] for label in LABEL_MAP.values()}\n",
    "    test_frames_by_label = {label: 0 for label in LABEL_MAP.values()}\n",
    "    \n",
    "    # Count frames per label in test set\n",
    "    for _, row in df_test.iterrows():\n",
    "        video_path = row['rgb_video_path']\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            frame_count = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            test_videos_by_label[label].append((video_path, frame_count))\n",
    "            test_frames_by_label[label] += frame_count\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test video {video_path}: {e}\")\n",
    "    \n",
    "    # Move excess videos from test to train\n",
    "    videos_to_move = []\n",
    "    \n",
    "    for label, videos in test_videos_by_label.items():\n",
    "        if test_frames_by_label[label] > MAX_TEST_FRAME:\n",
    "            # Sort videos by frame count (optional - depends on your strategy)\n",
    "            videos.sort(key=lambda x: x[1])\n",
    "            \n",
    "            current_frames = 0\n",
    "            keep_idx = 0\n",
    "            \n",
    "            # Find how many videos to keep in test set\n",
    "            for i, (_, frame_count) in enumerate(videos):\n",
    "                if current_frames + frame_count <= MAX_TEST_FRAME:\n",
    "                    current_frames += frame_count\n",
    "                    keep_idx = i + 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Identify videos to move to training\n",
    "            videos_to_move.extend([path for path, _ in videos[keep_idx:]])\n",
    "            \n",
    "            print(f\"Label {INV_LABEL_MAP[label]}: Moving {len(videos) - keep_idx} videos \" \n",
    "                  f\"({test_frames_by_label[label] - current_frames} frames) from test to train\")\n",
    "            \n",
    "            # Update frame count\n",
    "            test_frames_by_label[label] = current_frames\n",
    "    \n",
    "    # Create masks for videos to keep in test and move to train\n",
    "    keep_mask = ~df_test['rgb_video_path'].isin(videos_to_move)\n",
    "    \n",
    "    # Extract videos to move\n",
    "    df_to_move = df_test[~keep_mask].copy()\n",
    "    \n",
    "    # Update test dataframe\n",
    "    df_test_balanced = df_test[keep_mask].copy()\n",
    "    \n",
    "    # Add moved videos to training\n",
    "    df_train_updated = pd.concat([df_train, df_to_move], ignore_index=True)\n",
    "    \n",
    "    print(\"\\nUpdated frame distribution:\")\n",
    "    for label, frames in test_frames_by_label.items():\n",
    "        print(f\"Label {INV_LABEL_MAP[label]}: {frames} frames in test set\")\n",
    "    \n",
    "    return df_train_updated, df_test_balanced\n",
    "\n",
    "# Apply the balancing function\n",
    "df_train, df_test = balance_test_and_train_sets(df_train, df_test)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nBalanced number of videos per label in the training set:\")\n",
    "train_label_counts = df_train['label'].value_counts().sort_index().to_dict()\n",
    "for label_id, count in train_label_counts.items():\n",
    "    print(f\"{INV_LABEL_MAP[label_id]} ({label_id}): {count} videos\")\n",
    "\n",
    "print(\"\\nBalanced number of videos per label in the validation set:\")\n",
    "val_label_counts = df_test['label'].value_counts().sort_index().to_dict()\n",
    "for label_id, count in val_label_counts.items():\n",
    "    print(f\"{INV_LABEL_MAP[label_id]} ({label_id}): {count} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames per label in training set:\n",
      "Normal: 28988 frames across 110 videos (avg: 263.53 frames/video)\n",
      "Explosion: 29586 frames across 106 videos (avg: 279.11 frames/video)\n",
      "Fighting: 57256 frames across 114 videos (avg: 502.25 frames/video)\n",
      "Car Accident: 15132 frames across 110 videos (avg: 137.56 frames/video)\n",
      "Shooting: 22851 frames across 110 videos (avg: 207.74 frames/video)\n",
      "Riot: 126027 frames across 123 videos (avg: 1024.61 frames/video)\n",
      "\n",
      "DataFrame of training results:\n",
      "          Label  Total Frames  Video Count  Avg Frames/Video\n",
      "0        Normal         28988          110        263.527273\n",
      "1     Explosion         29586          106        279.113208\n",
      "2      Fighting         57256          114        502.245614\n",
      "3  Car Accident         15132          110        137.563636\n",
      "4      Shooting         22851          110        207.736364\n",
      "5          Riot        126027          123       1024.609756\n",
      "\n",
      "Total frames per label in test set:\n",
      "Normal: 9384 frames across 30 videos (avg: 312.80 frames/video)\n",
      "Explosion: 8604 frames across 27 videos (avg: 318.67 frames/video)\n",
      "Fighting: 9841 frames across 26 videos (avg: 378.50 frames/video)\n",
      "Car Accident: 3861 frames across 30 videos (avg: 128.70 frames/video)\n",
      "Shooting: 4422 frames across 30 videos (avg: 147.40 frames/video)\n",
      "Riot: 8920 frames across 17 videos (avg: 524.71 frames/video)\n",
      "\n",
      "DataFrame of test results:\n",
      "          Label  Total Frames  Video Count  Avg Frames/Video\n",
      "0        Normal          9384           30        312.800000\n",
      "1     Explosion          8604           27        318.666667\n",
      "2      Fighting          9841           26        378.500000\n",
      "3  Car Accident          3861           30        128.700000\n",
      "4      Shooting          4422           30        147.400000\n",
      "5          Riot          8920           17        524.705882\n",
      "\n",
      "Total frames - Train: 279840, Test: 45032\n",
      "Train-Test ratio: 0.86:0.14\n"
     ]
    }
   ],
   "source": [
    "# Analyze frame distribution\n",
    "def analyze_frame_distribution():\n",
    "    \"\"\"Analyze frame distribution across labels to understand dataset imbalance.\"\"\"\n",
    "    label_frame_counts = {}\n",
    "    label_video_counts = {}\n",
    "    problematic_videos = []\n",
    "    \n",
    "    for _, row in df_train.iterrows():\n",
    "        video_path = row['rgb_video_path']\n",
    "        label = row['label']\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            if label not in label_frame_counts:\n",
    "                label_frame_counts[label] = 0\n",
    "                label_video_counts[label] = 0\n",
    "                \n",
    "            label_frame_counts[label] += total_frames\n",
    "            label_video_counts[label] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            problematic_videos.append((video_path, str(e)))\n",
    "            print(f\"Error processing video {video_path}: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Total frames per label in training set:\")\n",
    "    for label, count in label_frame_counts.items():\n",
    "        label_name = INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label\n",
    "        avg_frames = count / label_video_counts[label] if label_video_counts[label] > 0 else 0\n",
    "        print(f\"{label_name}: {count} frames across {label_video_counts[label]} videos (avg: {avg_frames:.2f} frames/video)\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'Label': [INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label for label in label_frame_counts.keys()],\n",
    "        'Total Frames': label_frame_counts.values(),\n",
    "        'Video Count': [label_video_counts[label] for label in label_frame_counts.keys()],\n",
    "        'Avg Frames/Video': [label_frame_counts[label]/label_video_counts[label] \n",
    "                             if label_video_counts[label] > 0 else 0 \n",
    "                             for label in label_frame_counts.keys()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataFrame of training results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Now analyze test set\n",
    "    label_frame_counts_test = {}\n",
    "    label_video_counts_test = {}\n",
    "    \n",
    "    for _, row in df_test.iterrows():\n",
    "        video_path = row['rgb_video_path']\n",
    "        label = row['label']\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            container.close()\n",
    "            \n",
    "            if label not in label_frame_counts_test:\n",
    "                label_frame_counts_test[label] = 0\n",
    "                label_video_counts_test[label] = 0\n",
    "                \n",
    "            label_frame_counts_test[label] += total_frames\n",
    "            label_video_counts_test[label] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test video {video_path}: {e}\")\n",
    "    \n",
    "    print(\"\\nTotal frames per label in test set:\")\n",
    "    for label, count in label_frame_counts_test.items():\n",
    "        label_name = INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label\n",
    "        avg_frames = count / label_video_counts_test[label] if label_video_counts_test[label] > 0 else 0\n",
    "        print(f\"{label_name}: {count} frames across {label_video_counts_test[label]} videos (avg: {avg_frames:.2f} frames/video)\")\n",
    "    \n",
    "    results_df_test = pd.DataFrame({\n",
    "        'Label': [INV_LABEL_MAP[label] if label in INV_LABEL_MAP else label for label in label_frame_counts_test.keys()],\n",
    "        'Total Frames': label_frame_counts_test.values(),\n",
    "        'Video Count': [label_video_counts_test[label] for label in label_frame_counts_test.keys()],\n",
    "        'Avg Frames/Video': [label_frame_counts_test[label]/label_video_counts_test[label] \n",
    "                             if label_video_counts_test[label] > 0 else 0 \n",
    "                             for label in label_frame_counts_test.keys()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nDataFrame of test results:\")\n",
    "    print(results_df_test)\n",
    "    \n",
    "    # Calculate total frames and ratio\n",
    "    total_train_frames = sum(label_frame_counts.values())\n",
    "    total_test_frames = sum(label_frame_counts_test.values())\n",
    "    total_frames = total_train_frames + total_test_frames\n",
    "    \n",
    "    print(f\"\\nTotal frames - Train: {total_train_frames}, Test: {total_test_frames}\")\n",
    "    print(f\"Train-Test ratio: {total_train_frames/total_frames:.2f}:{total_test_frames/total_frames:.2f}\")\n",
    "    \n",
    "    if problematic_videos:\n",
    "        print(f\"\\nFound {len(problematic_videos)} problematic videos. Check logs for details.\")\n",
    "        with open(\"problematic_videos.log\", \"w\") as f:\n",
    "            for path, error in problematic_videos:\n",
    "                f.write(f\"{path}: {error}\\n\")\n",
    "    \n",
    "    return label_frame_counts, label_video_counts\n",
    "\n",
    "# Run frame distribution analysis\n",
    "label_frame_counts, label_video_counts = analyze_frame_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class-specific augmentation to target problematic classes\n",
    "def get_class_specific_augment(label, is_training=True, strong_augment=False):\n",
    "    \"\"\"Apply different augmentation strategies based on class and confusion patterns.\"\"\"\n",
    "    if not is_training:\n",
    "        return T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    # Stronger augmentation for underrepresented or problematic classes\n",
    "    if strong_augment:\n",
    "        # Base strong augmentation\n",
    "        transform = T.Compose([\n",
    "            T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            T.RandomRotation(30),\n",
    "            T.RandomHorizontalFlip(p=0.7),\n",
    "            T.RandomVerticalFlip(p=0.3),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        # Addressing Normal/Car Accident confusion\n",
    "        if label in [0, 3]:  # Normal or Car Accident\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.65, 0.95)),  # More aggressive cropping\n",
    "                T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "                T.RandomRotation(45),  # More rotation\n",
    "                T.RandomHorizontalFlip(p=0.8),\n",
    "                T.RandomVerticalFlip(p=0.4),\n",
    "                T.RandomGrayscale(p=0.3),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        # Addressing Fighting/Normal confusion\n",
    "        elif label == 2:  # Fighting\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "                T.ColorJitter(brightness=0.4, contrast=0.6, saturation=0.4),\n",
    "                T.RandomRotation(35),\n",
    "                T.RandomHorizontalFlip(p=0.9),  # Higher flip probability\n",
    "                T.RandomPerspective(distortion_scale=0.3, p=0.5),  # Add perspective transform\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        # Addressing Shooting class performance decline\n",
    "        elif label == 4:  # Shooting\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                T.ColorJitter(brightness=0.3, contrast=0.5, saturation=0.3),\n",
    "                T.RandomRotation(20),\n",
    "                T.RandomHorizontalFlip(p=0.6),\n",
    "                T.RandomGrayscale(p=0.1),  # Less grayscale for shooting class\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        # Default augmentation\n",
    "        else:\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                T.RandomRotation(15),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "            \n",
    "    return transform\n",
    "\n",
    "# Enhanced data augmentation with class-specific strategy\n",
    "def read_video_pyav(container, indices, is_training=True, strong_augment=False, label=None):\n",
    "    \"\"\"Read video frames with appropriate augmentation based on class.\"\"\"\n",
    "    transform = get_class_specific_augment(label, is_training, strong_augment)\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    try:\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i in indices:\n",
    "                frame = transform(frame.to_image())\n",
    "                frames.append(frame)\n",
    "            if len(frames) == len(indices):\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error during frame decoding: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not frames:\n",
    "        return None\n",
    "        \n",
    "    return torch.stack(frames)\n",
    "\n",
    "def split_into_chunks(total_frames, clip_len, overlap=0):\n",
    "    \"\"\"Split video into overlapping chunks.\"\"\"\n",
    "    step = clip_len - overlap\n",
    "    chunks = [(start, start + clip_len) for start in range(0, total_frames, step) if start + clip_len <= total_frames]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, clip_len=32, frame_sample_rate=1, \n",
    "                 target_frames=128, overlap=0, is_training=True, video_path_column='rgb_video_path',\n",
    "                 max_frames_per_label=MAX_FRAME):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_sample_rate = frame_sample_rate\n",
    "        self.target_frames = target_frames\n",
    "        self.overlap = overlap\n",
    "        self.is_training = is_training\n",
    "        self.video_path_column = video_path_column\n",
    "        self.max_frames_per_label = max_frames_per_label\n",
    "        self.problematic_files = []  # Initialize before calling _prepare_data\n",
    "        self.data = self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare dataset by processing videos into clips.\"\"\"\n",
    "        prepared_data = []\n",
    "        short_clips = []\n",
    "        \n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            video_path = row[self.video_path_column]\n",
    "            label = int(row['label'])\n",
    "            try:\n",
    "                container = av.open(video_path)\n",
    "                total_frames = container.streams.video[0].frames\n",
    "                container.close()\n",
    "                \n",
    "                if total_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                    chunks = split_into_chunks(total_frames, self.clip_len * self.frame_sample_rate, self.overlap)\n",
    "                    for start, end in chunks:\n",
    "                        prepared_data.append({\n",
    "                            \"video_path\": video_path, \n",
    "                            \"label\": label, \n",
    "                            \"start\": start, \n",
    "                            \"end\": end,\n",
    "                            \"combined\": False\n",
    "                        })\n",
    "                else:\n",
    "                    short_clips.append({\n",
    "                        \"video_path\": video_path,\n",
    "                        \"label\": label,\n",
    "                        \"frames\": total_frames,\n",
    "                        \"combined\": True\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                self.problematic_files.append((video_path, str(e)))\n",
    "                print(f\"Error processing video {video_path}: {e}\")\n",
    "        \n",
    "        # Log problematic files\n",
    "        if self.problematic_files:\n",
    "            log_path = \"problematic_files_dataset.log\"\n",
    "            with open(log_path, \"w\") as f:\n",
    "                for path, error in self.problematic_files:\n",
    "                    f.write(f\"{path}: {error}\\n\")\n",
    "            print(f\"Logged {len(self.problematic_files)} problematic files to {log_path}\")\n",
    "        \n",
    "        combined_clips = self._combine_short_clips(short_clips)\n",
    "        \n",
    "        # Print combined clips stats before adding them to prepared_data\n",
    "        print(f\"Combined clips processing summary: {len(combined_clips)} clips created from {len(short_clips)} short clips, {len(short_clips) - sum([len(clip['video_paths']) for clip in combined_clips])} clips discarded.\")\n",
    "        \n",
    "        prepared_data.extend(combined_clips)\n",
    "        \n",
    "        # Balance clips by frame count\n",
    "        if self.is_training:\n",
    "            prepared_data = self._balance_clips_by_label(prepared_data)\n",
    "        \n",
    "        print(f\"\\nTotal clips: {len(prepared_data)}, including {len(combined_clips)} combined clips\")\n",
    "\n",
    "        return prepared_data\n",
    "    \n",
    "    def _balance_clips_by_label(self, clips):\n",
    "        \"\"\"Balance clips to ensure each label has max_frames_per_label frames.\"\"\"\n",
    "        label_groups = {}\n",
    "        label_frame_counts = {}\n",
    "    \n",
    "        # Calculate frame counts per clip and per label\n",
    "        for clip in clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in label_groups:\n",
    "                label_groups[label] = []\n",
    "                label_frame_counts[label] = 0\n",
    "            \n",
    "            # Calculate actual frame count for this clip\n",
    "            if not clip.get(\"combined\", False):\n",
    "                frame_count = (clip[\"end\"] - clip[\"start\"]) // self.frame_sample_rate\n",
    "            else:\n",
    "                frame_count = sum(clip.get(\"frames_per_clip\", [0])) // self.frame_sample_rate\n",
    "            \n",
    "            clip['frame_count'] = frame_count  # Store frame count directly in clip\n",
    "            label_groups[label].append(clip)\n",
    "            label_frame_counts[label] += frame_count\n",
    "    \n",
    "        balanced_clips = []\n",
    "    \n",
    "        for label, label_clips in label_groups.items():\n",
    "            current_frames = label_frame_counts[label]\n",
    "            \n",
    "            if current_frames > self.max_frames_per_label:\n",
    "                # Downsample strategy: keep largest clips first\n",
    "                sorted_clips = sorted(label_clips, key=lambda x: x['frame_count'], reverse=True)\n",
    "                accumulated = 0\n",
    "                selected_clips = []\n",
    "                \n",
    "                for clip in sorted_clips:\n",
    "                    if accumulated + clip['frame_count'] <= self.max_frames_per_label:\n",
    "                        selected_clips.append(clip)\n",
    "                        accumulated += clip['frame_count']\n",
    "                    elif accumulated == 0:  # Handle case where single clip exceeds max\n",
    "                        selected_clips.append(clip)\n",
    "                        accumulated += clip['frame_count']\n",
    "                        break\n",
    "                \n",
    "                balanced_clips.extend(selected_clips)\n",
    "                print(f\"Label {INV_LABEL_MAP[label]} ({label}): \"\n",
    "                      f\"Reduced from {current_frames} to {accumulated} frames\")\n",
    "    \n",
    "            else:\n",
    "                # Upsample strategy: augment until reaching max_frames_per_label\n",
    "                balanced_clips.extend(label_clips)\n",
    "                accumulated = current_frames\n",
    "                deficit = self.max_frames_per_label - accumulated\n",
    "                \n",
    "                if deficit > 0:\n",
    "                    original_clips = label_clips.copy()\n",
    "                    while deficit > 0:\n",
    "                        for clip in original_clips:\n",
    "                            if deficit <= 0:\n",
    "                                break\n",
    "                                \n",
    "                            augmented_clip = clip.copy()\n",
    "                            augmented_clip[\"augment_strongly\"] = True\n",
    "                            balanced_clips.append(augmented_clip)\n",
    "                            deficit -= clip['frame_count']\n",
    "                            accumulated += clip['frame_count']\n",
    "    \n",
    "                    print(f\"Label {INV_LABEL_MAP[label]} ({label}): \"\n",
    "                          f\"Increased from {current_frames} to {accumulated - deficit} frames \"\n",
    "                          f\"(added {len(balanced_clips) - len(label_clips)} augmented clips)\")\n",
    "        \n",
    "        # Verification step to confirm balancing\n",
    "        final_label_frames = {}\n",
    "        for clip in balanced_clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in final_label_frames:\n",
    "                final_label_frames[label] = 0\n",
    "            final_label_frames[label] += clip['frame_count']\n",
    "        \n",
    "        print(\"\\nFinal frame count per label after balancing:\")\n",
    "        for label, count in final_label_frames.items():\n",
    "            print(f\"Label {INV_LABEL_MAP[label]} ({label}): {count} frames\")\n",
    "    \n",
    "        return balanced_clips\n",
    "\n",
    "    \n",
    "    def _combine_short_clips(self, short_clips):\n",
    "        \"\"\"Combine short clips to reach minimum length.\"\"\"\n",
    "        label_groups = {}\n",
    "        for clip in short_clips:\n",
    "            label = clip[\"label\"]\n",
    "            if label not in label_groups:\n",
    "                label_groups[label] = []\n",
    "            label_groups[label].append(clip)\n",
    "        \n",
    "        combined_data = []\n",
    "        for label, clips in label_groups.items():\n",
    "            current_clips = []\n",
    "            current_frames = 0\n",
    "            \n",
    "            for clip in clips:\n",
    "                if current_frames + clip[\"frames\"] <= self.target_frames:\n",
    "                    current_clips.append(clip)\n",
    "                    current_frames += clip[\"frames\"]\n",
    "                    \n",
    "                    if current_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                        combined_data.append({\n",
    "                            \"video_paths\": [c[\"video_path\"] for c in current_clips],\n",
    "                            \"label\": label,\n",
    "                            \"frames_per_clip\": [c[\"frames\"] for c in current_clips],\n",
    "                            \"combined\": True\n",
    "                        })\n",
    "                        current_clips = []\n",
    "                        current_frames = 0\n",
    "            \n",
    "            if current_clips and current_frames >= self.clip_len * self.frame_sample_rate:\n",
    "                combined_data.append({\n",
    "                    \"video_paths\": [c[\"video_path\"] for c in current_clips],\n",
    "                    \"label\": label,\n",
    "                    \"frames_per_clip\": [c[\"frames\"] for c in current_clips],\n",
    "                    \"combined\": True\n",
    "                })\n",
    "        \n",
    "        return combined_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        strong_augment = item.get(\"augment_strongly\", False)\n",
    "        label = item[\"label\"]\n",
    "        \n",
    "        if not item.get(\"combined\", False):\n",
    "            video_path, start, end = item[\"video_path\"], item[\"start\"], item[\"end\"]\n",
    "            try:\n",
    "                container = av.open(video_path)\n",
    "                indices = list(range(start, end, self.frame_sample_rate))\n",
    "                if len(indices) != self.clip_len:\n",
    "                    container.close()\n",
    "                    return None\n",
    "                    \n",
    "                video = read_video_pyav(container, indices, self.is_training, strong_augment, label)\n",
    "                container.close()\n",
    "                \n",
    "                if video is None:\n",
    "                    return None\n",
    "                \n",
    "                # Check if frames are valid\n",
    "                if torch.isnan(video).any() or torch.isinf(video).any():\n",
    "                    print(f\"Invalid frame values detected in {video_path}\")\n",
    "                    return None\n",
    "                \n",
    "                # Add temporal dropout for additional augmentation when training\n",
    "                if self.is_training and np.random.random() < 0.3:\n",
    "                    # Randomly drop up to 10% of frames and repeat adjacent frames\n",
    "                    num_frames_to_drop = max(1, int(0.1 * video.shape[0]))\n",
    "                    frames_to_drop = np.random.choice(video.shape[0], num_frames_to_drop, replace=False)\n",
    "                    for idx in frames_to_drop:\n",
    "                        # Replace with previous or next frame\n",
    "                        replace_idx = max(0, idx-1) if idx > 0 else min(idx+1, video.shape[0]-1)\n",
    "                        video[idx] = video[replace_idx]\n",
    "                \n",
    "                inputs = self.processor(list(video.permute(0, 2, 3, 1).numpy()), return_tensors=\"pt\")\n",
    "                return {\"pixel_values\": inputs[\"pixel_values\"].squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading video chunk from {video_path}: {e}\")\n",
    "                self.problematic_files.append((video_path, str(e)))\n",
    "                return None\n",
    "        \n",
    "        else:\n",
    "            video_paths = item[\"video_paths\"]\n",
    "            frames_per_clip = item[\"frames_per_clip\"]\n",
    "            \n",
    "            all_frames = []\n",
    "            current_frame_count = 0\n",
    "            \n",
    "            for i, video_path in enumerate(video_paths):\n",
    "                try:\n",
    "                    container = av.open(video_path)\n",
    "                    frames_needed = min(frames_per_clip[i], self.clip_len * self.frame_sample_rate - current_frame_count)\n",
    "                    indices = list(range(0, frames_needed))\n",
    "                    video = read_video_pyav(container, indices, self.is_training, strong_augment, label)\n",
    "                    container.close()\n",
    "                    \n",
    "                    if video is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if frames are valid\n",
    "                    if torch.isnan(video).any() or torch.isinf(video).any():\n",
    "                        print(f\"Invalid frame values detected in {video_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    all_frames.append(video)\n",
    "                    current_frame_count += frames_needed\n",
    "                    \n",
    "                    if current_frame_count >= self.clip_len * self.frame_sample_rate:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading combined video from {video_path}: {e}\")\n",
    "                    self.problematic_files.append((video_path, str(e)))\n",
    "            \n",
    "            if not all_frames or current_frame_count < self.clip_len * self.frame_sample_rate:\n",
    "                return None\n",
    "                \n",
    "            combined_video = torch.cat(all_frames, dim=0)\n",
    "            \n",
    "            if combined_video.shape[0] > self.clip_len:\n",
    "                combined_video = combined_video[:self.clip_len]\n",
    "                \n",
    "            inputs = self.processor(list(combined_video.permute(0, 2, 3, 1).numpy()), return_tensors=\"pt\")\n",
    "            return {\"pixel_values\": inputs[\"pixel_values\"].squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function that handles None values in batch.\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint: F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/v2.0/best_model_acc.pt\n",
      "Successfully loaded model from: F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/v2.0/best_model_acc.pt\n",
      "Starting additional training from this checkpoint...\n",
      "Combined clips processing summary: 19 clips created from 42 short clips, 3 clips discarded.\n",
      "Label Normal (0): Reduced from 53041 to 39985 frames\n",
      "Label Explosion (1): Reduced from 54426 to 39994 frames\n",
      "Label Fighting (2): Reduced from 109427 to 39987 frames\n",
      "Label Car Accident (3): Increased from 25293 to 40026 frames (added 4205 augmented clips)\n",
      "Label Shooting (4): Reduced from 40820 to 39988 frames\n",
      "Label Riot (5): Reduced from 247072 to 40000 frames\n",
      "\n",
      "Final frame count per label after balancing:\n",
      "Label Normal (0): 39985 frames\n",
      "Label Explosion (1): 39994 frames\n",
      "Label Fighting (2): 39987 frames\n",
      "Label Car Accident (3): 40013 frames\n",
      "Label Shooting (4): 39988 frames\n",
      "Label Riot (5): 40000 frames\n",
      "\n",
      "Total clips: 7489, including 19 combined clips\n",
      "Combined clips processing summary: 3 clips created from 8 short clips, 1 clips discarded.\n",
      "\n",
      "Total clips: 2585, including 3 combined clips\n",
      "Skipping learning rate finder since we're resuming from a checkpoint.\n",
      "Using learning rate: 8.538808556803076e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9976\\1266997899.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the processor with do_rescale=None and offset=None as requested\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\", do_rescale=None, offset=None)\n",
    "model = VivitForVideoClassification.from_pretrained(\n",
    "    \"google/vivit-b-16x2\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "\n",
    "# Load model from checkpoint if specified\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    print(f\"Loading model from checkpoint: {RESUME_FROM_CHECKPOINT}\")\n",
    "    model.load_state_dict(torch.load(RESUME_FROM_CHECKPOINT))\n",
    "    print(f\"Successfully loaded model from: {RESUME_FROM_CHECKPOINT}\")\n",
    "    print(\"Starting additional training from this checkpoint...\")\n",
    "\n",
    "# Initialize datasets and dataloaders with different max frames for train and test\n",
    "train_dataset = VideoDataset(df_train, processor, CLIP_LEN, FRAME_SAMPLE_RATE, \n",
    "                            target_frames=TARGET_FRAMES, overlap=16, is_training=True,\n",
    "                            max_frames_per_label=MAX_FRAME)\n",
    "test_dataset = VideoDataset(df_test, processor, CLIP_LEN, FRAME_SAMPLE_RATE, \n",
    "                           target_frames=TARGET_FRAMES, overlap=16, is_training=False,\n",
    "                           max_frames_per_label=MAX_TEST_FRAME)\n",
    "\n",
    "# Separate parameters with and without weight decay\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                  if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 1e-6,\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                  if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Adjust class weights based on confusion matrix insights\n",
    "def get_adjusted_class_weights(class_counts):\n",
    "    weights = 1.0 / class_counts\n",
    "    \n",
    "    # Increase weight for frequently misclassified classes\n",
    "    # Fighting (2) and Car Accident (3) need more attention\n",
    "    weights[2] *= 1.3  # Fighting\n",
    "    weights[3] *= 1.5  # Car Accident\n",
    "    weights[4] *= 1.2  # Shooting (declining performance)\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "train_labels = df_train[\"label\"].values\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = get_adjusted_class_weights(class_counts).to(device)\n",
    "\n",
    "# Define focal loss to focus more on hard examples\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2.0, reduction=\"mean\", label_smoothing=0.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\", label_smoothing=label_smoothing)\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Extract logits if inputs is a model output object (e.g., ImageClassifierOutput)\n",
    "        if hasattr(inputs, 'logits'):\n",
    "            inputs = inputs.logits\n",
    "            \n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "# Use focal loss with label smoothing\n",
    "loss_fn = FocalLoss(weight=class_weights, gamma=2.0, label_smoothing=0.1)\n",
    "\n",
    "# Create optimizer with initial learning rate (will be updated by LR finder)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "# Create the GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Create train dataloader for LR finder\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, \n",
    "                             shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, \n",
    "                            shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "# Only run the learning rate finder if we're not resuming from a checkpoint\n",
    "if not RESUME_FROM_CHECKPOINT:\n",
    "    # Set up mixed precision config for torch_lr_finder\n",
    "    amp_config = {\n",
    "        'device_type': 'cuda',\n",
    "        'dtype': torch.float16,\n",
    "    }\n",
    "\n",
    "    # Create and run the LR finder with custom iterator\n",
    "    print(\"Running learning rate finder...\")\n",
    "    train_data_iter = CustomTrainDataLoaderIter(train_dataloader)\n",
    "    \n",
    "    lr_finder = LRFinder(\n",
    "        model, \n",
    "        optimizer, \n",
    "        loss_fn, \n",
    "        device=device, \n",
    "        amp_backend='torch',\n",
    "        amp_config=amp_config, \n",
    "        grad_scaler=scaler\n",
    "    )\n",
    "\n",
    "    # Run range test with gradient accumulation\n",
    "    lr_finder.range_test(\n",
    "        train_data_iter,  # Use custom iterator instead of dataloader directly\n",
    "        end_lr=1e-2, \n",
    "        num_iter=100, \n",
    "        step_mode=\"exp\", \n",
    "        accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n",
    "    )\n",
    "\n",
    "    # Fixed code\n",
    "    fig, suggested_lr = lr_finder.plot(suggest_lr=True)  # This returns both the plot and the suggested learning rate\n",
    "    print(f\"Suggested learning rate: {suggested_lr}\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    LEARNING_RATE = suggested_lr if suggested_lr is not None else LEARNING_RATE\n",
    "    print(f\"Using learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "    # Reset the model and optimizer\n",
    "    lr_finder.reset()\n",
    "\n",
    "    # Recreate optimizer with the found learning rate\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8)\n",
    "else:\n",
    "    print(f\"Skipping learning rate finder since we're resuming from a checkpoint.\")\n",
    "    print(f\"Using learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Initialize training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": []\n",
    "}\n",
    "\n",
    "best_val_accuracy = 0\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  10%|                           | 261/2497 [20:12<2:58:01,  4.78s/it, Loss=0.00000021, Accuracy=94.25%]"
     ]
    }
   ],
   "source": [
    "# Start training from epoch 0 regardless of checkpoint source\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    epoch_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", leave=True)\n",
    "    for step, batch in enumerate(epoch_progress_bar):\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"])\n",
    "            raw_loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "            loss_for_backward = raw_loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss_for_backward).backward()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            # Unscale gradients first\n",
    "            scaler.unscale_(optimizer)\n",
    "    \n",
    "            # Add gradient clipping with smaller max_norm for better stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total_predictions += batch[\"labels\"].size(0)\n",
    "\n",
    "        epoch_loss += loss_for_backward.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        epoch_progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{loss_for_backward.item() * GRADIENT_ACCUMULATION_STEPS:.8f}\",\n",
    "            \"Accuracy\": f\"{(correct_predictions / total_predictions) * 100:.2f}%\"\n",
    "        })\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    history[\"train_loss\"].append(avg_epoch_loss)\n",
    "    history[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create validation progress bar similar to training\n",
    "    val_progress_bar = tqdm(test_dataloader, desc=\"Validation\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"], interpolate_pos_encoding=True)\n",
    "            \n",
    "            # Calculate batch loss\n",
    "            batch_loss = loss_fn(outputs.logits, batch[\"labels\"]).item()\n",
    "            val_loss += batch_loss\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            batch_correct = (predictions == batch[\"labels\"]).sum().item()\n",
    "            batch_total = batch[\"labels\"].size(0)\n",
    "            \n",
    "            val_correct_predictions += batch_correct\n",
    "            val_total_predictions += batch_total\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "            \n",
    "            # Update progress bar with current batch accuracy and loss\n",
    "            current_accuracy = (val_correct_predictions / val_total_predictions) * 100\n",
    "            val_progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{batch_loss:.8f}\",\n",
    "                \"Accuracy\": f\"{current_accuracy:.2f}%\"\n",
    "            })\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_model_acc.pt\"))\n",
    "        print(f\"New best model saved with validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    # Save model at each epoch\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, f\"vivit_epoch_{epoch + 1}.pt\"))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Train Loss: {avg_epoch_loss:.8f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy * 100:.2f}%, \"\n",
    "          f\"Val Loss: {avg_val_loss:.8f}, \"\n",
    "          f\"Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Create confusion matrix every epoch with larger figure size for better readability\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(LABEL_MAP.keys()))\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - Epoch {epoch + 1}\")\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f'confusion_matrix_epoch_{epoch + 1}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Implement early stopping\n",
    "    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Training and Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'training_metrics.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ViVi",
   "language": "python",
   "name": "vivi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa280c088d9450abcf24174d16b3c60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae09b7163b6242249038eff8c54f1270",
      "placeholder": "",
      "style": "IPY_MODEL_a2e0189ae3574945b2336e7415fa7cc2",
      "value": " 96/96 [00:05&lt;00:00, 16.12 examples/s]"
     }
    },
    "111140a7c8834455aed97be4a5b2253e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33b69a8b67d54f20bbdb663d41a53c60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "3fbca77d0b4945b480bde77597109dd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "453888b3c6e9432dac50ba5c26d19842": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_111140a7c8834455aed97be4a5b2253e",
      "placeholder": "",
      "style": "IPY_MODEL_5059041a1938466c8f11ae43c6e20c90",
      "value": " 960/960 [00:40&lt;00:00, 23.94 examples/s]"
     }
    },
    "5059041a1938466c8f11ae43c6e20c90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5132ae33e2a9420b855b48a1b3320d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dce3d31d53c441d4bf5cc6c2ae39dac7",
      "placeholder": "",
      "style": "IPY_MODEL_966d4e0db390439ea4f7e9243c07d4f4",
      "value": "Map: 100%"
     }
    },
    "780cedaa760a43b2ae99fe694e417126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1a94e8ac6634a5ab6f1782c42e1eac8",
      "max": 96,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fbca77d0b4945b480bde77597109dd2",
      "value": 96
     }
    },
    "81efd04ce230429d9c0cae00813d62a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966d4e0db390439ea4f7e9243c07d4f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a85f9a5d728459a929696169d3dcad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbfc602d75334b8886a4ca88e5cf3cb8",
      "max": 960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0d1bda1a0c74f1293eca52412632ec4",
      "value": 960
     }
    },
    "9fd242834e4841f6b50c54048be9e694": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e38dd76466024573b3868d2e2af0b730",
       "IPY_MODEL_780cedaa760a43b2ae99fe694e417126",
       "IPY_MODEL_0aa280c088d9450abcf24174d16b3c60"
      ],
      "layout": "IPY_MODEL_b63d2279bf974dc0bea8d889e98abc6a"
     }
    },
    "a2e0189ae3574945b2336e7415fa7cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8b703bbcdd440ee92c67f7abcb80eca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae09b7163b6242249038eff8c54f1270": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0d1bda1a0c74f1293eca52412632ec4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b63d2279bf974dc0bea8d889e98abc6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "bbfc602d75334b8886a4ca88e5cf3cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dce3d31d53c441d4bf5cc6c2ae39dac7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e38dd76466024573b3868d2e2af0b730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8b703bbcdd440ee92c67f7abcb80eca",
      "placeholder": "",
      "style": "IPY_MODEL_81efd04ce230429d9c0cae00813d62a0",
      "value": "Map: 100%"
     }
    },
    "e96df79b4c4046cba35d84ac56d54295": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5132ae33e2a9420b855b48a1b3320d54",
       "IPY_MODEL_9a85f9a5d728459a929696169d3dcad5",
       "IPY_MODEL_453888b3c6e9432dac50ba5c26d19842"
      ],
      "layout": "IPY_MODEL_33b69a8b67d54f20bbdb663d41a53c60"
     }
    },
    "f1a94e8ac6634a5ab6f1782c42e1eac8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
