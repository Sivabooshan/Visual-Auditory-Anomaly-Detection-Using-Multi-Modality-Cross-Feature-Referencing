{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0b287d-de39-4be5-8d71-73f162da20fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import av\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths and constants\n",
    "SAVED_MODEL_PATH = 'F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/v1.0/best_model_acc.pt'\n",
    "LABEL_MAP = {0: 'Normal', 1: 'Explosion', 2: 'Fighting', 3: 'Car Accident', 4: 'Shooting', 5: 'Riot'}\n",
    "CLIP_LEN = 32\n",
    "FRAME_SAMPLE_RATE = 1\n",
    "\n",
    "# Load ViVit processor and model with saved weights\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\", do_rescale=None, offset=None)\n",
    "model = VivitForVideoClassification.from_pretrained(\n",
    "    \"google/vivit-b-16x2\",\n",
    "    num_labels=len(LABEL_MAP),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e690f3-362c-4b18-88d0-802c030c0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved weights\n",
    "model.load_state_dict(torch.load(SAVED_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to extract frames from video\n",
    "def extract_frames_from_video(video_path, num_frames=CLIP_LEN, sample_rate=FRAME_SAMPLE_RATE):\n",
    "    frames = []\n",
    "    try:\n",
    "        container = av.open(video_path)\n",
    "        # Get total number of frames\n",
    "        total_frames = container.streams.video[0].frames\n",
    "        \n",
    "        # Calculate indices to sample\n",
    "        if total_frames >= num_frames * sample_rate:\n",
    "            # Uniform sampling\n",
    "            indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        else:\n",
    "            # If video is too short, loop the video\n",
    "            indices = np.arange(0, num_frames * sample_rate, sample_rate) % total_frames\n",
    "        \n",
    "        # Extract frames at the selected indices\n",
    "        container.seek(0)\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i in indices:\n",
    "                # Convert to PIL Image and apply basic transformations\n",
    "                img = frame.to_image()\n",
    "                frames.append(img)\n",
    "            if len(frames) == num_frames:\n",
    "                break\n",
    "                \n",
    "        container.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting frames from {video_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c682584c-b42b-4672-a148-cf79501e537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess frames and make predictions\n",
    "def predict_video_class(video_path):\n",
    "    # Extract frames\n",
    "    frames = extract_frames_from_video(video_path)\n",
    "    \n",
    "    if frames is None or len(frames) < CLIP_LEN:\n",
    "        print(f\"Could not extract enough frames from {video_path}\")\n",
    "        return \"Error: Insufficient frames\"\n",
    "    \n",
    "    # Process frames with the image processor\n",
    "    # Convert frames to format expected by the processor\n",
    "    transform = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    processed_frames = [transform(frame) for frame in frames]\n",
    "    frames_tensor = torch.stack(processed_frames)\n",
    "    \n",
    "    # Convert to numpy for processor\n",
    "    frames_numpy = [frame.permute(1, 2, 0).numpy() for frame in processed_frames]\n",
    "    \n",
    "    # Process with ViVit processor\n",
    "    inputs = processor(frames_numpy, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    return LABEL_MAP[predicted_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b647a7-2861-4437-a216-deb7d9b76d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted video class: Fighting\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "video_file_path = \"E:/SRC-Bhuvaneswari/VAD_XDViolence/ViVi/Dataset/XD Violence/Test/Brick.Mansions.2014__#00-16-26_00-17-12_label_B1-0-0.mp4\"\n",
    "predicted_class = predict_video_class(video_file_path)\n",
    "print(f\"Predicted video class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda548a9-1612-484b-b26b-43c198caec54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVi",
   "language": "python",
   "name": "vivi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
