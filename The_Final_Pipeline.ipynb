{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ead86f-dec0-4022-b652-2117a8d5fa55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import cv2\n",
    "import av\n",
    "import moviepy.editor as mp\n",
    "from transformers import (\n",
    "    VivitImageProcessor, \n",
    "    VivitForVideoClassification,\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2ForSequenceClassification\n",
    ")\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import tempfile\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "from collections import deque\n",
    "from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout,\n",
    "                            QHBoxLayout, QLabel, QPushButton, QSlider, QStyle,\n",
    "                            QFileDialog, QFrame, QProgressBar, QComboBox, QMessageBox,\n",
    "                            QCheckBox)\n",
    "from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QThread, QMutex\n",
    "from PyQt5.QtGui import QImage, QPixmap, QPainter, QColor, QPen, QBrush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5e224-500e-4ec6-93d5-0e5f5e508315",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_path(path):\n",
    "    \"\"\"Normalize file path by converting backslashes to forward slashes.\"\"\"\n",
    "    if path:\n",
    "        return path.replace('\\\\', '/')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b289f-06ab-4325-a273-1de8d01dac0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BidirectionalCrossModalAttention(nn.Module):\n",
    "    \"\"\"Enhanced bidirectional cross-modal attention for superior fusion\"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.visual_proj = nn.Linear(dim, dim)\n",
    "        self.audio_proj = nn.Linear(dim, dim)\n",
    "        self.output_proj = nn.Linear(dim*2, dim)  # Wider projection\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Two attention mechanisms for bidirectional flow\n",
    "        self.v2a_attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.a2v_attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "    def forward(self, visual, audio):\n",
    "        \"\"\"Apply bidirectional cross-attention between modalities\"\"\"\n",
    "        # Project and normalize\n",
    "        visual_proj = self.norm1(self.visual_proj(visual))\n",
    "        audio_proj = self.norm2(self.audio_proj(audio))\n",
    "        \n",
    "        # Bidirectional attention\n",
    "        v2a_output, _ = self.v2a_attention(visual_proj, audio_proj, audio_proj)\n",
    "        a2v_output, _ = self.a2v_attention(audio_proj, visual_proj, visual_proj)\n",
    "        \n",
    "        # Combine bidirectional information\n",
    "        combined = torch.cat([v2a_output, a2v_output], dim=-1)\n",
    "        output = self.output_proj(combined)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d10802-1f1b-41e7-a02e-4234745a5a14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IntraModalityFusion(nn.Module):\n",
    "    \"\"\"Fusion module for combining RGB and Optical Flow features\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.rgb_proj = nn.Linear(dim, dim)\n",
    "        self.flow_proj = nn.Linear(dim, dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, rgb_features, flow_features):\n",
    "        \"\"\"Combine RGB and optical flow features within visual modality\"\"\"\n",
    "        # Handle batched inputs - reshape if needed\n",
    "        if len(rgb_features.shape) == 3:  # [batch_size, seq_len, hidden_dim]\n",
    "            batch_size, seq_len, hidden_dim = rgb_features.shape\n",
    "            \n",
    "            # Reshape to [batch_size*seq_len, hidden_dim] for processing\n",
    "            rgb_flat = rgb_features.reshape(-1, hidden_dim)\n",
    "            flow_flat = flow_features.reshape(-1, hidden_dim)\n",
    "            \n",
    "            # Project and normalize\n",
    "            rgb_proj = self.norm1(self.rgb_proj(rgb_flat))\n",
    "            flow_proj = self.norm2(self.flow_proj(flow_flat))\n",
    "            \n",
    "            # Concatenate and fuse\n",
    "            combined = torch.cat([rgb_proj, flow_proj], dim=-1)\n",
    "            fused = self.fusion(combined)\n",
    "            \n",
    "            # Reshape back to [batch_size, seq_len, hidden_dim]\n",
    "            fused = fused.reshape(batch_size, seq_len, hidden_dim)\n",
    "        else:\n",
    "            # Standard processing for single frames\n",
    "            rgb_proj = self.norm1(self.rgb_proj(rgb_features))\n",
    "            flow_proj = self.norm2(self.flow_proj(flow_features))\n",
    "            \n",
    "            # Concatenate and fuse\n",
    "            combined = torch.cat([rgb_proj, flow_proj], dim=-1)\n",
    "            fused = self.fusion(combined)\n",
    "        \n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb47edc-7722-4682-8f34-f0ff59072ce9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InterModalityFusion(nn.Module):\n",
    "    \"\"\"Cross-modal fusion between visual and audio modalities\"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.visual_proj = nn.Linear(dim, dim)\n",
    "        self.audio_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Bidirectional attention mechanisms\n",
    "        self.v2a_attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.a2v_attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "            \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.norm_visual = nn.LayerNorm(dim)\n",
    "        self.norm_audio = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, visual_features, audio_features):\n",
    "        \"\"\"Apply bi-directional attention between visual and audio modalities\"\"\"\n",
    "        # Handle batched input - reshape if needed\n",
    "        if len(visual_features.shape) == 3:  # [batch_size, seq_len, hidden_dim]\n",
    "            batch_size, seq_len, hidden_dim = visual_features.shape\n",
    "            \n",
    "            # If audio is not temporal, expand it to match visual temporal dimension\n",
    "            if len(audio_features.shape) == 2:  # [batch_size, hidden_dim]\n",
    "                audio_features = audio_features.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "                \n",
    "            # Reshape to [batch_size*seq_len, hidden_dim] for processing\n",
    "            visual_flat = visual_features.reshape(-1, hidden_dim)  # [batch*seq, dim]\n",
    "            audio_flat = audio_features.reshape(-1, hidden_dim)    # [batch*seq, dim]\n",
    "            \n",
    "            # Project and normalize\n",
    "            visual_proj = self.norm_visual(self.visual_proj(visual_flat))\n",
    "            audio_proj = self.norm_audio(self.audio_proj(audio_flat))\n",
    "            \n",
    "            # Apply bidirectional attention (each frame independent)\n",
    "            v2a_feat, _ = self.v2a_attention(\n",
    "                visual_proj.unsqueeze(0),  # Add batch dimension for attention\n",
    "                audio_proj.unsqueeze(0), \n",
    "                audio_proj.unsqueeze(0)\n",
    "            )\n",
    "            \n",
    "            a2v_feat, _ = self.a2v_attention(\n",
    "                audio_proj.unsqueeze(0),\n",
    "                visual_proj.unsqueeze(0),\n",
    "                visual_proj.unsqueeze(0)\n",
    "            )\n",
    "            \n",
    "            # Remove added batch dimension\n",
    "            v2a_feat = v2a_feat.squeeze(0)\n",
    "            a2v_feat = a2v_feat.squeeze(0)\n",
    "            \n",
    "            # Combine bidirectional features\n",
    "            combined = torch.cat([v2a_feat, a2v_feat], dim=-1)\n",
    "            fused = self.fusion(combined)\n",
    "            \n",
    "            # Reshape back to [batch_size, seq_len, hidden_dim]\n",
    "            fused = fused.reshape(batch_size, seq_len, hidden_dim)\n",
    "        else:\n",
    "            # Standard processing for non-batched inputs\n",
    "            visual_proj = self.norm_visual(self.visual_proj(visual_features))\n",
    "            audio_proj = self.norm_audio(self.audio_proj(audio_features))\n",
    "            \n",
    "            # Bidirectional attention\n",
    "            v2a_features, _ = self.v2a_attention(\n",
    "                visual_proj.unsqueeze(0), \n",
    "                audio_proj.unsqueeze(0), \n",
    "                audio_proj.unsqueeze(0)\n",
    "            )\n",
    "            \n",
    "            a2v_features, _ = self.a2v_attention(\n",
    "                audio_proj.unsqueeze(0),\n",
    "                visual_proj.unsqueeze(0),\n",
    "                visual_proj.unsqueeze(0)\n",
    "            )\n",
    "            \n",
    "            # Remove batch dimension\n",
    "            v2a_features = v2a_features.squeeze(0)\n",
    "            a2v_features = a2v_features.squeeze(0)\n",
    "            \n",
    "            # Combine attended features\n",
    "            combined = torch.cat([v2a_features, a2v_features], dim=-1)\n",
    "            fused = self.fusion(combined)\n",
    "        \n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d26a18-3bc7-4bdf-b3c1-fcc81604b61e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DividedSpaceTimeAttention(nn.Module):\n",
    "    \"\"\"Divide attention across spatial and temporal dimensions separately - FIXED VERSION\"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.spatial_attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "            \n",
    "        # Feed-forward network for final fusion\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x, batch_size=1, seq_len=None):\n",
    "        \"\"\"Apply divided attention with fixed temporal attention processing\"\"\"\n",
    "        # Save original shape for later restoration\n",
    "        orig_shape = x.shape\n",
    "        \n",
    "        # Handle explicitly provided sequence length\n",
    "        if seq_len is None:\n",
    "            if len(orig_shape) == 3:\n",
    "                # Input is already [batch, seq, features]\n",
    "                batch_size, seq_len, dim = orig_shape\n",
    "            else:\n",
    "                # Default to single frame if not specified\n",
    "                seq_len = 1\n",
    "        \n",
    "        print(f\"DSTA - Input shape: {orig_shape}, Using batch_size={batch_size}, seq_len={seq_len}\")\n",
    "        \n",
    "        # Reshape to standard [batch, sequence, features] if needed\n",
    "        if len(orig_shape) != 3:\n",
    "            x = x.view(batch_size, seq_len, -1)\n",
    "            \n",
    "        # 1. Spatial attention (works for any sequence length)\n",
    "        spatial_x = x\n",
    "        spatial_norm = self.norm1(spatial_x)\n",
    "        attended_spatial, _ = self.spatial_attention(\n",
    "            spatial_norm, spatial_norm, spatial_norm\n",
    "        )\n",
    "        spatial_out = spatial_x + attended_spatial\n",
    "        \n",
    "        # 2. Temporal attention (only applied when multiple frames are present)\n",
    "        if seq_len <= 1:\n",
    "            print(\"DSTA - Skipping temporal attention as seq_len <= 1\")\n",
    "            temporal_out = spatial_out\n",
    "        else:\n",
    "            print(f\"DSTA - Applying temporal attention across {seq_len} frames\")\n",
    "            \n",
    "            # Apply feature-wise temporal attention without dimensions mismatch\n",
    "            # Process each feature through temporal dimension separately\n",
    "            batch, seq, feat_dim = spatial_out.shape\n",
    "            \n",
    "            # Take the first token representation instead of trying to process all tokens\n",
    "            # This fixes the dimension mismatch issue\n",
    "            cls_token = spatial_out[:, 0, :].unsqueeze(1)  # [batch, 1, feat_dim]\n",
    "            \n",
    "            # Apply temporal attention only to the cls token\n",
    "            normalized = self.norm2(cls_token)\n",
    "            attended_temporal, _ = self.temporal_attention(\n",
    "                normalized, normalized, normalized\n",
    "            )\n",
    "            \n",
    "            # Add the attended cls token back\n",
    "            spatial_out[:, 0, :] = (spatial_out[:, 0, :] + attended_temporal.squeeze(1))\n",
    "            temporal_out = spatial_out\n",
    "        \n",
    "        # 3. Feed-forward network with normalization\n",
    "        output = temporal_out + self.feed_forward(self.norm3(temporal_out))\n",
    "        \n",
    "        # Ensure output matches original shape\n",
    "        if output.shape != orig_shape:\n",
    "            output = output.reshape(orig_shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45bebd-b683-4e72-b00d-b1e8fa9545cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdvancedTemporalFilter:\n",
    "    \"\"\"Adaptive temporal smoothing with class-specific handling\"\"\"\n",
    "    def __init__(self, base_window_size=3, threshold=0.5, label_map=None):\n",
    "        self.base_window_size = base_window_size\n",
    "        self.threshold = threshold\n",
    "        self.history = []\n",
    "        self.label_map = label_map\n",
    "        \n",
    "        # Class-specific window sizes (Abuse handled via Fighting)\n",
    "        self.class_windows = {\n",
    "            'Explosion': 2,      # Brief events need shorter windows\n",
    "            'Shooting': 2,\n",
    "            'Fighting': 5,       # Sustained events need longer windows\n",
    "            'Riot': 5,\n",
    "            'Car Accident': 3,   # Medium events\n",
    "        }\n",
    "        \n",
    "    def update(self, prediction_dict):\n",
    "        \"\"\"Update with class-adaptive window sizing\"\"\"\n",
    "        # Store prediction\n",
    "        self.history.append(prediction_dict)\n",
    "        \n",
    "        # Determine primary class\n",
    "        primary_class = None\n",
    "        if prediction_dict['predicted_classes']:\n",
    "            primary_class = prediction_dict['predicted_classes'][0]\n",
    "            # Remap Abuse to Fighting in case any still exists\n",
    "            if primary_class == 'Abuse':\n",
    "                primary_class = 'Fighting'\n",
    "                print(\"DEBUG: Remapping Abuse to Fighting in temporal filter\")\n",
    "        \n",
    "        # Get appropriate window size for this class\n",
    "        window_size = self.class_windows.get(primary_class, self.base_window_size)\n",
    "        \n",
    "        # Maintain appropriate history length\n",
    "        while len(self.history) > max(self.class_windows.values()):\n",
    "            self.history.pop(0)\n",
    "            \n",
    "        # Not enough history for this class\n",
    "        if len(self.history) < window_size:\n",
    "            return prediction_dict\n",
    "            \n",
    "        # Get relevant history window for this class\n",
    "        relevant_history = self.history[-window_size:]\n",
    "        \n",
    "        # Calculate weighted average (more recent predictions have higher weight)\n",
    "        weights = np.linspace(0.7, 1.0, len(relevant_history))\n",
    "        weights = weights / weights.sum()  # Normalize\n",
    "        \n",
    "        avg_predictions = np.zeros_like(relevant_history[0]['raw_predictions'])\n",
    "        for i, pred in enumerate(relevant_history):\n",
    "            avg_predictions += weights[i] * pred['raw_predictions']\n",
    "        \n",
    "        # Apply confidence boosting for consistent low-confidence predictions\n",
    "        consistency = self._calculate_consistency(relevant_history)\n",
    "        boosted_predictions = avg_predictions * (1.0 + 0.1 * consistency)\n",
    "        \n",
    "        # Threshold and create result\n",
    "        smoothed_indices = np.where(boosted_predictions >= self.threshold)[0]\n",
    "        \n",
    "        # If no class is above threshold, take the highest probability class\n",
    "        if len(smoothed_indices) == 0:\n",
    "            smoothed_indices = [np.argmax(boosted_predictions)]\n",
    "            \n",
    "        # Map indices to class names\n",
    "        if self.label_map is not None:\n",
    "            smoothed_classes = []\n",
    "            for idx in smoothed_indices:\n",
    "                if idx < len(self.label_map):\n",
    "                    label = self.label_map[idx]\n",
    "                    # Remap Abuse to Fighting\n",
    "                    if label == 'Abuse':\n",
    "                        label = 'Fighting'\n",
    "                        print(\"DEBUG: Remapping Abuse to Fighting in temporal filter output\")\n",
    "                    smoothed_classes.append(label)\n",
    "        else:\n",
    "            # Fall back to using the original prediction's label mapping method\n",
    "            smoothed_classes = [prediction_dict['predicted_classes'][0]]  # Default\n",
    "        \n",
    "        return {\n",
    "            'raw_predictions': boosted_predictions,\n",
    "            'predicted_indices': smoothed_indices,\n",
    "            'predicted_classes': smoothed_classes\n",
    "        }\n",
    "        \n",
    "    def _calculate_consistency(self, history):\n",
    "        \"\"\"Calculate how consistent predictions have been\"\"\"\n",
    "        if not history:\n",
    "            return 0\n",
    "            \n",
    "        # Extract raw predictions\n",
    "        all_preds = np.array([h['raw_predictions'] for h in history])\n",
    "        \n",
    "        # Calculate standard deviation across time for each class\n",
    "        consistency = 1.0 - np.std(all_preds, axis=0)\n",
    "        return consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabd0f0-a07c-4a32-a0d6-5aa9dec5f54b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViolenceDetectionPipeline:\n",
    "    def __init__(self):\n",
    "        # Path configurations (normalize paths)\n",
    "        self.RGB_MODEL_PATH = normalize_path('F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/v1.0/best_model_acc.pt')\n",
    "        self.FLOW_MODEL_PATH = normalize_path('F:/SRC_Bhuvaneswari/typpo/Crimenet/VisTra/Checkpoints/flow/best_model_acc.pt')\n",
    "        self.AUDIO_MODEL_PATH = normalize_path('F:/SRC_Bhuvaneswari/typpo/Crimenet/W2V/Checkpoint/wav2vec2_epoch_10.pt')\n",
    "        \n",
    "        # Constants - Keep Abuse in audio model for compatibility\n",
    "        self.VISUAL_LABEL_MAP = {0: 'Normal', 1: 'Explosion', 2: 'Fighting', 3: 'Car Accident', 4: 'Shooting', 5: 'Riot'}\n",
    "        self.AUDIO_LABEL_MAP = {0: 'Normal', 1: 'Abuse', 2: 'Explosion', 3: 'Fighting', 4: 'Car Accident', 5: 'Shooting', 6: 'Riot'}\n",
    "        \n",
    "        # Combined label map for final classification with Abuse->Fighting mapping\n",
    "        self.COMBINED_LABEL_MAP = {\n",
    "            0: 'Normal',\n",
    "            1: 'Fighting',  # Abuse mapped to Fighting\n",
    "            2: 'Explosion', \n",
    "            3: 'Fighting',\n",
    "            4: 'Car Accident', \n",
    "            5: 'Shooting', \n",
    "            6: 'Riot'\n",
    "        }\n",
    "        \n",
    "        self.CLIP_LEN = 32\n",
    "        self.FRAME_SAMPLE_RATE = 1\n",
    "        self.SAMPLING_RATE = 16000\n",
    "        self.WINDOW_SIZE_SECONDS = 3\n",
    "        \n",
    "        # Fixed frame batch size - FIXED: Always use 32 frames for ViViT\n",
    "        self.FRAME_BATCH_SIZE = 32  # Always use 32 frames for model compatibility\n",
    "        \n",
    "        # Default fusion method (can be changed via UI)\n",
    "        self.fusion_method = 'proposed'  # Options: 'advanced', 'majority', 'rgb', 'flow', 'audio', 'proposed'\n",
    "        \n",
    "        # Optimization options\n",
    "        self.low_memory_mode = False  # Enable for devices with limited VRAM\n",
    "        self.use_mixed_precision = True  # Use FP16 for inference\n",
    "        self.audio_chunk_size = 32000  # Process audio in chunks (2 seconds)\n",
    "        \n",
    "        # Adaptive thresholding parameters\n",
    "        self.base_threshold = 0.5\n",
    "        self.threshold_range = (0.35, 0.65)  # Min/max threshold range\n",
    "        \n",
    "        # Initialize multi-scale detection\n",
    "        self.multi_scale_enabled = True\n",
    "        self.scale_weights = {\n",
    "            'Explosion': (0.6, 0.3, 0.1),  # (short, medium, long) windows\n",
    "            'Shooting': (0.6, 0.3, 0.1),\n",
    "            'Fighting': (0.2, 0.3, 0.5),   # Will also handle remapped Abuse\n",
    "            'Riot': (0.2, 0.3, 0.5),\n",
    "            'Car Accident': (0.3, 0.5, 0.2)\n",
    "        }\n",
    "        \n",
    "        # Temporal filter with class-specific settings\n",
    "        self.temporal_filter = AdvancedTemporalFilter(\n",
    "            base_window_size=3, \n",
    "            threshold=self.base_threshold,\n",
    "            label_map=self.COMBINED_LABEL_MAP\n",
    "        )\n",
    "        \n",
    "        # Diagnostic mode\n",
    "        self.diagnostics_enabled = False\n",
    "        \n",
    "        # Context tracking\n",
    "        self.context_events = {}\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device} (Low memory mode: {self.low_memory_mode}, Mixed precision: {self.use_mixed_precision})\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self._init_models()\n",
    "        \n",
    "    def _init_models(self):\n",
    "        print(\"\\n==== LOADING MODELS ====\")\n",
    "        \n",
    "        # Clear CUDA cache before starting\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # RGB ViVit model\n",
    "        print(\"Loading RGB ViVit model...\")\n",
    "        self.rgb_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\", do_rescale=None, offset=None)\n",
    "        self.rgb_model = VivitForVideoClassification.from_pretrained(\n",
    "            \"google/vivit-b-16x2\",\n",
    "            num_labels=len(self.VISUAL_LABEL_MAP),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Use robust model loading\n",
    "        self._load_model_weights(self.rgb_model, self.RGB_MODEL_PATH, \"RGB ViVit\")\n",
    "        \n",
    "        self.rgb_model.to(self.device)\n",
    "        self.rgb_model.eval()\n",
    "        \n",
    "        # Enable gradient checkpointing for VRAM efficiency\n",
    "        if hasattr(self.rgb_model, 'gradient_checkpointing_enable'):\n",
    "            self.rgb_model.gradient_checkpointing_enable()\n",
    "            \n",
    "        print(\"RGB model loaded successfully\")\n",
    "        \n",
    "        # Clear cache after loading RGB model\n",
    "        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Optical Flow ViVit model\n",
    "        print(\"\\nLoading Flow ViVit model...\")\n",
    "        self.flow_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\", do_rescale=None, offset=None)\n",
    "        self.flow_model = VivitForVideoClassification.from_pretrained(\n",
    "            \"google/vivit-b-16x2\",\n",
    "            num_labels=len(self.VISUAL_LABEL_MAP),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Use robust model loading\n",
    "        self._load_model_weights(self.flow_model, self.FLOW_MODEL_PATH, \"Flow ViVit\")\n",
    "            \n",
    "        self.flow_model.to(self.device)\n",
    "        self.flow_model.eval()\n",
    "        \n",
    "        # Enable gradient checkpointing for VRAM efficiency\n",
    "        if hasattr(self.flow_model, 'gradient_checkpointing_enable'):\n",
    "            self.flow_model.gradient_checkpointing_enable()\n",
    "            \n",
    "        print(\"Flow model loaded successfully\")\n",
    "        \n",
    "        # Clear cache after loading Flow model\n",
    "        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Audio Wav2Vec model - Keep 7 classes for model compatibility\n",
    "        print(\"\\nLoading Audio Wav2Vec2 model...\")\n",
    "        self.audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            \"facebook/wav2vec2-base\",\n",
    "            num_labels=7,  # Keep 7 labels for model compatibility\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Debug print model structure\n",
    "            print(f\"Expected Audio model output shape: 7 classes (including Abuse)\")\n",
    "            state_dict = torch.load(self.AUDIO_MODEL_PATH, map_location=self.device)\n",
    "            \n",
    "            # Use robust model loading\n",
    "            self._load_model_weights(self.audio_model, self.AUDIO_MODEL_PATH, \"Audio Wav2Vec2\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading Audio model weights: {e}\")\n",
    "            print(\"Model will use default initialization!\")\n",
    "            \n",
    "        self.audio_model.to(self.device)\n",
    "        self.audio_model.eval()\n",
    "        print(\"Audio model loaded successfully\")\n",
    "        \n",
    "        # Clear cache after loading Audio model\n",
    "        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Initialize fusion components with enhanced attention\n",
    "        self._init_fusion_components()\n",
    "        \n",
    "        # Final cache clear after all models loaded\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def _load_model_weights(self, model, path, model_name):\n",
    "        \"\"\"Robust model weight loading with error checking\"\"\"\n",
    "        try:\n",
    "            # Try to load state dict directly\n",
    "            state_dict = torch.load(path, map_location=self.device)\n",
    "            \n",
    "            # Print model structure before loading\n",
    "            print(f\"\\nModel structure for {model_name}:\")\n",
    "            model_keys = set(model.state_dict().keys())\n",
    "            print(f\"  Model has {len(model_keys)} keys\")\n",
    "            \n",
    "            # Check if it's a complete state dict or just weights\n",
    "            if isinstance(state_dict, dict) and 'state_dict' in state_dict:\n",
    "                state_dict = state_dict['state_dict']\n",
    "            \n",
    "            # Print checkpoint structure\n",
    "            checkpoint_keys = set(state_dict.keys())\n",
    "            print(f\"  Checkpoint has {len(checkpoint_keys)} keys\")\n",
    "            \n",
    "            # Find missing and unexpected keys\n",
    "            missing = model_keys - checkpoint_keys\n",
    "            unexpected = checkpoint_keys - model_keys\n",
    "            matching = model_keys.intersection(checkpoint_keys)\n",
    "            \n",
    "            if missing:\n",
    "                print(f\"  Missing keys: {', '.join(list(missing)[:5])}{'...' if len(missing) > 5 else ''}\")\n",
    "            if unexpected:\n",
    "                print(f\"  Unexpected keys: {', '.join(list(unexpected)[:5])}{'...' if len(unexpected) > 5 else ''}\")\n",
    "            print(f\"  {len(matching)} matching keys\")\n",
    "            \n",
    "            # Load with strict=False to handle mismatches\n",
    "            result = model.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  Loaded with {len(result.missing_keys)} missing and {len(result.unexpected_keys)} unexpected keys\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading {model_name} weights: {e}\")\n",
    "            print(\"Model will use default initialization!\")\n",
    "            return False\n",
    "    \n",
    "    def _init_fusion_components(self):\n",
    "        print(\"\\nInitializing fusion components with all methods...\")\n",
    "        \n",
    "        hidden_dim = 768\n",
    "        \n",
    "        # Original components for existing fusion methods\n",
    "        self.cross_attention = BidirectionalCrossModalAttention(\n",
    "            dim=hidden_dim,\n",
    "            num_heads=8\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim, \n",
    "            num_heads=8, \n",
    "            batch_first=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.visual_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # New components for proposed methodology\n",
    "        self.intra_modality_fusion = IntraModalityFusion(\n",
    "            dim=hidden_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.inter_modality_fusion = InterModalityFusion(\n",
    "            dim=hidden_dim,\n",
    "            num_heads=8\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.space_time_attention = DividedSpaceTimeAttention(\n",
    "            dim=hidden_dim,\n",
    "            num_heads=8\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Final classifier with GELU activation\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, 512),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 7),  # 7 classes output for model compatibility\n",
    "            nn.Sigmoid()\n",
    "        ).to(self.device)\n",
    "        \n",
    "        print(\"All fusion components initialized successfully\")\n",
    "        \n",
    "        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def extract_frames(self, video_path):\n",
    "        \"\"\"Extract RGB frames from video with explicit path normalization.\"\"\"\n",
    "        video_path = normalize_path(video_path)\n",
    "        frames = []\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            \n",
    "            # Calculate indices to sample\n",
    "            if total_frames >= self.CLIP_LEN * self.FRAME_SAMPLE_RATE:\n",
    "                indices = np.linspace(0, total_frames - 1, self.CLIP_LEN, dtype=int)\n",
    "            else:\n",
    "                indices = np.arange(0, self.CLIP_LEN * self.FRAME_SAMPLE_RATE, self.FRAME_SAMPLE_RATE) % total_frames\n",
    "                \n",
    "            # Extract frames\n",
    "            container.seek(0)\n",
    "            for i, frame in enumerate(container.decode(video=0)):\n",
    "                if i in indices:\n",
    "                    img = frame.to_image()\n",
    "                    frames.append(transform(img))\n",
    "                    if len(frames) == self.CLIP_LEN:\n",
    "                        break\n",
    "                        \n",
    "            container.close()\n",
    "            \n",
    "            # Process frames for model input\n",
    "            if len(frames) > 0:\n",
    "                frames_tensor = torch.stack(frames)\n",
    "                frames_numpy = [frame.permute(1, 2, 0).numpy() for frame in frames]\n",
    "                return frames_numpy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frames from {video_path}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_frames_batch(self, video_path, batch_size=None):\n",
    "        \"\"\"Extract RGB frames from video with batching for temporal processing.\n",
    "        FIXED: Always uses CLIP_LEN (32) frames for model compatibility.\"\"\"\n",
    "        # Always use CLIP_LEN frames for ViViT\n",
    "        batch_size = self.CLIP_LEN\n",
    "            \n",
    "        video_path = normalize_path(video_path)\n",
    "        all_frames = []\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            stream = container.streams.video[0]\n",
    "            total_frames = stream.frames\n",
    "            fps = stream.average_rate\n",
    "            \n",
    "            print(f\"Video has {total_frames} frames at {fps} FPS\")\n",
    "            \n",
    "            # Calculate optimal step size for the requested batch size\n",
    "            step = max(1, int(total_frames / (batch_size * 1.5)))  # Extract 1.5x for safety\n",
    "            print(f\"Using step size of {step} frames\")\n",
    "            \n",
    "            batch_frames = []\n",
    "            \n",
    "            container.seek(0)\n",
    "            for i, frame in enumerate(container.decode(video=0)):\n",
    "                if i % step == 0:\n",
    "                    img = frame.to_image()\n",
    "                    batch_frames.append(transform(img))\n",
    "                    \n",
    "                    if len(batch_frames) >= batch_size:\n",
    "                        break\n",
    "            \n",
    "            container.close()\n",
    "            \n",
    "            # Handle case where we didn't get enough frames\n",
    "            while len(batch_frames) < batch_size:\n",
    "                if len(batch_frames) == 0:\n",
    "                    print(\"Could not extract any frames, creating empty frame\")\n",
    "                    batch_frames.append(torch.zeros(3, 224, 224))\n",
    "                else:\n",
    "                    print(f\"Only got {len(batch_frames)} frames, duplicating last frame\")\n",
    "                    batch_frames.append(batch_frames[-1])\n",
    "            \n",
    "            # Convert to numpy for model input\n",
    "            frames_numpy = [frame.permute(1, 2, 0).numpy() for frame in batch_frames]\n",
    "            \n",
    "            print(f\"Successfully extracted batch of {len(frames_numpy)} frames\")\n",
    "            return frames_numpy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frame batch from {video_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def compute_optical_flow(self, video_path):\n",
    "        \"\"\"\n",
    "        Compute optical flow frames from video with path normalization and enhancements.\n",
    "        \"\"\"\n",
    "        video_path = normalize_path(video_path)\n",
    "        frames = []\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video {video_path}\")\n",
    "                return None\n",
    "                \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            # Calculate indices to sample\n",
    "            if total_frames >= self.CLIP_LEN * self.FRAME_SAMPLE_RATE:\n",
    "                indices = np.linspace(0, total_frames - 1, self.CLIP_LEN + 1, dtype=int)\n",
    "            else:\n",
    "                indices = np.arange(0, self.CLIP_LEN * self.FRAME_SAMPLE_RATE + 1, self.FRAME_SAMPLE_RATE) % total_frames\n",
    "            \n",
    "            # Read first frame\n",
    "            ret, prev_frame = cap.read()\n",
    "            if not ret:\n",
    "                cap.release()\n",
    "                return None\n",
    "                \n",
    "            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Process frames\n",
    "            frame_idx = 1\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret or len(frames) >= self.CLIP_LEN:\n",
    "                    break\n",
    "                    \n",
    "                if frame_idx in indices:\n",
    "                    # Convert to grayscale\n",
    "                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    # Calculate optical flow\n",
    "                    flow = cv2.calcOpticalFlowFarneback(\n",
    "                        prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "                    )\n",
    "                    \n",
    "                    # Convert flow to RGB image\n",
    "                    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                    hsv = np.zeros((frame.shape[0], frame.shape[1], 3), dtype=np.uint8)\n",
    "                    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "                    hsv[..., 1] = 255\n",
    "                    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                    flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "                    \n",
    "                    # Transform and append\n",
    "                    flow_pil = T.ToPILImage()(flow_rgb)\n",
    "                    flow_tensor = transform(flow_pil)\n",
    "                    frames.append(flow_tensor)\n",
    "                    \n",
    "                    # Update previous frame\n",
    "                    prev_gray = gray\n",
    "                \n",
    "                frame_idx += 1\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # Handle case where we didn't get enough frames\n",
    "            while len(frames) < self.CLIP_LEN:\n",
    "                # Duplicate the last frame if needed\n",
    "                frames.append(frames[-1] if frames else torch.zeros(3, 224, 224))\n",
    "                \n",
    "            # Convert to numpy for model input\n",
    "            frames_numpy = [frame.permute(1, 2, 0).numpy() for frame in frames]\n",
    "            return frames_numpy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing optical flow from {video_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compute_optical_flow_batch(self, video_path, batch_size=None):\n",
    "        \"\"\"Compute optical flow frames from video with batching for temporal processing.\n",
    "        FIXED: Always uses CLIP_LEN (32) frames for model compatibility.\"\"\"\n",
    "        # Always use CLIP_LEN frames for ViViT\n",
    "        batch_size = self.CLIP_LEN\n",
    "            \n",
    "        video_path = normalize_path(video_path)\n",
    "        frames = []\n",
    "        transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video {video_path}\")\n",
    "                return None\n",
    "                \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            \n",
    "            print(f\"Video has {total_frames} frames at {fps} FPS\")\n",
    "            \n",
    "            # Calculate optimal step for the requested batch size\n",
    "            step = max(1, int(total_frames / (batch_size * 1.5)))  # Extract 1.5x for safety\n",
    "            print(f\"Using step size of {step} frames\")\n",
    "            \n",
    "            # Read first frame\n",
    "            ret, prev_frame = cap.read()\n",
    "            if not ret:\n",
    "                cap.release()\n",
    "                return None\n",
    "                \n",
    "            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Process frames\n",
    "            frame_idx = 1\n",
    "            while frame_idx < total_frames:\n",
    "                # Skip frames according to step\n",
    "                if frame_idx % step != 0:\n",
    "                    ret = cap.grab()  # Just grab frame without decoding\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame_idx += 1\n",
    "                    continue\n",
    "                \n",
    "                # Read and process frame\n",
    "                ret, frame = cap.retrieve() if cap.grab() else (False, None)\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Convert to grayscale\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Calculate optical flow\n",
    "                flow = cv2.calcOpticalFlowFarneback(\n",
    "                    prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "                )\n",
    "                \n",
    "                # Convert flow to RGB image\n",
    "                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "                hsv = np.zeros((frame.shape[0], frame.shape[1], 3), dtype=np.uint8)\n",
    "                hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "                hsv[..., 1] = 255\n",
    "                hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "                \n",
    "                # Transform and append\n",
    "                flow_pil = T.ToPILImage()(flow_rgb)\n",
    "                flow_tensor = transform(flow_pil)\n",
    "                frames.append(flow_tensor)\n",
    "                \n",
    "                # Update previous frame\n",
    "                prev_gray = gray\n",
    "                \n",
    "                # Check if we have enough frames\n",
    "                if len(frames) >= batch_size:\n",
    "                    break\n",
    "                    \n",
    "                frame_idx += 1\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # Handle case where we didn't get enough frames\n",
    "            while len(frames) < batch_size:\n",
    "                # Duplicate the last frame if needed\n",
    "                if len(frames) == 0:\n",
    "                    print(\"Could not extract any flow frames, creating empty frame\")\n",
    "                    frames.append(torch.zeros(3, 224, 224))\n",
    "                else:\n",
    "                    print(f\"Only got {len(frames)} flow frames, duplicating last frame\")\n",
    "                    frames.append(frames[-1])\n",
    "                \n",
    "            # Convert to numpy for model input\n",
    "            frames_numpy = [frame.permute(1, 2, 0).numpy() for frame in frames]\n",
    "            \n",
    "            print(f\"Successfully extracted batch of {len(frames_numpy)} flow frames\")\n",
    "            return frames_numpy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing optical flow batch from {video_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def extract_audio(self, video_path):\n",
    "        \"\"\"Extract audio from video file with robust resource management.\"\"\"\n",
    "        video_path = normalize_path(video_path)\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        temp_audio_path = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "        \n",
    "        try:\n",
    "            # Use context manager to ensure video is closed properly\n",
    "            with mp.VideoFileClip(video_path) as video:\n",
    "                if video.audio is None:\n",
    "                    print(f\"Warning: No audio track in video {video_path}\")\n",
    "                    # Create silent audio of the right duration\n",
    "                    silence = np.zeros(int(video.duration * self.SAMPLING_RATE))\n",
    "                    return silence\n",
    "                    \n",
    "                video.audio.write_audiofile(temp_audio_path, fps=self.SAMPLING_RATE, verbose=False, logger=None)\n",
    "            \n",
    "            # Load audio\n",
    "            waveform, sample_rate = torchaudio.load(temp_audio_path)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sample_rate != self.SAMPLING_RATE:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.SAMPLING_RATE)\n",
    "                waveform = resampler(waveform)\n",
    "                \n",
    "            # Convert to mono and to numpy\n",
    "            waveform = waveform.mean(dim=0).numpy()\n",
    "            \n",
    "            return waveform\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting audio from {video_path}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup in finally block to ensure it always happens\n",
    "            try:\n",
    "                if os.path.exists(temp_audio_path):\n",
    "                    os.remove(temp_audio_path)\n",
    "                if os.path.exists(temp_dir):\n",
    "                    os.rmdir(temp_dir)\n",
    "            except Exception as cleanup_error:\n",
    "                print(f\"Cleanup error: {cleanup_error}\")\n",
    "    \n",
    "    def process_rgb_frames(self, frames):\n",
    "        \"\"\"Process RGB frames through ViViT model and extract features.\"\"\"\n",
    "        if frames is None or len(frames) < self.CLIP_LEN:\n",
    "            print(\"Insufficient RGB frames for processing\")\n",
    "            return None\n",
    "        \n",
    "        # Use mixed precision for inference if enabled\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # Process frames with the RGB model\n",
    "                inputs = self.rgb_processor(frames, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = self.rgb_model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Extract features from the last hidden state\n",
    "                rgb_features = outputs.hidden_states[-1][:, 0, :]  # Use CLS token\n",
    "                \n",
    "        return rgb_features\n",
    "    \n",
    "    def process_rgb_frames_batch(self, frames):\n",
    "        \"\"\"Process RGB frames in batches to preserve temporal information.\"\"\"\n",
    "        if frames is None or len(frames) < 1:\n",
    "            print(\"Insufficient RGB frames for batch processing\")\n",
    "            return None\n",
    "        \n",
    "        # Use mixed precision for inference if enabled\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # Process frames with the RGB model - FIXED: Add interpolate flag\n",
    "                inputs = self.rgb_processor(frames, return_tensors=\"pt\", interpolate_pos_encoding=True)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = self.rgb_model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Extract features from the last hidden state\n",
    "                rgb_features = outputs.hidden_states[-1]\n",
    "                \n",
    "                print(f\"RGB features extracted: shape {rgb_features.shape}\")\n",
    "                \n",
    "        return rgb_features\n",
    "        \n",
    "    def process_flow_frames(self, frames):\n",
    "        \"\"\"Process optical flow frames through ViViT model and extract features.\"\"\"\n",
    "        if frames is None or len(frames) < self.CLIP_LEN:\n",
    "            print(\"Insufficient optical flow frames for processing\")\n",
    "            return None\n",
    "            \n",
    "        # Use mixed precision for inference if enabled\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # Process frames with the flow model\n",
    "                inputs = self.flow_processor(frames, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = self.flow_model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Extract features\n",
    "                flow_features = outputs.hidden_states[-1][:, 0, :]  # Use CLS token\n",
    "                \n",
    "        return flow_features\n",
    "    \n",
    "    def process_flow_frames_batch(self, frames):\n",
    "        \"\"\"Process optical flow frames in batches to preserve temporal information.\"\"\"\n",
    "        if frames is None or len(frames) < 1:\n",
    "            print(\"Insufficient flow frames for batch processing\")\n",
    "            return None\n",
    "            \n",
    "        # Use mixed precision for inference if enabled\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # Process frames with the flow model - FIXED: Add interpolate flag\n",
    "                inputs = self.flow_processor(frames, return_tensors=\"pt\", interpolate_pos_encoding=True)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = self.flow_model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Extract features - preserve all tokens for temporal info\n",
    "                flow_features = outputs.hidden_states[-1]\n",
    "                \n",
    "                print(f\"Flow features extracted: shape {flow_features.shape}\")\n",
    "                \n",
    "        return flow_features\n",
    "        \n",
    "    def process_audio(self, audio_waveform):\n",
    "        \"\"\"Process audio through Wav2Vec2 model and extract features with chunking.\"\"\"\n",
    "        if audio_waveform is None:\n",
    "            print(\"Audio waveform is None\")\n",
    "            return None\n",
    "            \n",
    "        # Process audio in chunks to manage memory usage\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                if len(audio_waveform) > self.audio_chunk_size and self.low_memory_mode:\n",
    "                    # Process in chunks\n",
    "                    chunk_features = []\n",
    "                    # Calculate how many chunks we need\n",
    "                    num_chunks = int(np.ceil(len(audio_waveform) / self.audio_chunk_size))\n",
    "                    \n",
    "                    for i in range(num_chunks):\n",
    "                        start_idx = i * self.audio_chunk_size\n",
    "                        end_idx = min(start_idx + self.audio_chunk_size, len(audio_waveform))\n",
    "                        chunk = audio_waveform[start_idx:end_idx]\n",
    "                        \n",
    "                        # Process chunk\n",
    "                        inputs = self.audio_processor(\n",
    "                            chunk, \n",
    "                            sampling_rate=self.SAMPLING_RATE, \n",
    "                            return_tensors=\"pt\", \n",
    "                            padding=True\n",
    "                        )\n",
    "                        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                        outputs = self.audio_model(**inputs, output_hidden_states=True)\n",
    "                        \n",
    "                        # Extract features from this chunk\n",
    "                        chunk_feature = outputs.hidden_states[-1][:, 0, :]\n",
    "                        chunk_features.append(chunk_feature)\n",
    "                        \n",
    "                        # Clear cache after each chunk if in low memory mode\n",
    "                        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "                            torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Average the features from all chunks\n",
    "                    audio_features = torch.mean(torch.stack(chunk_features, dim=0), dim=0)\n",
    "                else:\n",
    "                    # Process the entire audio at once\n",
    "                    inputs = self.audio_processor(\n",
    "                        audio_waveform, \n",
    "                        sampling_rate=self.SAMPLING_RATE, \n",
    "                        return_tensors=\"pt\", \n",
    "                        padding=True\n",
    "                    )\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    outputs = self.audio_model(**inputs, output_hidden_states=True)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    audio_features = outputs.hidden_states[-1][:, 0, :]  # Use CLS token\n",
    "                \n",
    "        return audio_features\n",
    "        \n",
    "    def apply_bidirectional_cross_attention(self, rgb_features, flow_features, audio_features):\n",
    "        \"\"\"Apply bidirectional cross-attention fusion between visual and audio features.\"\"\"\n",
    "        if rgb_features is None or flow_features is None or audio_features is None:\n",
    "            print(\"Missing features for cross-attention fusion\")\n",
    "            return None\n",
    "            \n",
    "        # Use mixed precision for fusion operations\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # 1. Combine RGB and flow features to get visual representation\n",
    "                combined_visual = torch.cat([rgb_features, flow_features], dim=1)\n",
    "                visual_features = self.visual_fusion(combined_visual)\n",
    "                \n",
    "                # 2. Apply bidirectional cross-attention between visual and audio\n",
    "                cross_attended = self.cross_attention(visual_features, audio_features)\n",
    "                \n",
    "                # 3. Combine the visual features with cross-attended features\n",
    "                multimodal_features = torch.cat([visual_features, cross_attended], dim=1)\n",
    "                \n",
    "        return multimodal_features\n",
    "    \n",
    "    def apply_intra_modality_fusion(self, rgb_features, flow_features):\n",
    "        \"\"\"Apply intra-modality fusion to batched RGB and optical flow features.\"\"\"\n",
    "        if rgb_features is None or flow_features is None:\n",
    "            print(\"Missing features for intra-modality fusion\")\n",
    "            return None\n",
    "            \n",
    "        # Forward through the IntraModalityFusion module\n",
    "        # This handles batched inputs automatically\n",
    "        visual_features = self.intra_modality_fusion(rgb_features, flow_features)\n",
    "        \n",
    "        return visual_features\n",
    "    \n",
    "    def apply_inter_modality_fusion(self, visual_features, audio_features):\n",
    "        \"\"\"Apply inter-modality fusion to batched visual and audio features.\"\"\"\n",
    "        if visual_features is None or audio_features is None:\n",
    "            print(\"Missing features for inter-modality fusion\")\n",
    "            return None\n",
    "            \n",
    "        # Forward through the InterModalityFusion module\n",
    "        # This handles batched inputs automatically \n",
    "        multimodal_features = self.inter_modality_fusion(visual_features, audio_features)\n",
    "        \n",
    "        return multimodal_features\n",
    "    \n",
    "    def apply_divided_space_time_attention(self, features):\n",
    "        \"\"\"Apply divided space-time attention to properly use temporal attention with batched frames.\"\"\"\n",
    "        if features is None:\n",
    "            print(\"Features for divided space-time attention are None\")\n",
    "            return None\n",
    "            \n",
    "        # Get batch size and sequence length\n",
    "        batch_size = 1  # Single video\n",
    "        \n",
    "        if len(features.shape) == 3:\n",
    "            # Input is already [batch, seq, features]\n",
    "            _, seq_len, _ = features.shape\n",
    "        else:\n",
    "            # Need to reshape\n",
    "            seq_len = 1\n",
    "            print(f\"Warning: Reshaping features of shape {features.shape} for space-time attention\")\n",
    "            \n",
    "        # Apply space-time attention with explicit batch size and sequence length\n",
    "        attended_features = self.space_time_attention(features, batch_size=batch_size, seq_len=seq_len)\n",
    "        \n",
    "        return attended_features\n",
    "    \n",
    "    def apply_proposed_methodology(self, rgb_features, flow_features, audio_features):\n",
    "        \"\"\"Apply the proposed methodology with intra/inter fusion and space-time attention\"\"\"\n",
    "        if rgb_features is None or flow_features is None or audio_features is None:\n",
    "            print(\"Missing features for proposed methodology\")\n",
    "            return None\n",
    "            \n",
    "        # Use mixed precision for fusion operations\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # Print input feature shapes for debugging\n",
    "                print(f\"RGB features shape: {rgb_features.shape}\")\n",
    "                print(f\"Flow features shape: {flow_features.shape}\")\n",
    "                print(f\"Audio features shape: {audio_features.shape}\")\n",
    "                \n",
    "                # Step 1: Intra-modality fusion for visual data (RGB + Flow)\n",
    "                visual_features = self.apply_intra_modality_fusion(rgb_features, flow_features)\n",
    "                print(f\"After intra-modality fusion: {visual_features.shape}\")\n",
    "                \n",
    "                # Step 2: Inter-modality fusion between visual and audio\n",
    "                multimodal_features = self.apply_inter_modality_fusion(visual_features, audio_features)\n",
    "                print(f\"After inter-modality fusion: {multimodal_features.shape}\")\n",
    "                \n",
    "                # Step 3: Apply divided space-time attention with batched frames\n",
    "                attended_features = self.apply_divided_space_time_attention(multimodal_features)\n",
    "                print(f\"After divided space-time attention: {attended_features.shape}\")\n",
    "                \n",
    "                # Step 4: Format for classifier (maintain compatibility with existing model)\n",
    "                # Average across frames to get a single feature vector per batch\n",
    "                if len(attended_features.shape) == 3:  # [batch_size, seq_len, hidden_dim]\n",
    "                    # FIXED: Use only the CLS token instead of averaging all tokens\n",
    "                    attended_features = attended_features[:, 0, :]\n",
    "                    visual_features = visual_features[:, 0, :]\n",
    "                \n",
    "                final_features = torch.cat([visual_features, attended_features], dim=1)\n",
    "                print(f\"Final features shape for classification: {final_features.shape}\")\n",
    "                \n",
    "        return final_features\n",
    "        \n",
    "    def classify(self, features):\n",
    "        \"\"\"Apply multi-label classification to the processed features.\"\"\"\n",
    "        if features is None:\n",
    "            print(\"Features for classification are None\")\n",
    "            return None\n",
    "            \n",
    "        # Use mixed precision for classification\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # Apply classification\n",
    "                outputs = self.classifier(features)\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    def get_adaptive_threshold(self, scene_complexity=0.5, motion_level=0.5, audio_clarity=0.5):\n",
    "        \"\"\"Calculate adaptive threshold based on content analysis.\"\"\"\n",
    "        # Start with base threshold\n",
    "        threshold = self.base_threshold\n",
    "        \n",
    "        # Adjust for difficult scenes\n",
    "        if scene_complexity > 0.7:  # Complex scene (many objects, poor lighting)\n",
    "            threshold -= 0.1        # Lower threshold to avoid missing events\n",
    "        \n",
    "        # Adjust for high motion\n",
    "        if motion_level > 0.8:      # High motion scenes\n",
    "            threshold += 0.05       # Increase threshold to reduce false positives\n",
    "            \n",
    "        # Adjust for audio quality\n",
    "        if audio_clarity < 0.4:     # Poor audio quality\n",
    "            threshold += 0.1        # Increase threshold when audio is unreliable\n",
    "            \n",
    "        # Ensure threshold stays within range\n",
    "        return max(self.threshold_range[0], min(self.threshold_range[1], threshold))\n",
    "    \n",
    "    def apply_multi_scale_detection(self, predictions, confidence_boost=None):\n",
    "        \"\"\"Apply multi-scale temporal analysis to predictions.\"\"\"\n",
    "        if not self.multi_scale_enabled:\n",
    "            return predictions\n",
    "            \n",
    "        # Get original predictions\n",
    "        raw_preds = predictions['raw_predictions']\n",
    "        \n",
    "        # Apply confidence boost from context if available\n",
    "        if confidence_boost:\n",
    "            for class_name, boost in confidence_boost.items():\n",
    "                # Find index for this class\n",
    "                for idx, label in self.COMBINED_LABEL_MAP.items():\n",
    "                    if label == class_name:\n",
    "                        raw_preds[idx] += boost\n",
    "        \n",
    "        # Apply different weighting based on class\n",
    "        for class_idx, class_name in self.COMBINED_LABEL_MAP.items():\n",
    "            if class_name in self.scale_weights:\n",
    "                # Get weights for this class\n",
    "                short_w, med_w, long_w = self.scale_weights[class_name]\n",
    "                \n",
    "                # Weighted value based on expected duration\n",
    "                weighted_pred = raw_preds[class_idx] * (short_w + med_w + long_w)\n",
    "                \n",
    "                # Update prediction\n",
    "                raw_preds[class_idx] = min(1.0, weighted_pred)\n",
    "        \n",
    "        # Recalculate predicted indices based on threshold\n",
    "        threshold = self.get_adaptive_threshold()\n",
    "        predicted_indices = np.where(raw_preds >= threshold)[0]\n",
    "        \n",
    "        # If no class is above threshold, take the highest probability class\n",
    "        if len(predicted_indices) == 0:\n",
    "            predicted_indices = [np.argmax(raw_preds)]\n",
    "            \n",
    "        # Get class names with Abuse remapping\n",
    "        predicted_classes = []\n",
    "        for idx in predicted_indices:\n",
    "            label = self.COMBINED_LABEL_MAP.get(idx, \"Unknown\")\n",
    "            # Remap Abuse to Fighting if it ever shows up\n",
    "            if label == \"Abuse\":\n",
    "                label = \"Fighting\"\n",
    "                print(\"DEBUG: Remapped Abuse to Fighting in multi-scale detection\")\n",
    "            predicted_classes.append(label)\n",
    "        \n",
    "        # Return updated predictions\n",
    "        return {\n",
    "            'raw_predictions': raw_preds,\n",
    "            'predicted_indices': predicted_indices,\n",
    "            'predicted_classes': predicted_classes\n",
    "        }\n",
    "    \n",
    "    def update_context(self, predictions):\n",
    "        \"\"\"Track contextual information for event relationships.\"\"\"\n",
    "        if not predictions or 'predicted_classes' not in predictions:\n",
    "            return None, 0\n",
    "            \n",
    "        context_boost = {}\n",
    "        duration = 0\n",
    "        \n",
    "        # Extract event types\n",
    "        events = predictions['predicted_classes']\n",
    "        \n",
    "        # Define contextual relationships and boosts\n",
    "        if 'Explosion' in events:\n",
    "            # Explosion often followed by fire/smoke/injury\n",
    "            context_boost = {'Fighting': 0.1, 'Car Accident': 0.15}\n",
    "            duration = 30  # Boost for next 30 seconds\n",
    "            \n",
    "        elif 'Car Accident' in events:\n",
    "            # Car accidents may involve subsequent fighting\n",
    "            context_boost = {'Fighting': 0.05}\n",
    "            duration = 20  # Boost for next 20 seconds\n",
    "            \n",
    "        return context_boost, duration\n",
    "    \n",
    "    def run_diagnostics(self, temp_path):\n",
    "        \"\"\"Run diagnostics on a video segment to understand prediction errors\"\"\"\n",
    "        print(\"\\n===== PREDICTION DIAGNOSTICS =====\")\n",
    "        \n",
    "        # Extract inputs\n",
    "        rgb_frames = self.extract_frames(temp_path)\n",
    "        flow_frames = self.compute_optical_flow(temp_path)\n",
    "        audio = self.extract_audio(temp_path)\n",
    "        \n",
    "        # Get raw predictions from individual models\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "            with torch.no_grad():\n",
    "                # RGB\n",
    "                rgb_inputs = self.rgb_processor(rgb_frames, return_tensors=\"pt\")\n",
    "                rgb_inputs = {k: v.to(self.device) for k, v in rgb_inputs.items()}\n",
    "                rgb_outputs = self.rgb_model(**rgb_inputs)\n",
    "                rgb_probs = torch.softmax(rgb_outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "                \n",
    "                # Flow\n",
    "                flow_inputs = self.flow_processor(flow_frames, return_tensors=\"pt\")\n",
    "                flow_inputs = {k: v.to(self.device) for k, v in flow_inputs.items()}\n",
    "                flow_outputs = self.flow_model(**flow_inputs)\n",
    "                flow_probs = torch.softmax(flow_outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "                \n",
    "                # Audio\n",
    "                audio_inputs = self.audio_processor(\n",
    "                    audio, sampling_rate=self.SAMPLING_RATE, return_tensors=\"pt\", padding=True\n",
    "                )\n",
    "                audio_inputs = {k: v.to(self.device) for k, v in audio_inputs.items()}\n",
    "                audio_outputs = self.audio_model(**audio_inputs)\n",
    "                audio_probs = torch.softmax(audio_outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "        \n",
    "        # Print raw probabilities for each model\n",
    "        print(\"RGB Probabilities:\")\n",
    "        for i, label in self.VISUAL_LABEL_MAP.items():\n",
    "            print(f\"  {label}: {rgb_probs[i]:.4f}\")\n",
    "        print(\"\\nFlow Probabilities:\")\n",
    "        for i, label in self.VISUAL_LABEL_MAP.items():\n",
    "            print(f\"  {label}: {flow_probs[i]:.4f}\")\n",
    "        print(\"\\nAudio Probabilities:\")\n",
    "        for i, label in self.AUDIO_LABEL_MAP.items():\n",
    "            # Add special note if this is abuse class being remapped\n",
    "            if i == 1:  # Abuse index\n",
    "                print(f\"  {label} (remapped to Fighting): {audio_probs[i]:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {label}: {audio_probs[i]:.4f}\")\n",
    "        \n",
    "        # Calculate majority vote as a simple reference\n",
    "        rgb_class = self.VISUAL_LABEL_MAP[np.argmax(rgb_probs)]\n",
    "        flow_class = self.VISUAL_LABEL_MAP[np.argmax(flow_probs)]\n",
    "        \n",
    "        # Handle special remapping for audio class\n",
    "        audio_idx = np.argmax(audio_probs)\n",
    "        audio_class = self.AUDIO_LABEL_MAP[audio_idx]\n",
    "        if audio_idx == 1:  # Abuse index\n",
    "            print(f\"NOTE: Audio detected 'Abuse' which is being remapped to 'Fighting'\")\n",
    "            audio_class = \"Fighting\"  # Remap\n",
    "        \n",
    "        print(f\"\\nModel predictions: RGB={rgb_class}, Flow={flow_class}, Audio={audio_class}\")\n",
    "        \n",
    "        # Try each fusion method for comparison\n",
    "        current_method = self.fusion_method\n",
    "        self.fusion_method = 'majority'\n",
    "        majority_result = self.majority_vote_prediction(rgb_class, flow_class, audio_class)\n",
    "        print(f\"Majority vote result: {majority_result['predicted_classes'][0]}\")\n",
    "        \n",
    "        # Reset fusion method\n",
    "        self.fusion_method = current_method\n",
    "        \n",
    "        # Return diagnostics for reference\n",
    "        return {\n",
    "            \"rgb\": (rgb_class, rgb_probs),\n",
    "            \"flow\": (flow_class, flow_probs),\n",
    "            \"audio\": (audio_class, audio_probs),\n",
    "            \"majority\": majority_result['predicted_classes'][0]\n",
    "        }\n",
    "    \n",
    "    def majority_vote_prediction(self, rgb_class, flow_class, audio_class):\n",
    "        \"\"\"Simple majority voting fusion as a fallback\"\"\"\n",
    "        # Process abuse remapping\n",
    "        if audio_class == \"Abuse\":\n",
    "            audio_class = \"Fighting\"\n",
    "            print(\"DEBUG: Remapped Abuse to Fighting in majority voting\")\n",
    "            \n",
    "        # Count occurrences of each class\n",
    "        votes = {}\n",
    "        for cls in [rgb_class, flow_class, audio_class]:\n",
    "            votes[cls] = votes.get(cls, 0) + 1\n",
    "        \n",
    "        # Find maximum votes\n",
    "        max_votes = max(votes.values())\n",
    "        winners = [cls for cls, count in votes.items() if count == max_votes]\n",
    "        \n",
    "        # Return the winner (or first winner if tie)\n",
    "        winner = winners[0]\n",
    "        \n",
    "        # Create a result structure similar to other methods\n",
    "        winner_idx = -1\n",
    "        for idx, label in self.COMBINED_LABEL_MAP.items():\n",
    "            if label == winner:\n",
    "                winner_idx = idx\n",
    "                break\n",
    "        \n",
    "        if winner_idx == -1:\n",
    "            # Handle case where winner isn't in combined map\n",
    "            winner_idx = 0  # Default to Normal\n",
    "            winner = self.COMBINED_LABEL_MAP[0]\n",
    "        \n",
    "        # Create fake probabilities with winner at 0.9\n",
    "        raw_preds = np.ones(len(self.COMBINED_LABEL_MAP)) * 0.1 / (len(self.COMBINED_LABEL_MAP) - 1)\n",
    "        raw_preds[winner_idx] = 0.9\n",
    "        \n",
    "        return {\n",
    "            'raw_predictions': raw_preds,\n",
    "            'predicted_indices': [winner_idx],\n",
    "            'predicted_classes': [winner]\n",
    "        }\n",
    "    \n",
    "    def _create_result_from_single(self, pred_class, label_map):\n",
    "        \"\"\"Create a standardized result from a single model prediction\"\"\"\n",
    "        # Remap Abuse to Fighting if present\n",
    "        if pred_class == \"Abuse\":\n",
    "            pred_class = \"Fighting\"\n",
    "            print(\"DEBUG: Remapped Abuse to Fighting in single-modality result\")\n",
    "            \n",
    "        # Find the equivalent in the combined map\n",
    "        combined_idx = -1\n",
    "        for idx, label in self.COMBINED_LABEL_MAP.items():\n",
    "            if label == pred_class:\n",
    "                combined_idx = idx\n",
    "                break\n",
    "        \n",
    "        # Create fake probabilities with prediction at 0.9\n",
    "        raw_preds = np.ones(len(self.COMBINED_LABEL_MAP)) * 0.1 / (len(self.COMBINED_LABEL_MAP) - 1)\n",
    "        if combined_idx >= 0:\n",
    "            raw_preds[combined_idx] = 0.9\n",
    "        \n",
    "        return {\n",
    "            'raw_predictions': raw_preds,\n",
    "            'predicted_indices': [combined_idx if combined_idx >= 0 else 0],\n",
    "            'predicted_classes': [pred_class]\n",
    "        }\n",
    "    \n",
    "    def predict(self, video_path, threshold=None):\n",
    "        \"\"\"Full prediction pipeline for a video with normalized paths and batched frame processing.\"\"\"\n",
    "        video_path = normalize_path(video_path)\n",
    "        print(f\"\\n==== PREDICTING VIOLENCE IN: {video_path} ====\")\n",
    "        \n",
    "        try:\n",
    "            # Extract inputs with fixed CLIP_LEN (32) frames for model compatibility\n",
    "            print(f\"Using batched frame extraction with {self.FRAME_BATCH_SIZE} frames for proposed fusion method\")\n",
    "            rgb_frames = self.extract_frames_batch(video_path, self.FRAME_BATCH_SIZE)\n",
    "            flow_frames = self.compute_optical_flow_batch(video_path, self.FRAME_BATCH_SIZE)\n",
    "                \n",
    "            # Audio extraction is the same for all methods\n",
    "            audio = self.extract_audio(video_path)\n",
    "            \n",
    "            if rgb_frames is None or flow_frames is None or audio is None:\n",
    "                print(\"Error: Could not process inputs\")\n",
    "                return None\n",
    "                \n",
    "            # Get individual model predictions first\n",
    "            with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):\n",
    "                with torch.no_grad():\n",
    "                    # RGB prediction - FIXED: Add interpolate flag\n",
    "                    rgb_inputs = self.rgb_processor(rgb_frames, return_tensors=\"pt\", interpolate_pos_encoding=True)\n",
    "                    rgb_inputs = {k: v.to(self.device) for k, v in rgb_inputs.items()}\n",
    "                    rgb_outputs = self.rgb_model(**rgb_inputs)\n",
    "                    rgb_logits = rgb_outputs.logits\n",
    "                    rgb_probs = torch.softmax(rgb_logits, dim=-1)[0]\n",
    "                    rgb_pred = torch.argmax(rgb_logits, dim=-1).item()\n",
    "                    rgb_class = self.VISUAL_LABEL_MAP[rgb_pred]\n",
    "                    \n",
    "                    # Flow prediction - FIXED: Add interpolate flag\n",
    "                    flow_inputs = self.flow_processor(flow_frames, return_tensors=\"pt\", interpolate_pos_encoding=True)\n",
    "                    flow_inputs = {k: v.to(self.device) for k, v in flow_inputs.items()}\n",
    "                    flow_outputs = self.flow_model(**flow_inputs)\n",
    "                    flow_logits = flow_outputs.logits\n",
    "                    flow_probs = torch.softmax(flow_logits, dim=-1)[0]\n",
    "                    flow_pred = torch.argmax(flow_logits, dim=-1).item()\n",
    "                    flow_class = self.VISUAL_LABEL_MAP[flow_pred]\n",
    "                    \n",
    "                    # Audio prediction\n",
    "                    audio_inputs = self.audio_processor(\n",
    "                        audio, sampling_rate=self.SAMPLING_RATE, return_tensors=\"pt\", padding=True\n",
    "                    )\n",
    "                    audio_inputs = {k: v.to(self.device) for k, v in audio_inputs.items()}\n",
    "                    audio_outputs = self.audio_model(**audio_inputs)\n",
    "                    audio_logits = audio_outputs.logits\n",
    "                    audio_probs = torch.softmax(audio_logits, dim=-1)[0]\n",
    "                    audio_pred = torch.argmax(audio_logits, dim=-1).item()\n",
    "                    audio_class = self.AUDIO_LABEL_MAP[audio_pred]\n",
    "                    \n",
    "                    # Special handling for Abuse -> Fighting remapping\n",
    "                    if audio_pred == 1:  # Abuse index\n",
    "                        print(\"DEBUG: Audio predicted Abuse, remapping to Fighting\")\n",
    "                        audio_class = \"Fighting\"\n",
    "            \n",
    "            # Clear memory after individual model predictions\n",
    "            if self.low_memory_mode:\n",
    "                del rgb_inputs, flow_inputs, audio_inputs\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Choose fusion method based on current setting\n",
    "            print(f\"Using fusion method: {self.fusion_method}\")\n",
    "            \n",
    "            if self.fusion_method == 'rgb':\n",
    "                # RGB-only prediction\n",
    "                return self._create_result_from_single(rgb_class, self.VISUAL_LABEL_MAP)\n",
    "                \n",
    "            elif self.fusion_method == 'flow':\n",
    "                # Flow-only prediction\n",
    "                return self._create_result_from_single(flow_class, self.VISUAL_LABEL_MAP)\n",
    "                \n",
    "            elif self.fusion_method == 'audio':\n",
    "                # Audio-only prediction\n",
    "                return self._create_result_from_single(audio_class, self.AUDIO_LABEL_MAP)\n",
    "                \n",
    "            elif self.fusion_method == 'majority':\n",
    "                # Simple majority voting\n",
    "                return self.majority_vote_prediction(rgb_class, flow_class, audio_class)\n",
    "                \n",
    "            elif self.fusion_method == 'proposed':\n",
    "                # Proposed methodology with batched intra/inter fusion and space-time attention\n",
    "                # Extract features from each modality with batching\n",
    "                rgb_features = self.process_rgb_frames_batch(rgb_frames)\n",
    "                \n",
    "                # Clear memory after RGB processing\n",
    "                if self.low_memory_mode:\n",
    "                    del rgb_frames\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                flow_features = self.process_flow_frames_batch(flow_frames)\n",
    "                \n",
    "                # Clear memory after flow processing\n",
    "                if self.low_memory_mode:\n",
    "                    del flow_frames\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                audio_features = self.process_audio(audio)\n",
    "                \n",
    "                # Clear memory after audio processing\n",
    "                if self.low_memory_mode:\n",
    "                    del audio\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Apply proposed methodology with batched processing\n",
    "                multimodal_features = self.apply_proposed_methodology(\n",
    "                    rgb_features, flow_features, audio_features\n",
    "                )\n",
    "                \n",
    "                # Clear feature variables to free memory\n",
    "                if self.low_memory_mode:\n",
    "                    del rgb_features, flow_features, audio_features\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Get adaptive threshold if not provided\n",
    "                if threshold is None:\n",
    "                    threshold = self.get_adaptive_threshold()\n",
    "                    \n",
    "                # Final classification\n",
    "                predictions = self.classify(multimodal_features)\n",
    "                \n",
    "                # Process predictions and apply filtering\n",
    "                raw_result = self._process_predictions(predictions, threshold)\n",
    "                scaled_result = self.apply_multi_scale_detection(raw_result, None)\n",
    "                result = self.temporal_filter.update(scaled_result)\n",
    "                \n",
    "                # Log results for debugging\n",
    "                print(\"Proposed Method Results:\")\n",
    "                for i, prob in enumerate(result['raw_predictions']):\n",
    "                    class_name = self.COMBINED_LABEL_MAP.get(i, f\"Unknown_{i}\")\n",
    "                    print(f\"  {class_name}: {prob:.4f}\")\n",
    "                print(f\"Predicted classes: {result['predicted_classes']}\")\n",
    "                \n",
    "                # Update context based on new predictions\n",
    "                new_context, duration = self.update_context(result)\n",
    "                if new_context and duration:\n",
    "                    context_key = f\"context_{int(time.time())}\"\n",
    "                    self.context_events[context_key] = (new_context, time.time() + duration)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            else:\n",
    "                # Advanced fusion with cross-attention (default method)\n",
    "                # Process inputs through models to get features\n",
    "                rgb_features = self.process_rgb_frames(rgb_frames)\n",
    "                \n",
    "                # Clear memory after RGB processing\n",
    "                if self.low_memory_mode:\n",
    "                    del rgb_frames\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                flow_features = self.process_flow_frames(flow_frames)\n",
    "                \n",
    "                # Clear memory after flow processing\n",
    "                if self.low_memory_mode:\n",
    "                    del flow_frames\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                audio_features = self.process_audio(audio)\n",
    "                \n",
    "                # Clear memory after audio processing\n",
    "                if self.low_memory_mode:\n",
    "                    del audio\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Apply bidirectional cross-attention fusion for multimodal integration\n",
    "                multimodal_features = self.apply_bidirectional_cross_attention(\n",
    "                    rgb_features, flow_features, audio_features\n",
    "                )\n",
    "                \n",
    "                # Clear feature variables to free memory\n",
    "                if self.low_memory_mode:\n",
    "                    del rgb_features, flow_features, audio_features\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Get adaptive threshold if not provided\n",
    "                if threshold is None:\n",
    "                    threshold = self.get_adaptive_threshold()\n",
    "                    \n",
    "                # Final classification\n",
    "                predictions = self.classify(multimodal_features)\n",
    "                \n",
    "                # Process predictions\n",
    "                raw_result = self._process_predictions(predictions, threshold)\n",
    "                \n",
    "                # Apply multi-scale analysis with context awareness\n",
    "                context_boost = None\n",
    "                if self.context_events:\n",
    "                    # Check for active contextual boosts\n",
    "                    current_time = time.time()\n",
    "                    context_boost = {}\n",
    "                    \n",
    "                    for context_type, (boost, end_time) in list(self.context_events.items()):\n",
    "                        if current_time < end_time:\n",
    "                            # This context is still active\n",
    "                            context_boost.update(boost)\n",
    "                        else:\n",
    "                            # Expired context\n",
    "                            del self.context_events[context_type]\n",
    "                \n",
    "                # Apply multi-scale analysis with context\n",
    "                scaled_result = self.apply_multi_scale_detection(raw_result, context_boost)\n",
    "                \n",
    "                # Apply temporal consistency filtering\n",
    "                result = self.temporal_filter.update(scaled_result)\n",
    "                \n",
    "                # Log results for debugging\n",
    "                print(\"Advanced Method Results:\")\n",
    "                for i, prob in enumerate(result['raw_predictions']):\n",
    "                    class_name = self.COMBINED_LABEL_MAP.get(i, f\"Unknown_{i}\")\n",
    "                    print(f\"  {class_name}: {prob:.4f}\")\n",
    "                print(f\"Predicted classes: {result['predicted_classes']}\")\n",
    "                \n",
    "                # Update context based on new predictions\n",
    "                new_context, duration = self.update_context(result)\n",
    "                if new_context and duration:\n",
    "                    context_key = f\"context_{int(time.time())}\"\n",
    "                    self.context_events[context_key] = (new_context, time.time() + duration)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction pipeline: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def _process_predictions(self, predictions, threshold):\n",
    "        \"\"\"Process raw predictions to get final output with class labels.\"\"\"\n",
    "        if predictions is None:\n",
    "            return None\n",
    "            \n",
    "        # Convert predictions to numpy and apply threshold\n",
    "        predictions_np = predictions.cpu().numpy()[0]\n",
    "        \n",
    "        # Get predicted classes with Abuse remapping\n",
    "        predicted_indices = np.where(predictions_np >= threshold)[0]\n",
    "        predicted_classes = []\n",
    "        \n",
    "        for idx in predicted_indices:\n",
    "            if idx < len(self.COMBINED_LABEL_MAP):\n",
    "                label = self.COMBINED_LABEL_MAP[idx]\n",
    "                if label == \"Abuse\":  # Remap Abuse to Fighting\n",
    "                    label = \"Fighting\"\n",
    "                    print(f\"DEBUG: Remapping Abuse (index {idx}) to Fighting in final output\")\n",
    "                predicted_classes.append(label)\n",
    "            else:\n",
    "                print(f\"WARNING: Prediction index {idx} is out of range for label map\")\n",
    "        \n",
    "        # If no class is above threshold, take the highest probability class\n",
    "        if len(predicted_classes) == 0:\n",
    "            max_idx = np.argmax(predictions_np)\n",
    "            if max_idx < len(self.COMBINED_LABEL_MAP):\n",
    "                label = self.COMBINED_LABEL_MAP[max_idx]\n",
    "                if label == \"Abuse\":  # Remap Abuse to Fighting\n",
    "                    label = \"Fighting\" \n",
    "                    print(f\"DEBUG: Remapping max-probability Abuse to Fighting in final output\")\n",
    "                predicted_classes = [label]\n",
    "                predicted_indices = [max_idx]\n",
    "            else:\n",
    "                print(f\"WARNING: Max probability index {max_idx} is out of range\")\n",
    "                predicted_classes = [\"Normal\"]  # Fallback\n",
    "                predicted_indices = [0]\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result = {\n",
    "            'raw_predictions': predictions_np,\n",
    "            'predicted_indices': predicted_indices,\n",
    "            'predicted_classes': predicted_classes\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb09f6f-4d32-4994-8454-7dadea1e2386",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define color map for different event types (Abuse removed - remapped to Fighting)\n",
    "COLOR_MAP = {\n",
    "    'Normal': QColor(50, 200, 50),      # Green\n",
    "    'Explosion': QColor(255, 127, 0),   # Orange\n",
    "    'Fighting': QColor(200, 50, 50),    # Red\n",
    "    'Car Accident': QColor(50, 50, 200),# Blue\n",
    "    'Shooting': QColor(200, 0, 200),    # Purple\n",
    "    'Riot': QColor(153, 51, 255),       # Purple-blue\n",
    "    'None': QColor(100, 100, 100)       # Gray - used for areas without any classification data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cc4fe-58f7-4314-a62e-04e4b5d9bec8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalPredictionWorker(QThread):\n",
    "    predictionReady = pyqtSignal(float, float, str, str)  # start_time, end_time, modality, anomaly_type\n",
    "    segmentProcessed = pyqtSignal(int, int)  # progress signal: current, total\n",
    "    progressUpdated = pyqtSignal(int, int)  # additional signal for UI updates\n",
    "    \n",
    "    def __init__(self, pipeline):\n",
    "        super().__init__()\n",
    "        self.pipeline = pipeline\n",
    "        self.video_segments = deque()\n",
    "        self.running = True\n",
    "        self.mutex = QMutex()\n",
    "        self.debug_mode = True  # Enable debug mode\n",
    "        self.processing = False  # Flag to track if currently processing\n",
    "        self.low_memory_mode = pipeline.low_memory_mode\n",
    "        \n",
    "    def add_video_segment(self, video_path, start_time, end_time):\n",
    "        self.mutex.lock()\n",
    "        self.video_segments.append((video_path, start_time, end_time))\n",
    "        total_segments = len(self.video_segments)\n",
    "        self.mutex.unlock()\n",
    "        \n",
    "        if self.debug_mode:\n",
    "            print(f\"Added segment for processing: {start_time:.2f}-{end_time:.2f} from {video_path}\")\n",
    "            print(f\"Queue size: {total_segments} segments\")\n",
    "        \n",
    "        # Update progress immediately\n",
    "        self.progressUpdated.emit(0, total_segments)\n",
    "    \n",
    "    def clear_queue(self):\n",
    "        \"\"\"Clear all pending work from the queue.\"\"\"\n",
    "        self.mutex.lock()\n",
    "        queue_size = len(self.video_segments)\n",
    "        self.video_segments.clear()\n",
    "        print(f\"Cleared {queue_size} pending video segments from worker queue\")\n",
    "        self.mutex.unlock()\n",
    "        \n",
    "        # Reset progress\n",
    "        self.progressUpdated.emit(0, 0)\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Process segments with enhanced progress reporting.\"\"\"\n",
    "        total_segments = 0\n",
    "        processed_segments = 0\n",
    "        \n",
    "        while self.running:\n",
    "            # Get current queue state\n",
    "            self.mutex.lock()\n",
    "            total_segments = len(self.video_segments)\n",
    "            self.mutex.unlock()\n",
    "            \n",
    "            # CRITICAL: Emit progress update even when idle\n",
    "            self.progressUpdated.emit(processed_segments, processed_segments + total_segments)\n",
    "            \n",
    "            # IMPORTANT: Force UI update - this ensures the UI thread gets to process our signal\n",
    "            QApplication.processEvents()\n",
    "            \n",
    "            if total_segments > 0 and not self.processing:\n",
    "                self.processing = True  # Set flag to indicate processing\n",
    "                \n",
    "                # Get the next segment to process\n",
    "                self.mutex.lock()\n",
    "                video_path, start_time, end_time = self.video_segments.popleft()\n",
    "                current_queue_size = len(self.video_segments)\n",
    "                self.mutex.unlock()\n",
    "                \n",
    "                processed_segments += 1\n",
    "                \n",
    "                # Report progress - emit both signals\n",
    "                self.segmentProcessed.emit(processed_segments, processed_segments + current_queue_size)\n",
    "                self.progressUpdated.emit(processed_segments, processed_segments + current_queue_size)\n",
    "                \n",
    "                # Force UI update before intensive processing\n",
    "                QApplication.processEvents()\n",
    "                \n",
    "                temp_dir = None\n",
    "                temp_path = None\n",
    "                \n",
    "                try:\n",
    "                    # Clear CUDA cache before processing new segment\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Create temporary directory and files\n",
    "                    temp_dir = tempfile.mkdtemp()\n",
    "                    temp_path = os.path.join(temp_dir, f\"temp_segment_{start_time:.2f}_{end_time:.2f}.mp4\")\n",
    "                    if self.debug_mode:\n",
    "                        print(f\"Creating temp segment at: {temp_path}\")\n",
    "                    \n",
    "                    # Process with explicit context management\n",
    "                    with mp.VideoFileClip(video_path) as video:\n",
    "                        segment = video.subclip(start_time, end_time)\n",
    "                        segment.write_videofile(temp_path, codec='libx264', \n",
    "                                              audio_codec='aac', \n",
    "                                              verbose=False, \n",
    "                                              logger=None)\n",
    "                        segment.close()\n",
    "                    \n",
    "                    # Force garbage collection after writing\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # DEBUG: Extract frame and audio info\n",
    "                    if self.debug_mode:\n",
    "                        with mp.VideoFileClip(temp_path) as clip:\n",
    "                            print(f\"Temp clip duration: {clip.duration:.2f}s, FPS: {clip.fps}\")\n",
    "                            if clip.audio:\n",
    "                                print(f\"Audio detected in clip: {clip.audio.fps}Hz\")\n",
    "                            else:\n",
    "                                print(\"WARNING: No audio detected in clip\")\n",
    "                    \n",
    "                    # Update progress after segment preparation\n",
    "                    self.progressUpdated.emit(processed_segments, processed_segments + current_queue_size)\n",
    "                    QApplication.processEvents()\n",
    "                    \n",
    "                    # Process RGB - one modality at a time to manage memory\n",
    "                    print(\"\\n--- RGB MODEL PROCESSING ---\")\n",
    "                    rgb_frames = self.pipeline.extract_frames(temp_path)\n",
    "                    if rgb_frames is not None:\n",
    "                        print(f\"RGB frames extracted: {len(rgb_frames)} frames\")\n",
    "                        \n",
    "                        # Use mixed precision for inference\n",
    "                        with torch.cuda.amp.autocast(enabled=self.pipeline.use_mixed_precision):\n",
    "                            with torch.no_grad():\n",
    "                                rgb_inputs = self.pipeline.rgb_processor(rgb_frames, return_tensors=\"pt\")\n",
    "                                rgb_inputs = {k: v.to(self.pipeline.device) for k, v in rgb_inputs.items()}\n",
    "                                rgb_outputs = self.pipeline.rgb_model(**rgb_inputs)\n",
    "                                rgb_logits = rgb_outputs.logits.detach().cpu()\n",
    "                                rgb_probs = torch.softmax(rgb_logits, dim=-1)[0]\n",
    "                                rgb_pred = torch.argmax(rgb_logits, dim=-1).item()\n",
    "                                rgb_class = self.pipeline.VISUAL_LABEL_MAP[rgb_pred]\n",
    "                        \n",
    "                        print(f\"RGB Prediction: {rgb_class} (class {rgb_pred}) with confidence {rgb_probs[rgb_pred]:.4f}\")\n",
    "                        print(\"RGB Class Probabilities:\")\n",
    "                        for i, prob in enumerate(rgb_probs):\n",
    "                            print(f\"  {self.pipeline.VISUAL_LABEL_MAP[i]}: {prob:.4f}\")\n",
    "                        \n",
    "                        # Emit signal\n",
    "                        self.predictionReady.emit(start_time, end_time, \"rgb\", rgb_class)\n",
    "                        print(f\"RGB signal emitted: {rgb_class} for {start_time:.2f}-{end_time:.2f}\")\n",
    "                        \n",
    "                        # Clear unnecessary variables\n",
    "                        del rgb_inputs, rgb_outputs, rgb_logits, rgb_probs, rgb_frames\n",
    "                        \n",
    "                        # Clear CUDA cache if in low memory mode\n",
    "                        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "                            torch.cuda.empty_cache()\n",
    "                    else:\n",
    "                        print(\"ERROR: RGB frames extraction failed\")\n",
    "                        rgb_class = \"Normal\"  # Default\n",
    "                    \n",
    "                    # Update progress after RGB processing\n",
    "                    self.progressUpdated.emit(processed_segments, processed_segments + current_queue_size)\n",
    "                    QApplication.processEvents()\n",
    "                    \n",
    "                    # Process flow\n",
    "                    print(\"\\n--- FLOW MODEL PROCESSING ---\")\n",
    "                    flow_frames = self.pipeline.compute_optical_flow(temp_path)\n",
    "                    if flow_frames is not None:\n",
    "                        print(f\"Flow frames extracted: {len(flow_frames)} frames\")\n",
    "                        \n",
    "                        # Use mixed precision for inference\n",
    "                        with torch.cuda.amp.autocast(enabled=self.pipeline.use_mixed_precision):\n",
    "                            with torch.no_grad():\n",
    "                                flow_inputs = self.pipeline.flow_processor(flow_frames, return_tensors=\"pt\")\n",
    "                                flow_inputs = {k: v.to(self.pipeline.device) for k, v in flow_inputs.items()}\n",
    "                                flow_outputs = self.pipeline.flow_model(**flow_inputs)\n",
    "                                flow_logits = flow_outputs.logits.detach().cpu()\n",
    "                                flow_probs = torch.softmax(flow_logits, dim=-1)[0]\n",
    "                                flow_pred = torch.argmax(flow_logits, dim=-1).item()\n",
    "                                flow_class = self.pipeline.VISUAL_LABEL_MAP[flow_pred]\n",
    "                        \n",
    "                        print(f\"Flow Prediction: {flow_class} (class {flow_pred}) with confidence {flow_probs[flow_pred]:.4f}\")\n",
    "                        print(\"Flow Class Probabilities:\")\n",
    "                        for i, prob in enumerate(flow_probs):\n",
    "                            print(f\"  {self.pipeline.VISUAL_LABEL_MAP[i]}: {prob:.4f}\")\n",
    "                        \n",
    "                        # Emit signal\n",
    "                        self.predictionReady.emit(start_time, end_time, \"flow\", flow_class)\n",
    "                        print(f\"Flow signal emitted: {flow_class} for {start_time:.2f}-{end_time:.2f}\")\n",
    "                        \n",
    "                        # Clear unnecessary variables\n",
    "                        del flow_inputs, flow_outputs, flow_logits, flow_probs, flow_frames\n",
    "                        \n",
    "                        # Clear CUDA cache if in low memory mode\n",
    "                        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "                            torch.cuda.empty_cache()\n",
    "                    else:\n",
    "                        print(\"ERROR: Flow frames extraction failed\")\n",
    "                        flow_class = \"Normal\"  # Default\n",
    "                    \n",
    "                    # Update progress after Flow processing\n",
    "                    self.progressUpdated.emit(processed_segments, processed_segments + current_queue_size)\n",
    "                    QApplication.processEvents()\n",
    "                    \n",
    "                    # Process audio\n",
    "                    print(\"\\n--- AUDIO MODEL PROCESSING ---\")\n",
    "                    audio = self.pipeline.extract_audio(temp_path)\n",
    "                    if audio is not None:\n",
    "                        print(f\"Audio extracted: {len(audio)} samples\")\n",
    "                        \n",
    "                        # Process audio with detailed output\n",
    "                        with torch.cuda.amp.autocast(enabled=self.pipeline.use_mixed_precision):\n",
    "                            with torch.no_grad():\n",
    "                                audio_inputs = self.pipeline.audio_processor(\n",
    "                                    audio, sampling_rate=self.pipeline.SAMPLING_RATE, return_tensors=\"pt\", padding=True\n",
    "                                )\n",
    "                                audio_inputs = {k: v.to(self.pipeline.device) for k, v in audio_inputs.items()}\n",
    "                                audio_outputs = self.pipeline.audio_model(**audio_inputs)\n",
    "                                audio_logits = audio_outputs.logits.detach().cpu()\n",
    "                                audio_probs = torch.softmax(audio_logits, dim=-1)[0]\n",
    "                                audio_pred = torch.argmax(audio_logits, dim=-1).item()\n",
    "                                audio_class = self.pipeline.AUDIO_LABEL_MAP[audio_pred]\n",
    "                                \n",
    "                                # Special handling for Abuse -> Fighting remapping\n",
    "                                if audio_pred == 1:  # Abuse index\n",
    "                                    print(\"DEBUG: Audio detected Abuse, remapping to Fighting\")\n",
    "                                    audio_class = \"Fighting\"\n",
    "                        \n",
    "                        print(f\"Audio Prediction: {audio_class} (class {audio_pred}) with confidence {audio_probs[audio_pred]:.4f}\")\n",
    "                        print(\"Audio Class Probabilities:\")\n",
    "                        for i, prob in enumerate(audio_probs):\n",
    "                            if i == 1:  # Abuse class\n",
    "                                print(f\"  {self.pipeline.AUDIO_LABEL_MAP[i]} (remapped to Fighting): {prob:.4f}\")\n",
    "                            else:\n",
    "                                print(f\"  {self.pipeline.AUDIO_LABEL_MAP[i]}: {prob:.4f}\")\n",
    "                        \n",
    "                        # Emit signal\n",
    "                        self.predictionReady.emit(start_time, end_time, \"audio\", audio_class)\n",
    "                        print(f\"Audio signal emitted: {audio_class} for {start_time:.2f}-{end_time:.2f}\")\n",
    "                        \n",
    "                        # Clear unnecessary variables\n",
    "                        del audio_inputs, audio_outputs, audio_logits, audio_probs, audio\n",
    "                        \n",
    "                        # Clear CUDA cache if in low memory mode\n",
    "                        if torch.cuda.is_available() and self.low_memory_mode:\n",
    "                            torch.cuda.empty_cache()\n",
    "                    else:\n",
    "                        print(\"ERROR: Audio extraction failed\")\n",
    "                        audio_class = \"Normal\"  # Default\n",
    "\n",
    "                    # Update progress after Audio processing\n",
    "                    self.progressUpdated.emit(processed_segments, processed_segments + current_queue_size)\n",
    "                    QApplication.processEvents()\n",
    "                    \n",
    "                    # Combined prediction using the selected fusion method\n",
    "                    print(f\"\\n--- COMBINED MODEL PROCESSING USING {self.pipeline.fusion_method.upper()} FUSION ---\")\n",
    "                    try:\n",
    "                        # Use current fusion method\n",
    "                        fusion_method = self.pipeline.fusion_method\n",
    "                        \n",
    "                        # Generate combined prediction based on fusion method\n",
    "                        if fusion_method == 'rgb':\n",
    "                            result = self.pipeline._create_result_from_single(\n",
    "                                rgb_class,\n",
    "                                self.pipeline.VISUAL_LABEL_MAP\n",
    "                            )\n",
    "                        elif fusion_method == 'flow':\n",
    "                            result = self.pipeline._create_result_from_single(\n",
    "                                flow_class,\n",
    "                                self.pipeline.VISUAL_LABEL_MAP\n",
    "                            )\n",
    "                        elif fusion_method == 'audio':\n",
    "                            result = self.pipeline._create_result_from_single(\n",
    "                                audio_class,\n",
    "                                self.pipeline.AUDIO_LABEL_MAP\n",
    "                            )\n",
    "                        elif fusion_method == 'majority':\n",
    "                            result = self.pipeline.majority_vote_prediction(rgb_class, flow_class, audio_class)\n",
    "                        elif fusion_method == 'proposed' or fusion_method == 'advanced':\n",
    "                            # Process the full segment with advanced fusion (reloads the temp file)\n",
    "                            result = self.pipeline.predict(temp_path)\n",
    "                        else:\n",
    "                            # Fallback to advanced fusion\n",
    "                            result = self.pipeline.predict(temp_path)\n",
    "                        \n",
    "                        if result and result['predicted_classes']:\n",
    "                            print(f\"Combined Prediction ({fusion_method}): {result['predicted_classes']}\")\n",
    "                            print(\"Combined Raw Probabilities:\")\n",
    "                            for i, prob in enumerate(result['raw_predictions']):\n",
    "                                label = self.pipeline.COMBINED_LABEL_MAP.get(i, f\"Class_{i}\")\n",
    "                                # Special handling for Abuse label\n",
    "                                if label == \"Abuse\":\n",
    "                                    print(f\"  {label} (remapped to Fighting): {prob:.4f}\")\n",
    "                                else:\n",
    "                                    print(f\"  {label}: {prob:.4f}\")\n",
    "                            \n",
    "                            # Print verification message for console and timeline match\n",
    "                            print(\"\\nVERIFICATION: Prediction in console matches what will appear in timeline:\")\n",
    "                            for cls in result['predicted_classes']:\n",
    "                                print(f\"  TIMELINE WILL SHOW: {cls} for {start_time:.2f}-{end_time:.2f}\")\n",
    "                                print(f\"  EMITTING SIGNAL NOW: combined, {cls}, {start_time:.2f}-{end_time:.2f}\")\n",
    "                                self.predictionReady.emit(start_time, end_time, \"combined\", cls)\n",
    "                        else:\n",
    "                            print(\"ERROR: No combined predictions generated\")\n",
    "                            \n",
    "                    except Exception as fusion_error:\n",
    "                        print(f\"Error during fusion: {fusion_error}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR in prediction worker: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                \n",
    "                finally:\n",
    "                    # Force garbage collection\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Add a delay to ensure resources are released\n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "                    # Clean up temporary files\n",
    "                    try:\n",
    "                        if temp_path and os.path.exists(temp_path):\n",
    "                            os.remove(temp_path)\n",
    "                            if self.debug_mode:\n",
    "                                print(f\"Removed temp file: {temp_path}\")\n",
    "                        if temp_dir and os.path.exists(temp_dir):\n",
    "                            os.rmdir(temp_dir)\n",
    "                            if self.debug_mode:\n",
    "                                print(f\"Removed temp dir: {temp_dir}\")\n",
    "                    except Exception as cleanup_error:\n",
    "                        print(f\"Cleanup error: {cleanup_error}\")\n",
    "                \n",
    "                # Final progress update after segment completion\n",
    "                self.progressUpdated.emit(processed_segments, processed_segments + current_queue_size)\n",
    "                QApplication.processEvents()\n",
    "                \n",
    "                # Reset processing flag and wait before next segment\n",
    "                self.processing = False\n",
    "                time.sleep(0.5)  # 500ms delay between segments\n",
    "            \n",
    "            # Use shorter sleep times and process events during wait periods\n",
    "            time.sleep(0.05)\n",
    "            QApplication.processEvents()  # Keep UI responsive during sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd8f44-6b8f-4708-988b-f86d1fcc0d8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimelineWidget(QFrame):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setMinimumHeight(80)\n",
    "        self.setStyleSheet(\"background-color: #2d2d2d;\")\n",
    "        self.anomalies = []\n",
    "        self.filtered_anomalies = None  # For interactive legends\n",
    "        self.duration = 0\n",
    "        self.debug_mode = True  # Enable debug mode\n",
    "    \n",
    "    def set_anomalies(self, anomalies):\n",
    "        if self.debug_mode:\n",
    "            print(f\"\\n==== TIMELINE WIDGET UPDATE ====\")\n",
    "            print(f\"Received {len(anomalies)} anomalies to display\")\n",
    "            for i, (start, end, anomaly) in enumerate(anomalies):\n",
    "                if i < 10:  # Limit the logging to avoid excessive output\n",
    "                    print(f\"  {i+1}. {anomaly} at {start:.2f}-{end:.2f}\")\n",
    "                elif i == 10:\n",
    "                    print(f\"  ... and {len(anomalies)-10} more\")\n",
    "        \n",
    "        self.anomalies = anomalies\n",
    "        self.filtered_anomalies = None  # Reset filtered view\n",
    "        self.update()  # Force repaint\n",
    "    \n",
    "    def set_filtered_anomalies(self, anomalies):\n",
    "        \"\"\"Set anomalies with filtering applied\"\"\"\n",
    "        self.filtered_anomalies = anomalies\n",
    "        self.update()  # Force repaint\n",
    "    \n",
    "    def set_duration(self, duration):\n",
    "        if self.debug_mode:\n",
    "            print(f\"Timeline duration set to: {duration:.2f} seconds\")\n",
    "        self.duration = duration\n",
    "        self.update()\n",
    "    \n",
    "    def paintEvent(self, event):\n",
    "        if self.duration <= 0:\n",
    "            if self.debug_mode:\n",
    "                print(\"Timeline paint skipped: Duration is zero\")\n",
    "            return\n",
    "        \n",
    "        painter = QPainter(self)\n",
    "        painter.setRenderHint(QPainter.Antialiasing)\n",
    "        \n",
    "        # Draw background\n",
    "        painter.fillRect(self.rect(), QBrush(QColor(45, 45, 45)))\n",
    "        \n",
    "        # Draw timeline base\n",
    "        painter.setPen(QPen(QColor(200, 200, 200), 1))\n",
    "        y_middle = self.height() // 2\n",
    "        painter.drawLine(0, y_middle, self.width(), y_middle)\n",
    "        \n",
    "        # Draw time markers\n",
    "        painter.setPen(QPen(QColor(150, 150, 150), 1))\n",
    "        marker_interval = self.width() / 10\n",
    "        for i in range(11):\n",
    "            x = i * marker_interval\n",
    "            painter.drawLine(int(x), y_middle - 5, int(x), y_middle + 5)\n",
    "            \n",
    "            # Draw time text\n",
    "            time_at_marker = (i / 10) * self.duration\n",
    "            minutes = int(time_at_marker / 60)\n",
    "            seconds = int(time_at_marker % 60)\n",
    "            time_text = f\"{minutes:02d}:{seconds:02d}\"\n",
    "            painter.drawText(int(x) - 15, y_middle + 20, time_text)\n",
    "        \n",
    "        # Use filtered_anomalies if available, otherwise use all anomalies\n",
    "        segments_to_draw = self.filtered_anomalies if self.filtered_anomalies is not None else self.anomalies\n",
    "        \n",
    "        # Draw anomaly segments\n",
    "        segments_drawn = 0\n",
    "        for start_time, end_time, anomaly_type in segments_to_draw:\n",
    "            if start_time >= self.duration:\n",
    "                continue\n",
    "            \n",
    "            # Calculate positions\n",
    "            start_pos = int((start_time / self.duration) * self.width())\n",
    "            end_pos = int((min(end_time, self.duration) / self.duration) * self.width())\n",
    "            \n",
    "            # Get color for anomaly type\n",
    "            color = COLOR_MAP.get(anomaly_type, QColor(100, 100, 100))\n",
    "            \n",
    "            # Draw segment\n",
    "            painter.fillRect(start_pos, 5, end_pos - start_pos, self.height() - 10, QBrush(color))\n",
    "            \n",
    "            # Draw label if segment is wide enough\n",
    "            if end_pos - start_pos > 50:\n",
    "                painter.setPen(QPen(QColor(255, 255, 255), 1))\n",
    "                painter.drawText(start_pos + 5, y_middle + 5, anomaly_type)\n",
    "            \n",
    "            segments_drawn += 1\n",
    "        \n",
    "        if self.debug_mode and segments_drawn > 0 and segments_drawn != len(segments_to_draw):\n",
    "            print(f\"Timeline painted with {segments_drawn} visible segments out of {len(segments_to_draw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872d17e-5bd6-4bcd-8b70-e28411d7e280",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InteractiveLegendWidget(QFrame):\n",
    "    legendToggled = pyqtSignal(str, bool)  # Signal: category, is_visible\n",
    "    \n",
    "    def __init__(self, color_map):\n",
    "        super().__init__()\n",
    "        self.color_map = color_map\n",
    "        self.setMinimumHeight(40)\n",
    "        self.setMaximumHeight(60)\n",
    "        self.setStyleSheet(\"background-color: #333333; border-radius: 5px;\")\n",
    "        \n",
    "        self.visibility = {label: True for label in self.color_map.keys()}\n",
    "        \n",
    "        # Create layout\n",
    "        self.layout = QHBoxLayout(self)\n",
    "        self.layout.setSpacing(15)\n",
    "        self.layout.setContentsMargins(10, 5, 10, 5)\n",
    "        \n",
    "        # Add title\n",
    "        title = QLabel(\"Legend:\")\n",
    "        title.setStyleSheet(\"color: white; font-weight: bold;\")\n",
    "        self.layout.addWidget(title)\n",
    "        \n",
    "        # Create legend items for each class\n",
    "        self.checkboxes = {}\n",
    "        for label, color in self.color_map.items():\n",
    "            if label != \"None\":  # Skip the \"None\" category in legends\n",
    "                legend_item = QWidget()\n",
    "                item_layout = QHBoxLayout(legend_item)\n",
    "                item_layout.setContentsMargins(0, 0, 0, 0)\n",
    "                item_layout.setSpacing(5)\n",
    "                \n",
    "                # Checkbox for toggling\n",
    "                checkbox = QCheckBox()\n",
    "                checkbox.setChecked(True)\n",
    "                checkbox.setStyleSheet(\"QCheckBox::indicator { width: 12px; height: 12px; }\")\n",
    "                checkbox.stateChanged.connect(lambda state, l=label: self.toggle_category(l, state))\n",
    "                self.checkboxes[label] = checkbox\n",
    "                \n",
    "                # Color indicator\n",
    "                color_box = QLabel()\n",
    "                color_box.setFixedSize(16, 16)\n",
    "                color_box.setStyleSheet(f\"background-color: rgb({color.red()}, {color.green()}, {color.blue()}); border-radius: 3px;\")\n",
    "                \n",
    "                # Text label\n",
    "                text_label = QLabel(label)\n",
    "                text_label.setStyleSheet(\"color: white; font-size: 12px;\")\n",
    "                \n",
    "                item_layout.addWidget(checkbox)\n",
    "                item_layout.addWidget(color_box)\n",
    "                item_layout.addWidget(text_label)\n",
    "                \n",
    "                self.layout.addWidget(legend_item)\n",
    "        \n",
    "        # Add stretch to keep items left-aligned\n",
    "        self.layout.addStretch(1)\n",
    "    \n",
    "    def toggle_category(self, label, state):\n",
    "        \"\"\"Toggle visibility of a category when checkbox changes\"\"\"\n",
    "        is_visible = (state == Qt.Checked)\n",
    "        self.visibility[label] = is_visible\n",
    "        self.legendToggled.emit(label, is_visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ddfe5-223f-4d9d-8f96-2bd05469343b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalTimelinePlayerWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize pipeline\n",
    "        self.pipeline = ViolenceDetectionPipeline()\n",
    "        self.current_video_path = None\n",
    "        \n",
    "        # Setup UI\n",
    "        self.setWindowTitle(\"Multimodal Violence Detection Video Player\")\n",
    "        self.setGeometry(100, 100, 1200, 800)  # Make window taller for extra controls\n",
    "        \n",
    "        # Create central widget and layout\n",
    "        self.central_widget = QWidget()\n",
    "        self.setCentralWidget(self.central_widget)\n",
    "        self.layout = QVBoxLayout(self.central_widget)\n",
    "        \n",
    "        # Video display area\n",
    "        self.video_frame = QLabel()\n",
    "        self.video_frame.setAlignment(Qt.AlignCenter)\n",
    "        self.video_frame.setMinimumSize(640, 360)\n",
    "        self.video_frame.setStyleSheet(\"background-color: black;\")\n",
    "        self.layout.addWidget(self.video_frame)\n",
    "        \n",
    "        # Current classification displays (one row with different modalities)\n",
    "        self.classifications_layout = QHBoxLayout()\n",
    "        \n",
    "        # RGB classification\n",
    "        self.rgb_label = QLabel(\"RGB: None\")\n",
    "        self.rgb_label.setAlignment(Qt.AlignCenter)\n",
    "        self.rgb_label.setStyleSheet(\"font-size: 14px; font-weight: bold;\")\n",
    "        self.classifications_layout.addWidget(self.rgb_label)\n",
    "        \n",
    "        # Flow classification\n",
    "        self.flow_label = QLabel(\"Flow: None\")\n",
    "        self.flow_label.setAlignment(Qt.AlignCenter)\n",
    "        self.flow_label.setStyleSheet(\"font-size: 14px; font-weight: bold;\")\n",
    "        self.classifications_layout.addWidget(self.flow_label)\n",
    "        \n",
    "        # Audio classification\n",
    "        self.audio_label = QLabel(\"Audio: None\")\n",
    "        self.audio_label.setAlignment(Qt.AlignCenter)\n",
    "        self.audio_label.setStyleSheet(\"font-size: 14px; font-weight: bold;\")\n",
    "        self.classifications_layout.addWidget(self.audio_label)\n",
    "        \n",
    "        # Combined classification (larger)\n",
    "        self.combined_label = QLabel(\"Combined: None\")\n",
    "        self.combined_label.setAlignment(Qt.AlignCenter)\n",
    "        self.combined_label.setStyleSheet(\"font-size: 18px; font-weight: bold;\")\n",
    "        \n",
    "        self.layout.addLayout(self.classifications_layout)\n",
    "        self.layout.addWidget(self.combined_label)\n",
    "        \n",
    "        # Timeline widget\n",
    "        self.timeline_widget = TimelineWidget()\n",
    "        self.layout.addWidget(self.timeline_widget)\n",
    "        \n",
    "        # Add interactive legends widget right after timeline\n",
    "        self.legend_widget = InteractiveLegendWidget(COLOR_MAP)\n",
    "        self.legend_widget.legendToggled.connect(self.toggle_category_visibility)\n",
    "        self.layout.addWidget(self.legend_widget)\n",
    "        \n",
    "        # Fusion method selector\n",
    "        self.fusion_layout = QHBoxLayout()\n",
    "        self.fusion_label = QLabel(\"Fusion Method:\")\n",
    "        self.fusion_dropdown = QComboBox()\n",
    "        self.fusion_dropdown.addItems([\n",
    "            \"Advanced (Cross-Attention)\", \n",
    "            \"Proposed (Intra/Inter Fusion)\", \n",
    "            \"RGB Only\", \n",
    "            \"Flow Only\", \n",
    "            \"Audio Only\",\n",
    "            \"Simple (Majority Vote)\"\n",
    "        ])\n",
    "        self.fusion_dropdown.setCurrentIndex(5)  # Default to proposed method\n",
    "        self.fusion_dropdown.currentIndexChanged.connect(self.change_fusion_method)\n",
    "        self.fusion_layout.addWidget(self.fusion_label)\n",
    "        self.fusion_layout.addWidget(self.fusion_dropdown)\n",
    "        self.layout.addLayout(self.fusion_layout)\n",
    "        \n",
    "        # Threshold slider\n",
    "        self.threshold_layout = QHBoxLayout()\n",
    "        self.threshold_label = QLabel(f\"Threshold: {self.pipeline.base_threshold:.2f}\")\n",
    "        self.threshold_slider = QSlider(Qt.Horizontal)\n",
    "        self.threshold_slider.setRange(0, 100)\n",
    "        self.threshold_slider.setValue(int(self.pipeline.base_threshold * 100))\n",
    "        self.threshold_slider.valueChanged.connect(self.update_threshold)\n",
    "        self.threshold_layout.addWidget(self.threshold_label)\n",
    "        self.threshold_layout.addWidget(self.threshold_slider)\n",
    "        self.layout.addLayout(self.threshold_layout)\n",
    "        \n",
    "        # Progress bar for segment processing\n",
    "        self.progress_layout = QHBoxLayout()\n",
    "        self.progress_label = QLabel(\"Processing segments: 0/0\")\n",
    "        self.progress_bar = QProgressBar()\n",
    "        self.progress_bar.setRange(0, 100)\n",
    "        self.progress_bar.setValue(0)\n",
    "        self.progress_layout.addWidget(self.progress_label)\n",
    "        self.progress_layout.addWidget(self.progress_bar)\n",
    "        self.layout.addLayout(self.progress_layout)\n",
    "        \n",
    "        # Controls layout\n",
    "        self.controls_layout = QHBoxLayout()\n",
    "        \n",
    "        # Play/Pause button\n",
    "        self.play_button = QPushButton()\n",
    "        self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "        self.play_button.clicked.connect(self.toggle_play)\n",
    "        self.controls_layout.addWidget(self.play_button)\n",
    "        \n",
    "        # Time display\n",
    "        self.time_label = QLabel(\"00:00 / 00:00\")\n",
    "        self.controls_layout.addWidget(self.time_label)\n",
    "        \n",
    "        # Position slider\n",
    "        self.position_slider = QSlider(Qt.Horizontal)\n",
    "        self.position_slider.sliderMoved.connect(self.set_position)\n",
    "        self.controls_layout.addWidget(self.position_slider)\n",
    "        \n",
    "        # Open file button\n",
    "        self.open_button = QPushButton(\"Open Video\")\n",
    "        self.open_button.clicked.connect(self.open_file)\n",
    "        self.controls_layout.addWidget(self.open_button)\n",
    "        \n",
    "        # Diagnostic button\n",
    "        self.diagnostic_button = QPushButton(\"Run Diagnostics\")\n",
    "        self.diagnostic_button.setStyleSheet(\"background-color: #17a2b8; color: white; font-weight: bold; padding: 8px;\")\n",
    "        self.diagnostic_button.clicked.connect(self.run_diagnostics)\n",
    "        self.controls_layout.addWidget(self.diagnostic_button)\n",
    "        \n",
    "        self.layout.addLayout(self.controls_layout)\n",
    "        \n",
    "        # Start processing button\n",
    "        self.process_layout = QHBoxLayout()\n",
    "        self.start_processing_button = QPushButton(\"▶ Start Processing\")\n",
    "        self.start_processing_button.setStyleSheet(\"background-color: #28a745; color: white; font-weight: bold; padding: 8px;\")\n",
    "        self.start_processing_button.clicked.connect(self.toggle_processing)\n",
    "        self.process_layout.addWidget(self.start_processing_button)\n",
    "        self.layout.addLayout(self.process_layout)\n",
    "        \n",
    "        # Video processing variables\n",
    "        self.cap = None\n",
    "        self.timer = QTimer(self)\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.current_frame = 0\n",
    "        self.fps = 0\n",
    "        self.total_frames = 0\n",
    "        self.playing = False\n",
    "        \n",
    "        # Prediction variables\n",
    "        self.rgb_segments = []\n",
    "        self.flow_segments = []\n",
    "        self.audio_segments = []\n",
    "        self.combined_segments = []\n",
    "        self.current_time = 0\n",
    "        self.last_prediction_time = 0\n",
    "        \n",
    "        # Set debug mode\n",
    "        self.debug_mode = True\n",
    "        \n",
    "        # Create UI refresh timer\n",
    "        self.ui_refresh_timer = QTimer(self)\n",
    "        self.ui_refresh_timer.timeout.connect(lambda: QApplication.processEvents())\n",
    "        self.ui_refresh_timer.start(100)  # Refresh UI every 100ms\n",
    "        \n",
    "        # Start prediction thread\n",
    "        self.prediction_worker = MultimodalPredictionWorker(self.pipeline)\n",
    "        \n",
    "        # Connect signals with explicit connection type for thread safety\n",
    "        self.prediction_worker.predictionReady.connect(self.update_anomaly, Qt.QueuedConnection)\n",
    "        self.prediction_worker.segmentProcessed.connect(self.update_progress, Qt.QueuedConnection)\n",
    "        self.prediction_worker.progressUpdated.connect(self.update_progress_bar, Qt.QueuedConnection)\n",
    "        self.prediction_worker.start()\n",
    "        \n",
    "        # Show file dialog upon launch\n",
    "        QTimer.singleShot(100, self.open_file)\n",
    "    \n",
    "    def update_progress_bar(self, current, total):\n",
    "        \"\"\"Enhanced progress bar update with forced refresh\"\"\"\n",
    "        if total > 0:\n",
    "            percentage = int((current / total) * 100)\n",
    "            \n",
    "            # Update progress bar with new value\n",
    "            self.progress_bar.setValue(percentage)\n",
    "            \n",
    "            # Update label with detailed information\n",
    "            self.progress_label.setText(f\"Processing segments: {current}/{total} ({percentage}%)\")\n",
    "            \n",
    "            # Print debug statement to verify updates\n",
    "            print(f\"Progress update: {percentage}% ({current}/{total})\")\n",
    "            \n",
    "            # Force immediate UI refresh\n",
    "            self.progress_bar.repaint()\n",
    "            QApplication.processEvents()\n",
    "    \n",
    "    def toggle_category_visibility(self, category, is_visible):\n",
    "        \"\"\"Handle toggling visibility of event categories\"\"\"\n",
    "        # Filter segments based on visibility settings\n",
    "        visible_segments = []\n",
    "        for start, end, event_type in self.combined_segments:\n",
    "            if event_type == category and not is_visible:\n",
    "                continue  # Skip this segment as its category is hidden\n",
    "            visible_segments.append((start, end, event_type))\n",
    "        \n",
    "        # Update timeline with filtered segments\n",
    "        self.timeline_widget.set_filtered_anomalies(visible_segments)\n",
    "    \n",
    "    def update_threshold(self, value):\n",
    "        \"\"\"Update the detection threshold based on slider value\"\"\"\n",
    "        threshold = value / 100.0\n",
    "        self.threshold_label.setText(f\"Threshold: {threshold:.2f}\")\n",
    "        self.pipeline.base_threshold = threshold\n",
    "        self.pipeline.temporal_filter.threshold = threshold\n",
    "        print(f\"Updated detection threshold to {threshold:.2f}\")\n",
    "    \n",
    "    def change_fusion_method(self, index):\n",
    "        \"\"\"Change the fusion method used for predictions\"\"\"\n",
    "        methods = [\"advanced\", \"majority\", \"rgb\", \"flow\", \"audio\", \"proposed\"]\n",
    "        self.pipeline.fusion_method = methods[index]\n",
    "        print(f\"Changed fusion method to: {self.pipeline.fusion_method}\")\n",
    "        \n",
    "        # Update threshold slider based on fusion method\n",
    "        if self.pipeline.fusion_method in [\"rgb\", \"flow\", \"audio\", \"majority\"]:\n",
    "            # Single modality methods use higher threshold\n",
    "            self.threshold_slider.setValue(70)\n",
    "            self.update_threshold(70)\n",
    "        else:\n",
    "            # Advanced fusion methods use default threshold\n",
    "            self.threshold_slider.setValue(50)\n",
    "            self.update_threshold(50)\n",
    "    \n",
    "    def toggle_processing(self):\n",
    "        \"\"\"Start or pause the background processing of video segments\"\"\"\n",
    "        if self.start_processing_button.text().startswith(\"▶\"):\n",
    "            # Start processing\n",
    "            if self.current_video_path:\n",
    "                self.start_processing_button.setText(\"⏸ Pause Processing\")\n",
    "                self.start_processing_button.setStyleSheet(\"background-color: #dc3545; color: white; font-weight: bold; padding: 8px;\")\n",
    "                \n",
    "                # Reset progress indicators before starting\n",
    "                self.progress_bar.setValue(0)\n",
    "                self.progress_label.setText(\"Processing segments: 0/0\")\n",
    "                QApplication.processEvents()  # Force immediate UI update\n",
    "                \n",
    "                self.start_background_analysis(self.current_video_path)\n",
    "        else:\n",
    "            # Pause processing\n",
    "            self.start_processing_button.setText(\"▶ Start Processing\")\n",
    "            self.start_processing_button.setStyleSheet(\"background-color: #28a745; color: white; font-weight: bold; padding: 8px;\")\n",
    "            self.prediction_worker.clear_queue()\n",
    "    \n",
    "    def run_diagnostics(self):\n",
    "        \"\"\"Run diagnostic analysis on the current video position\"\"\"\n",
    "        if not self.cap or not self.current_video_path:\n",
    "            print(\"No video loaded. Please load a video first.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n===== RUNNING MODEL DIAGNOSTICS =====\")\n",
    "        \n",
    "        # Create a short temp segment from current position\n",
    "        current_time = self.current_frame / self.fps\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        temp_path = os.path.join(temp_dir, \"diagnostic_segment.mp4\")\n",
    "        \n",
    "        try:\n",
    "            # Extract 3-second clip around current position\n",
    "            with mp.VideoFileClip(self.current_video_path) as video:\n",
    "                start_time = max(0, current_time - 1.5)\n",
    "                end_time = min(video.duration, current_time + 1.5)\n",
    "                segment = video.subclip(start_time, end_time)\n",
    "                segment.write_videofile(temp_path, codec='libx264', \n",
    "                                      audio_codec='aac', \n",
    "                                      verbose=False,\n",
    "                                      logger=None)\n",
    "            \n",
    "            # Run diagnostics\n",
    "            results = self.pipeline.run_diagnostics(temp_path)\n",
    "            \n",
    "            # Show simple vote result as reference\n",
    "            rgb_class, rgb_probs = results[\"rgb\"] \n",
    "            flow_class, flow_probs = results[\"flow\"]\n",
    "            audio_class, audio_probs = results[\"audio\"]\n",
    "            majority_vote = results[\"majority\"]\n",
    "            \n",
    "            # Create visualization of all probabilities\n",
    "            fig = plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # RGB subplot\n",
    "            ax1 = fig.add_subplot(311)\n",
    "            ax1.bar(list(self.pipeline.VISUAL_LABEL_MAP.values()), rgb_probs)\n",
    "            ax1.set_title(\"RGB Predictions\")\n",
    "            ax1.set_ylim(0, 1)\n",
    "            \n",
    "            # Flow subplot\n",
    "            ax2 = fig.add_subplot(312)\n",
    "            ax2.bar(list(self.pipeline.VISUAL_LABEL_MAP.values()), flow_probs)\n",
    "            ax2.set_title(\"Flow Predictions\")\n",
    "            ax2.set_ylim(0, 1)\n",
    "            \n",
    "            # Audio subplot\n",
    "            ax3 = fig.add_subplot(313)\n",
    "            ax3.bar(list(self.pipeline.AUDIO_LABEL_MAP.values()), audio_probs)\n",
    "            ax3.set_title(\"Audio Predictions\")\n",
    "            ax3.set_ylim(0, 1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the diagnostic figure\n",
    "            diag_image_path = os.path.join(temp_dir, \"diagnostics.png\")\n",
    "            plt.savefig(diag_image_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Create diagnostic message\n",
    "            diagnostic_text = (\n",
    "                f\"DIAGNOSTIC RESULTS at {self.format_time(current_time)}:\\n\\n\"\n",
    "                f\"RGB Model: {rgb_class} ({rgb_probs[list(self.pipeline.VISUAL_LABEL_MAP.values()).index(rgb_class)]:.3f})\\n\"\n",
    "                f\"Flow Model: {flow_class} ({flow_probs[list(self.pipeline.VISUAL_LABEL_MAP.values()).index(flow_class)]:.3f})\\n\"\n",
    "                f\"Audio Model: {audio_class} ({audio_probs[list(self.pipeline.AUDIO_LABEL_MAP.values()).index(audio_class)]:.3f})\\n\\n\"\n",
    "                f\"Majority Vote: {majority_vote}\\n\"\n",
    "                f\"Current Fusion Method: {self.pipeline.fusion_method}\\n\"\n",
    "                f\"Current Threshold: {self.pipeline.base_threshold:.2f}\"\n",
    "            )\n",
    "            \n",
    "            # Show dialog with results\n",
    "            msg_box = QMessageBox()\n",
    "            msg_box.setWindowTitle(\"Diagnostic Results\")\n",
    "            msg_box.setText(diagnostic_text)\n",
    "            msg_box.setIconPixmap(QPixmap(diag_image_path).scaled(600, 400, Qt.KeepAspectRatio))\n",
    "            msg_box.exec_()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error running diagnostics: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            # Clean up\n",
    "            try:\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "                if os.path.exists(os.path.join(temp_dir, \"diagnostics.png\")):\n",
    "                    os.remove(os.path.join(temp_dir, \"diagnostics.png\"))\n",
    "                if os.path.exists(temp_dir):\n",
    "                    os.rmdir(temp_dir)\n",
    "            except Exception as cleanup_error:\n",
    "                print(f\"Cleanup error: {cleanup_error}\")\n",
    "    \n",
    "    def update_progress(self, current, total):\n",
    "        \"\"\"Update progress bar and label for segment processing.\"\"\"\n",
    "        percentage = int((current / total) * 100) if total > 0 else 0\n",
    "        self.progress_bar.setValue(percentage)\n",
    "        self.progress_label.setText(f\"Processing segments: {current}/{total} ({percentage}%)\")\n",
    "    \n",
    "    def open_file(self):\n",
    "        file_path, _ = QFileDialog.getOpenFileName(self, \"Open Video File\", \"\",\n",
    "            \"Video Files (*.mp4 *.avi *.mkv *.mov);;All Files (*)\")\n",
    "        if file_path:\n",
    "            self.load_video(normalize_path(file_path))\n",
    "    \n",
    "    def load_video(self, video_path):\n",
    "        print(f\"\\n==== LOADING VIDEO: {video_path} ====\")\n",
    "        \n",
    "        # Stop current video if playing\n",
    "        if self.playing:\n",
    "            self.timer.stop()\n",
    "            self.playing = False\n",
    "        \n",
    "        # Clean up resources from previous video\n",
    "        self.cleanup_temp_resources()\n",
    "        \n",
    "        # Release previous capture if any\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "        \n",
    "        # Reset all UI elements\n",
    "        self.reset_ui_state()\n",
    "        \n",
    "        # Open new video\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        if not self.cap.isOpened():\n",
    "            print(f\"ERROR: Could not open video {video_path}\")\n",
    "            return\n",
    "        \n",
    "        # Get video properties\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f\"Video properties: {self.total_frames} frames, {self.fps} FPS\")\n",
    "        print(f\"Video duration: {self.format_time(self.total_frames / self.fps)}\")\n",
    "        \n",
    "        self.current_frame = 0\n",
    "        self.current_time = 0\n",
    "        self.last_prediction_time = 0\n",
    "        self.current_video_path = video_path\n",
    "        \n",
    "        # Clear previous anomalies\n",
    "        self.rgb_segments = []\n",
    "        self.flow_segments = []\n",
    "        self.audio_segments = []\n",
    "        self.combined_segments = []\n",
    "        print(\"Cleared previous anomalies\")\n",
    "        \n",
    "        # Get video duration\n",
    "        duration = self.total_frames / self.fps\n",
    "        \n",
    "        # Initialize timeline\n",
    "        self.timeline_widget.set_anomalies([])\n",
    "        self.timeline_widget.set_duration(duration)\n",
    "        \n",
    "        # Reset UI\n",
    "        self.position_slider.setRange(0, self.total_frames)\n",
    "        duration_str = self.format_time(duration)\n",
    "        self.time_label.setText(f\"00:00 / {duration_str}\")\n",
    "        \n",
    "        # Show first frame\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            self.display_frame(frame)\n",
    "        \n",
    "        print(\"Video loaded successfully\")\n",
    "        \n",
    "        # Reset processing button\n",
    "        self.start_processing_button.setText(\"▶ Start Processing\")\n",
    "        self.start_processing_button.setStyleSheet(\"background-color: #28a745; color: white; font-weight: bold; padding: 8px;\")\n",
    "    \n",
    "    def cleanup_temp_resources(self):\n",
    "        \"\"\"Clean up all temporary resources and files.\"\"\"\n",
    "        print(\"Cleaning up temporary resources...\")\n",
    "        \n",
    "        # Cancel any pending predictions\n",
    "        self.prediction_worker.clear_queue()\n",
    "        \n",
    "        # Clean temp directories in the system temp folder\n",
    "        temp_dirs = [d for d in os.listdir(tempfile.gettempdir()) \n",
    "                     if d.startswith('tmp') and os.path.isdir(os.path.join(tempfile.gettempdir(), d))]\n",
    "        \n",
    "        for d in temp_dirs:\n",
    "            try:\n",
    "                temp_path = os.path.join(tempfile.gettempdir(), d)\n",
    "                # Check if directory contains our temp video segments\n",
    "                if any(f.endswith('.mp4') or f.endswith('.wav') for f in os.listdir(temp_path) \n",
    "                      if os.path.isfile(os.path.join(temp_path, f))):\n",
    "                    # Force garbage collection to release any file handles\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # Remove files in directory\n",
    "                    for f in os.listdir(temp_path):\n",
    "                        try:\n",
    "                            file_path = os.path.join(temp_path, f)\n",
    "                            if os.path.isfile(file_path):\n",
    "                                os.remove(file_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error removing file {f}: {e}\")\n",
    "                    \n",
    "                    # Remove directory\n",
    "                    os.rmdir(temp_path)\n",
    "                    print(f\"Cleaned up temporary directory: {temp_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning temp directory {d}: {e}\")\n",
    "        \n",
    "        # Reset model caches if needed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleanup complete\")\n",
    "    \n",
    "    def reset_ui_state(self):\n",
    "        \"\"\"Reset all UI elements to their initial state.\"\"\"\n",
    "        print(\"Resetting UI state...\")\n",
    "        \n",
    "        # Reset classification labels\n",
    "        self.rgb_label.setText(\"RGB: None\")\n",
    "        self.rgb_label.setStyleSheet(\"font-size: 14px; font-weight: bold;\")\n",
    "        \n",
    "        self.flow_label.setText(\"Flow: None\")\n",
    "        self.flow_label.setStyleSheet(\"font-size: 14px; font-weight: bold;\")\n",
    "        \n",
    "        self.audio_label.setText(\"Audio: None\")\n",
    "        self.audio_label.setStyleSheet(\"font-size: 14px; font-weight: bold;\")\n",
    "        \n",
    "        self.combined_label.setText(\"Combined: None\")\n",
    "        self.combined_label.setStyleSheet(\"font-size: 18px; font-weight: bold;\")\n",
    "        \n",
    "        # Reset progress bar\n",
    "        self.progress_bar.setValue(0)\n",
    "        self.progress_label.setText(\"Processing segments: 0/0\")\n",
    "        \n",
    "        # Clear video frame\n",
    "        self.video_frame.clear()\n",
    "        self.video_frame.setPixmap(QPixmap())\n",
    "        \n",
    "        # Reset timeline\n",
    "        self.timeline_widget.set_anomalies([])\n",
    "        self.timeline_widget.set_duration(0)\n",
    "        \n",
    "        # Reset time label\n",
    "        self.time_label.setText(\"00:00 / 00:00\")\n",
    "        \n",
    "        # Reset slider\n",
    "        self.position_slider.setValue(0)\n",
    "        self.position_slider.setRange(0, 100)  # Default range until video is loaded\n",
    "    \n",
    "    def start_background_analysis(self, video_path):\n",
    "        \"\"\"Start background processing of the entire video for timeline.\"\"\"\n",
    "        try:\n",
    "            # Get video duration with proper resource management\n",
    "            with mp.VideoFileClip(video_path) as video:\n",
    "                duration = video.duration\n",
    "                \n",
    "            # Create segments with 50% overlap for better temporal coverage\n",
    "            # Using variable segment length based on video duration for more efficient processing\n",
    "            if duration <= 60:  # For short videos (under 1 minute)\n",
    "                segment_length = 3\n",
    "                overlap = 1.5\n",
    "            elif duration <= 180:  # For medium videos (1-3 minutes)\n",
    "                segment_length = 4\n",
    "                overlap = 2\n",
    "            else:  # For longer videos\n",
    "                segment_length = 5\n",
    "                overlap = 2.5\n",
    "            \n",
    "            start_times = np.arange(0, duration - segment_length/2, segment_length - overlap)\n",
    "            \n",
    "            total_segments = len(start_times)\n",
    "            print(f\"Preparing to analyze video in {total_segments} segments...\")\n",
    "            \n",
    "            # Initialize progress tracking\n",
    "            self.progress_bar.setRange(0, total_segments)\n",
    "            self.progress_bar.setValue(0)\n",
    "            self.progress_label.setText(f\"Processing segments: 0/{total_segments} (0%)\")\n",
    "            \n",
    "            # Queue segments for processing\n",
    "            for start_time in start_times:\n",
    "                end_time = min(start_time + segment_length, duration)\n",
    "                self.prediction_worker.add_video_segment(video_path, start_time, end_time)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing video analysis: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def toggle_play(self):\n",
    "        if self.cap is None:\n",
    "            return\n",
    "        \n",
    "        if self.playing:\n",
    "            self.timer.stop()\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "        else:\n",
    "            self.timer.start(1000 // 30)  # 30 fps display\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPause))\n",
    "        \n",
    "        self.playing = not self.playing\n",
    "    \n",
    "    def update_frame(self):\n",
    "        if self.cap is None or not self.playing:\n",
    "            return\n",
    "        \n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            # End of video - perform cleanup\n",
    "            print(\"End of video reached, performing cleanup...\")\n",
    "            self.timer.stop()\n",
    "            self.playing = False\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "            \n",
    "            # Clean up temporary resources when video ends\n",
    "            self.cleanup_temp_resources()\n",
    "            return\n",
    "        \n",
    "        # Display the frame\n",
    "        self.display_frame(frame)\n",
    "        \n",
    "        # Update current time\n",
    "        self.current_time = self.current_frame / self.fps\n",
    "        \n",
    "        # Debug the timeline position periodically (every 30 frames)\n",
    "        if self.debug_mode and self.current_frame % 30 == 0:\n",
    "            print(f\"\\n==== CURRENT POSITION: {self.format_time(self.current_time)} ====\")\n",
    "            print(f\"RGB anomalies: {len(self.rgb_segments)}\")\n",
    "            print(f\"Flow anomalies: {len(self.flow_segments)}\")\n",
    "            print(f\"Audio anomalies: {len(self.audio_segments)}\")\n",
    "            print(f\"Combined anomalies: {len(self.combined_segments)}\")\n",
    "        \n",
    "        # Update slider and time label\n",
    "        self.position_slider.setValue(self.current_frame)\n",
    "        current_time_str = self.format_time(self.current_time)\n",
    "        duration_str = self.format_time(self.total_frames / self.fps)\n",
    "        self.time_label.setText(f\"{current_time_str} / {duration_str}\")\n",
    "        \n",
    "        # Update classification display\n",
    "        rgb_class = self.get_classification_at_time(self.current_time, self.rgb_segments)\n",
    "        flow_class = self.get_classification_at_time(self.current_time, self.flow_segments)\n",
    "        audio_class = self.get_classification_at_time(self.current_time, self.audio_segments)\n",
    "        combined_class = self.get_classification_at_time(self.current_time, self.combined_segments)\n",
    "        \n",
    "        # Print classifications periodically\n",
    "        if self.debug_mode and self.current_frame % 30 == 0:\n",
    "            print(f\"Current classifications at {self.format_time(self.current_time)}:\")\n",
    "            print(f\"  RGB: {rgb_class}\")\n",
    "            print(f\"  Flow: {flow_class}\")\n",
    "            print(f\"  Audio: {audio_class}\")\n",
    "            print(f\"  Combined: {combined_class}\")\n",
    "        \n",
    "        self.rgb_label.setText(f\"RGB: {rgb_class}\")\n",
    "        self.flow_label.setText(f\"Flow: {flow_class}\")\n",
    "        self.audio_label.setText(f\"Audio: {audio_class}\")\n",
    "        self.combined_label.setText(f\"Combined: {combined_class}\")\n",
    "        \n",
    "        # Set classification colors\n",
    "        self.set_label_color(self.rgb_label, rgb_class)\n",
    "        self.set_label_color(self.flow_label, flow_class)\n",
    "        self.set_label_color(self.audio_label, audio_class)\n",
    "        self.set_label_color(self.combined_label, combined_class)\n",
    "        \n",
    "        # Increment frame counter\n",
    "        self.current_frame += 1\n",
    "    \n",
    "    def set_label_color(self, label, class_name):\n",
    "        \"\"\"Set label color based on the class name.\"\"\"\n",
    "        if class_name == \"None\":\n",
    "            if label == self.combined_label:\n",
    "                label.setStyleSheet(\"font-size: 18px; font-weight: bold;\")\n",
    "            else:\n",
    "                label.setStyleSheet(\"font-size: 14px; font-weight: bold;\")\n",
    "            return\n",
    "            \n",
    "        color = COLOR_MAP.get(class_name, QColor(100, 100, 100))\n",
    "        \n",
    "        # Combined label is larger\n",
    "        if label == self.combined_label:\n",
    "            label.setStyleSheet(f\"font-size: 18px; font-weight: bold; color: rgb({color.red()}, {color.green()}, {color.blue()})\")\n",
    "        else:\n",
    "            label.setStyleSheet(f\"font-size: 14px; font-weight: bold; color: rgb({color.red()}, {color.green()}, {color.blue()})\")\n",
    "    \n",
    "    def display_frame(self, frame):\n",
    "        \"\"\"Convert frame to QImage and display it.\"\"\"\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = frame_rgb.shape\n",
    "        bytes_per_line = ch * w\n",
    "        q_img = QImage(frame_rgb.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "        self.video_frame.setPixmap(QPixmap.fromImage(q_img).scaled(\n",
    "            self.video_frame.width(), self.video_frame.height(),\n",
    "            Qt.KeepAspectRatio, Qt.SmoothTransformation))\n",
    "    \n",
    "    def set_position(self, position):\n",
    "        \"\"\"Set video playback position.\"\"\"\n",
    "        if self.cap is None:\n",
    "            return\n",
    "        \n",
    "        # Seek to position\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, position)\n",
    "        self.current_frame = position\n",
    "        self.current_time = position / self.fps\n",
    "        \n",
    "        # Read and display the frame\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            self.display_frame(frame)\n",
    "        \n",
    "        # Update time label\n",
    "        current_time_str = self.format_time(self.current_time)\n",
    "        duration_str = self.format_time(self.total_frames / self.fps)\n",
    "        self.time_label.setText(f\"{current_time_str} / {duration_str}\")\n",
    "        \n",
    "        # Update classification displays\n",
    "        rgb_class = self.get_classification_at_time(self.current_time, self.rgb_segments)\n",
    "        flow_class = self.get_classification_at_time(self.current_time, self.flow_segments)\n",
    "        audio_class = self.get_classification_at_time(self.current_time, self.audio_segments)\n",
    "        combined_class = self.get_classification_at_time(self.current_time, self.combined_segments)\n",
    "        \n",
    "        self.rgb_label.setText(f\"RGB: {rgb_class}\")\n",
    "        self.flow_label.setText(f\"Flow: {flow_class}\")\n",
    "        self.audio_label.setText(f\"Audio: {audio_class}\")\n",
    "        self.combined_label.setText(f\"Combined: {combined_class}\")\n",
    "        \n",
    "        # Set classification colors\n",
    "        self.set_label_color(self.rgb_label, rgb_class)\n",
    "        self.set_label_color(self.flow_label, flow_class)\n",
    "        self.set_label_color(self.audio_label, audio_class)\n",
    "        self.set_label_color(self.combined_label, combined_class)\n",
    "    \n",
    "    def update_anomaly(self, start_time, end_time, modality, anomaly_type):\n",
    "        \"\"\"Update anomaly segments from prediction thread with debugging.\"\"\"\n",
    "        if self.debug_mode:\n",
    "            print(f\"\\n==== ANOMALY RECEIVED: {modality.upper()} ====\")\n",
    "            print(f\"Time: {start_time:.2f}-{end_time:.2f}, Type: {anomaly_type}\")\n",
    "        \n",
    "        if modality == \"rgb\":\n",
    "            self.rgb_segments.append((start_time, end_time, anomaly_type))\n",
    "            if self.debug_mode:\n",
    "                print(f\"Added to rgb_segments. Total: {len(self.rgb_segments)}\")\n",
    "        elif modality == \"flow\":\n",
    "            self.flow_segments.append((start_time, end_time, anomaly_type))\n",
    "            if self.debug_mode:\n",
    "                print(f\"Added to flow_segments. Total: {len(self.flow_segments)}\")\n",
    "        elif modality == \"audio\":\n",
    "            self.audio_segments.append((start_time, end_time, anomaly_type))\n",
    "            if self.debug_mode:\n",
    "                print(f\"Added to audio_segments. Total: {len(self.audio_segments)}\")\n",
    "        elif modality == \"combined\":\n",
    "            self.combined_segments.append((start_time, end_time, anomaly_type))\n",
    "            if self.debug_mode:\n",
    "                print(f\"Added to combined_segments. Total: {len(self.combined_segments)}\")\n",
    "        \n",
    "        # Update timeline with combined segments\n",
    "        if modality == \"combined\":\n",
    "            if self.debug_mode:\n",
    "                print(f\"Updating timeline with {len(self.combined_segments)} combined segments\")\n",
    "            self.timeline_widget.set_anomalies(self.combined_segments)\n",
    "        \n",
    "        # Force UI update if we're at this time point\n",
    "        if start_time <= self.current_time <= end_time:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Updating UI labels for current time ({self.current_time:.2f})\")\n",
    "            if modality == \"rgb\":\n",
    "                self.rgb_label.setText(f\"RGB: {anomaly_type}\")\n",
    "                self.set_label_color(self.rgb_label, anomaly_type)\n",
    "            elif modality == \"flow\":\n",
    "                self.flow_label.setText(f\"Flow: {anomaly_type}\")\n",
    "                self.set_label_color(self.flow_label, anomaly_type)\n",
    "            elif modality == \"audio\":\n",
    "                self.audio_label.setText(f\"Audio: {anomaly_type}\")\n",
    "                self.set_label_color(self.audio_label, anomaly_type)\n",
    "            elif modality == \"combined\":\n",
    "                self.combined_label.setText(f\"Combined: {anomaly_type}\")\n",
    "                self.set_label_color(self.combined_label, anomaly_type)\n",
    "    \n",
    "    def get_classification_at_time(self, time_point, segments):\n",
    "        \"\"\"Find the classification for the current time with debugging.\"\"\"\n",
    "        if len(segments) == 0:\n",
    "            return \"None\"\n",
    "            \n",
    "        matching_segments = []\n",
    "        for start_time, end_time, anomaly_type in reversed(segments):\n",
    "            if start_time <= time_point <= end_time:\n",
    "                matching_segments.append((start_time, end_time, anomaly_type))\n",
    "        \n",
    "        if matching_segments:\n",
    "            # Take the first match (most recent in the reversed list)\n",
    "            start_time, end_time, anomaly_type = matching_segments[0]\n",
    "            return anomaly_type\n",
    "        \n",
    "        return \"None\"\n",
    "    \n",
    "    def format_time(self, seconds):\n",
    "        \"\"\"Format time in seconds to HH:MM:SS or MM:SS format.\"\"\"\n",
    "        minutes, seconds = divmod(int(seconds), 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\" if hours else f\"{minutes:02d}:{seconds:02d}\"\n",
    "    \n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Handle window close event with thorough cleanup.\"\"\"\n",
    "        print(\"Application closing, performing final cleanup...\")\n",
    "        \n",
    "        # Stop playback\n",
    "        if self.playing:\n",
    "            self.timer.stop()\n",
    "            self.playing = False\n",
    "        \n",
    "        # Clean up all temporary resources\n",
    "        self.cleanup_temp_resources()\n",
    "        \n",
    "        # Release video capture\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "        \n",
    "        # Stop prediction worker thread\n",
    "        if hasattr(self, 'prediction_worker'):\n",
    "            self.prediction_worker.stop()\n",
    "            self.prediction_worker.wait()\n",
    "        \n",
    "        # Release CUDA memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"Application cleanup complete\")\n",
    "        event.accept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975a254-f9eb-4451-ba98-717881352e35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main entry point\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = MultimodalTimelinePlayerWindow()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a7b3d-47ef-4ed9-a6c1-d708f75979fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVi",
   "language": "python",
   "name": "vivi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
