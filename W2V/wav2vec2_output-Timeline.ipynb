{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d65e45-3f9a-48c3-a145-e8ac74662b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wav2Vec2 model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import tempfile\n",
    "import moviepy.editor as mp\n",
    "from collections import deque\n",
    "from threading import Thread, Lock\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, \n",
    "                            QHBoxLayout, QLabel, QPushButton, QSlider, QStyle, \n",
    "                            QFileDialog, QFrame)\n",
    "from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QThread, QMutex\n",
    "from PyQt5.QtGui import QImage, QPixmap, QPainter, QColor, QPen, QBrush\n",
    "\n",
    "# Paths and constants\n",
    "SAVED_MODEL_PATH = 'F:/SRC_Bhuvaneswari/typpo/Crimenet/W2V/Checkpoint/wav2vec2_epoch_10.pt'\n",
    "LABEL_MAP = {0: 'Normal', 1: 'Abuse', 2: 'Explosion', 3: 'Fighting', 4: 'Car Accident', 5: 'Shooting', 6: 'Riot'}\n",
    "COLOR_MAP = {\n",
    "    'Normal': QColor(50, 200, 50),     # Green\n",
    "    'Abuse': QColor(255, 0, 0),        # Bright Red\n",
    "    'Explosion': QColor(255, 127, 0),  # Orange\n",
    "    'Fighting': QColor(200, 50, 50),   # Red\n",
    "    'Car Accident': QColor(50, 50, 200), # Blue\n",
    "    'Shooting': QColor(200, 0, 200),   # Purple\n",
    "    'Riot': QColor(255, 255, 0)        # Yellow\n",
    "}\n",
    "SAMPLING_RATE = 16000\n",
    "WINDOW_SIZE_SECONDS = 3  # Process 3 seconds of audio at a time\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading Wav2Vec2 model and processor...\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\",\n",
    "    num_labels=len(LABEL_MAP)\n",
    ")\n",
    "model.load_state_dict(torch.load(SAVED_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da18392c-e148-45dd-b0ed-89c3290d9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for audio prediction worker thread\n",
    "class AudioPredictionWorker(QThread):\n",
    "    predictionReady = pyqtSignal(float, float, str)\n",
    "    \n",
    "    def __init__(self, model, processor, sampling_rate):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_queue = deque()\n",
    "        self.running = True\n",
    "        self.mutex = QMutex()\n",
    "        \n",
    "    def add_audio_segment(self, audio_segment, start_time, end_time):\n",
    "        self.mutex.lock()\n",
    "        self.audio_queue.append((audio_segment, start_time, end_time))\n",
    "        self.mutex.unlock()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        \n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            if self.audio_queue:\n",
    "                self.mutex.lock()\n",
    "                audio_segment, start_time, end_time = self.audio_queue.popleft()\n",
    "                self.mutex.unlock()\n",
    "                \n",
    "                # Process and make prediction\n",
    "                try:\n",
    "                    # Process the audio\n",
    "                    inputs = self.processor(audio_segment, sampling_rate=self.sampling_rate, \n",
    "                                           return_tensors=\"pt\", padding=True)\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        logits = outputs.logits\n",
    "                        predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "                        predicted_class = LABEL_MAP[predicted_id]\n",
    "                    \n",
    "                    # Emit the prediction result\n",
    "                    self.predictionReady.emit(start_time, end_time, predicted_class)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error making prediction: {e}\")\n",
    "            \n",
    "            # Sleep to avoid high CPU usage\n",
    "            time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37de029-e3de-4f99-83c3-8cb6cbee0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio extraction class\n",
    "class AudioExtractor:\n",
    "    def __init__(self, video_path, chunk_duration=WINDOW_SIZE_SECONDS):\n",
    "        self.video_path = video_path\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.sampling_rate = SAMPLING_RATE\n",
    "        \n",
    "        # Extract full audio\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        self.temp_audio_path = os.path.join(self.temp_dir, \"temp_audio.wav\")\n",
    "        video = mp.VideoFileClip(video_path)\n",
    "        self.duration = video.duration\n",
    "        video.audio.write_audiofile(self.temp_audio_path, fps=self.sampling_rate, verbose=False, logger=None)\n",
    "        \n",
    "        # Load audio\n",
    "        self.waveform, self.orig_sample_rate = torchaudio.load(self.temp_audio_path)\n",
    "        self.waveform = self.waveform.mean(dim=0)  # Convert to mono\n",
    "        \n",
    "    def get_audio_segment(self, start_time, end_time):\n",
    "        # Convert time to samples\n",
    "        start_sample = int(start_time * self.sampling_rate)\n",
    "        end_sample = int(end_time * self.sampling_rate)\n",
    "        \n",
    "        # Ensure within bounds\n",
    "        if start_sample >= self.waveform.size(0) or end_sample <= 0:\n",
    "            return None\n",
    "        \n",
    "        start_sample = max(0, start_sample)\n",
    "        end_sample = min(self.waveform.size(0), end_sample)\n",
    "        \n",
    "        # Extract segment\n",
    "        segment = self.waveform[start_sample:end_sample].numpy()\n",
    "        return segment\n",
    "    \n",
    "    def cleanup(self):\n",
    "        # Clean up temporary files\n",
    "        try:\n",
    "            os.remove(self.temp_audio_path)\n",
    "            os.rmdir(self.temp_dir)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe11dfe-bc4a-42bc-8c83-1070d10662c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main application window\n",
    "class AudioVideoPlayerWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup UI\n",
    "        self.setWindowTitle(\"Audio Anomaly Detection Video Player\")\n",
    "        self.setGeometry(100, 100, 1000, 600)\n",
    "        \n",
    "        # Create central widget and layout\n",
    "        self.central_widget = QWidget()\n",
    "        self.setCentralWidget(self.central_widget)\n",
    "        self.layout = QVBoxLayout(self.central_widget)\n",
    "        \n",
    "        # Video display area\n",
    "        self.video_frame = QLabel()\n",
    "        self.video_frame.setAlignment(Qt.AlignCenter)\n",
    "        self.video_frame.setMinimumSize(640, 480)\n",
    "        self.video_frame.setStyleSheet(\"background-color: black;\")\n",
    "        self.layout.addWidget(self.video_frame)\n",
    "        \n",
    "        # Current classification display\n",
    "        self.classification_label = QLabel(\"Classification: None\")\n",
    "        self.classification_label.setAlignment(Qt.AlignCenter)\n",
    "        self.classification_label.setStyleSheet(\"font-size: 18px; font-weight: bold;\")\n",
    "        self.layout.addWidget(self.classification_label)\n",
    "        \n",
    "        # Timeline widget\n",
    "        self.timeline_widget = TimelineWidget()\n",
    "        self.layout.addWidget(self.timeline_widget)\n",
    "        \n",
    "        # Controls layout\n",
    "        self.controls_layout = QHBoxLayout()\n",
    "        \n",
    "        # Play/Pause button\n",
    "        self.play_button = QPushButton()\n",
    "        self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "        self.play_button.clicked.connect(self.toggle_play)\n",
    "        self.controls_layout.addWidget(self.play_button)\n",
    "        \n",
    "        # Time display\n",
    "        self.time_label = QLabel(\"00:00 / 00:00\")\n",
    "        self.controls_layout.addWidget(self.time_label)\n",
    "        \n",
    "        # Position slider\n",
    "        self.position_slider = QSlider(Qt.Horizontal)\n",
    "        self.position_slider.sliderMoved.connect(self.set_position)\n",
    "        self.controls_layout.addWidget(self.position_slider)\n",
    "        \n",
    "        # Open file button\n",
    "        self.open_button = QPushButton(\"Open Video\")\n",
    "        self.open_button.clicked.connect(self.open_file)\n",
    "        self.controls_layout.addWidget(self.open_button)\n",
    "        \n",
    "        self.layout.addLayout(self.controls_layout)\n",
    "        \n",
    "        # Video processing variables\n",
    "        self.cap = None\n",
    "        self.timer = QTimer(self)\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.current_frame = 0\n",
    "        self.fps = 0\n",
    "        self.total_frames = 0\n",
    "        self.playing = False\n",
    "        \n",
    "        # Audio processing variables\n",
    "        self.audio_extractor = None\n",
    "        self.anomaly_segments = []\n",
    "        self.current_time = 0\n",
    "        self.last_prediction_time = 0\n",
    "        \n",
    "        # Start prediction thread\n",
    "        self.prediction_worker = AudioPredictionWorker(model, processor, SAMPLING_RATE)\n",
    "        self.prediction_worker.predictionReady.connect(self.update_anomaly)\n",
    "        self.prediction_worker.start()\n",
    "\n",
    "    def open_file(self):\n",
    "        file_path, _ = QFileDialog.getOpenFileName(self, \"Open Video File\", \"\", \n",
    "                                                \"Video Files (*.mp4 *.avi *.mkv *.mov);;All Files (*)\")\n",
    "        if file_path:\n",
    "            self.load_video(file_path)\n",
    "    \n",
    "    def load_video(self, file_path):\n",
    "        # Stop current video if playing\n",
    "        if self.playing:\n",
    "            self.timer.stop()\n",
    "            self.playing = False\n",
    "        \n",
    "        # Release previous capture if any\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        \n",
    "        # Cleanup previous audio extractor\n",
    "        if self.audio_extractor:\n",
    "            self.audio_extractor.cleanup()\n",
    "        \n",
    "        # Open new video\n",
    "        self.cap = cv2.VideoCapture(file_path)\n",
    "        if not self.cap.isOpened():\n",
    "            print(f\"Error: Could not open video {file_path}\")\n",
    "            return\n",
    "        \n",
    "        # Extract audio\n",
    "        print(\"Extracting audio from video...\")\n",
    "        self.audio_extractor = AudioExtractor(file_path)\n",
    "        \n",
    "        # Get video properties\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.current_frame = 0\n",
    "        self.current_time = 0\n",
    "        self.last_prediction_time = 0\n",
    "        \n",
    "        # Clear previous anomalies\n",
    "        self.anomaly_segments = []\n",
    "        self.timeline_widget.set_anomalies([])\n",
    "        self.timeline_widget.set_duration(self.audio_extractor.duration)\n",
    "        \n",
    "        # Reset UI\n",
    "        self.position_slider.setRange(0, self.total_frames)\n",
    "        duration_str = self.format_time(self.total_frames / self.fps)\n",
    "        self.time_label.setText(f\"00:00 / {duration_str}\")\n",
    "        self.classification_label.setText(\"Classification: None\")\n",
    "        \n",
    "        # Show first frame\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            self.display_frame(frame)\n",
    "        \n",
    "        print(\"Video loaded successfully\")\n",
    "    \n",
    "    def toggle_play(self):\n",
    "        if self.cap is None:\n",
    "            return\n",
    "            \n",
    "        if self.playing:\n",
    "            self.timer.stop()\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "        else:\n",
    "            self.timer.start(1000 // 30)  # 30 fps display\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPause))\n",
    "        \n",
    "        self.playing = not self.playing\n",
    "    \n",
    "    def update_frame(self):\n",
    "        if self.cap is None or not self.playing:\n",
    "            return\n",
    "            \n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            # End of video\n",
    "            self.timer.stop()\n",
    "            self.playing = False\n",
    "            self.play_button.setIcon(self.style().standardIcon(QStyle.SP_MediaPlay))\n",
    "            return\n",
    "        \n",
    "        # Display the frame\n",
    "        self.display_frame(frame)\n",
    "        \n",
    "        # Update current time\n",
    "        self.current_time = self.current_frame / self.fps\n",
    "        \n",
    "        # Process audio segment for prediction\n",
    "        if self.current_time - self.last_prediction_time >= WINDOW_SIZE_SECONDS / 2:  # 50% overlap\n",
    "            start_time = max(0, self.current_time - WINDOW_SIZE_SECONDS / 2)\n",
    "            end_time = start_time + WINDOW_SIZE_SECONDS\n",
    "            \n",
    "            # Get audio segment\n",
    "            audio_segment = self.audio_extractor.get_audio_segment(start_time, end_time)\n",
    "            if audio_segment is not None:\n",
    "                # Add to prediction queue\n",
    "                self.prediction_worker.add_audio_segment(audio_segment, start_time, end_time)\n",
    "            \n",
    "            self.last_prediction_time = self.current_time\n",
    "        \n",
    "        # Update slider and time label\n",
    "        self.position_slider.setValue(self.current_frame)\n",
    "        current_time_str = self.format_time(self.current_time)\n",
    "        duration_str = self.format_time(self.total_frames / self.fps)\n",
    "        self.time_label.setText(f\"{current_time_str} / {duration_str}\")\n",
    "        \n",
    "        # Update classification display\n",
    "        current_class = self.get_classification_at_time(self.current_time)\n",
    "        self.classification_label.setText(f\"Classification: {current_class}\")\n",
    "        \n",
    "        # Set classification color\n",
    "        color = COLOR_MAP.get(current_class, QColor(100, 100, 100))\n",
    "        self.classification_label.setStyleSheet(f\"font-size: 18px; font-weight: bold; color: rgb({color.red()}, {color.green()}, {color.blue()})\")\n",
    "        \n",
    "        # Increment frame counter\n",
    "        self.current_frame += 1\n",
    "    \n",
    "    def display_frame(self, frame):\n",
    "        # Convert frame to QImage and display\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = frame_rgb.shape\n",
    "        bytes_per_line = ch * w\n",
    "        q_img = QImage(frame_rgb.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "        self.video_frame.setPixmap(QPixmap.fromImage(q_img).scaled(\n",
    "            self.video_frame.width(), self.video_frame.height(), \n",
    "            Qt.KeepAspectRatio, Qt.SmoothTransformation))\n",
    "    \n",
    "    def set_position(self, position):\n",
    "        if self.cap is None:\n",
    "            return\n",
    "            \n",
    "        # Seek to position\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, position)\n",
    "        self.current_frame = position\n",
    "        self.current_time = position / self.fps\n",
    "        \n",
    "        # Read and display the frame\n",
    "        ret, frame = self.cap.read()\n",
    "        if ret:\n",
    "            self.display_frame(frame)\n",
    "            \n",
    "            # Update time label\n",
    "            current_time_str = self.format_time(self.current_time)\n",
    "            duration_str = self.format_time(self.total_frames / self.fps)\n",
    "            self.time_label.setText(f\"{current_time_str} / {duration_str}\")\n",
    "            \n",
    "            # Update classification display\n",
    "            current_class = self.get_classification_at_time(self.current_time)\n",
    "            self.classification_label.setText(f\"Classification: {current_class}\")\n",
    "            \n",
    "            # Set classification color\n",
    "            color = COLOR_MAP.get(current_class, QColor(100, 100, 100))\n",
    "            self.classification_label.setStyleSheet(f\"font-size: 18px; font-weight: bold; color: rgb({color.red()}, {color.green()}, {color.blue()})\")\n",
    "    \n",
    "    def update_anomaly(self, start_time, end_time, anomaly_type):\n",
    "        # Add new anomaly segment\n",
    "        self.anomaly_segments.append((start_time, end_time, anomaly_type))\n",
    "        # Update timeline display\n",
    "        self.timeline_widget.set_anomalies(self.anomaly_segments)\n",
    "    \n",
    "    def get_classification_at_time(self, time_point):\n",
    "        # Find the most recent classification for the current time\n",
    "        for start_time, end_time, anomaly_type in reversed(self.anomaly_segments):\n",
    "            if start_time <= time_point and time_point <= end_time:\n",
    "                return anomaly_type\n",
    "        return \"None\"\n",
    "    \n",
    "    def format_time(self, seconds):\n",
    "        minutes, seconds = divmod(int(seconds), 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\" if hours else f\"{minutes:02d}:{seconds:02d}\"\n",
    "    \n",
    "    def closeEvent(self, event):\n",
    "        # Clean up resources\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        if self.audio_extractor:\n",
    "            self.audio_extractor.cleanup()\n",
    "        self.prediction_worker.stop()\n",
    "        self.prediction_worker.wait()\n",
    "        event.accept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4802bb02-d9e3-4c50-93c2-c8fefff30987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline widget to show anomaly segments\n",
    "class TimelineWidget(QFrame):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setMinimumHeight(40)\n",
    "        self.setStyleSheet(\"background-color: #2d2d2d;\")\n",
    "        \n",
    "        self.anomalies = []\n",
    "        self.duration = 0\n",
    "        \n",
    "    def set_anomalies(self, anomalies):\n",
    "        self.anomalies = anomalies\n",
    "        self.update()\n",
    "        \n",
    "    def set_duration(self, duration):\n",
    "        self.duration = duration\n",
    "        self.update()\n",
    "        \n",
    "    def paintEvent(self, event):\n",
    "        if self.duration <= 0:\n",
    "            return\n",
    "            \n",
    "        painter = QPainter(self)\n",
    "        painter.setRenderHint(QPainter.Antialiasing)\n",
    "        \n",
    "        # Draw background\n",
    "        painter.fillRect(self.rect(), QBrush(QColor(45, 45, 45)))\n",
    "        \n",
    "        # Draw timeline base\n",
    "        painter.setPen(QPen(QColor(200, 200, 200), 1))\n",
    "        y_middle = self.height() // 2\n",
    "        painter.drawLine(0, y_middle, self.width(), y_middle)\n",
    "        \n",
    "        # Draw time markers\n",
    "        painter.setPen(QPen(QColor(150, 150, 150), 1))\n",
    "        marker_interval = self.width() / 10\n",
    "        for i in range(11):\n",
    "            x = i * marker_interval\n",
    "            painter.drawLine(int(x), y_middle - 5, int(x), y_middle + 5)\n",
    "            \n",
    "            # Draw time text\n",
    "            time_at_marker = (i / 10) * self.duration\n",
    "            minutes = int(time_at_marker / 60)\n",
    "            seconds = int(time_at_marker % 60)\n",
    "            time_text = f\"{minutes:02d}:{seconds:02d}\"\n",
    "            painter.drawText(int(x) - 15, y_middle + 20, time_text)\n",
    "        \n",
    "        # Draw anomaly segments\n",
    "        for start_time, end_time, anomaly_type in self.anomalies:\n",
    "            if start_time >= self.duration:\n",
    "                continue\n",
    "                \n",
    "            # Calculate positions\n",
    "            start_pos = int((start_time / self.duration) * self.width())\n",
    "            end_pos = int((min(end_time, self.duration) / self.duration) * self.width())\n",
    "            \n",
    "            # Get color for anomaly type\n",
    "            color = COLOR_MAP.get(anomaly_type, QColor(100, 100, 100))\n",
    "            \n",
    "            # Draw segment\n",
    "            painter.fillRect(start_pos, 5, end_pos - start_pos, self.height() - 10, QBrush(color))\n",
    "            \n",
    "            # Draw label if segment is wide enough\n",
    "            if end_pos - start_pos > 50:\n",
    "                painter.setPen(QPen(QColor(255, 255, 255), 1))\n",
    "                painter.drawText(start_pos + 5, y_middle + 5, anomaly_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5da5658-559d-4e9c-9632-9bdd654fb4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio from video...\n",
      "Video loaded successfully\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# Main application\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = AudioVideoPlayerWindow()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVi",
   "language": "python",
   "name": "vivi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
